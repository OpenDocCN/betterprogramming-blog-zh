<html>
<head>
<title>A Deep Dive Into Spark Datasets and DataFrames Using Scala</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scala深入探究Spark数据集和数据帧</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/a-deep-dive-into-spark-datasets-and-dataframes-using-scala-a268b4af7491?source=collection_archive---------0-----------------------#2021-05-28">https://betterprogramming.pub/a-deep-dive-into-spark-datasets-and-dataframes-using-scala-a268b4af7491?source=collection_archive---------0-----------------------#2021-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fc38" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Spark数据集和数据框架综合指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9448a3b0dea4f614487b1a94e7667cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLpfOeaXweYKeliuHmQXPA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="4d57" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">初步的</h1><p id="cca5" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mm" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spar </a> k是一个开源的分布式数据处理引擎，可用于大数据分析。它内置了用于流、图形处理和机器学习的库，数据科学家可以使用Spark快速分析大规模数据。Spark支持的编程语言包括Python、Java、Scala和r。</p><p id="f6e5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://docs.scala-lang.org/?_ga=2.112570636.905715846.1622125401-132110329.1622125401" rel="noopener ugc nofollow" target="_blank"> Scala </a>是一种结合了函数式和面向对象编程的强大编程语言。它是一种基于JVM的统计类型语言。Apache Spark是用Scala编写的，由于它在JVM上的可伸缩性，它是从事Spark项目的数据开发人员的一种流行编程语言。在本文中，我将向您展示如何通过Scala使用Spark数据集和数据帧。</p><h2 id="a150" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">代码列表</h2><p id="cb82" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">本文中的代码清单已经在带有Spark 3.1.1和Scala 2.12的Databricks Community Edition cluster(Runtime 8.2)上进行了测试。一些代码清单可能无法在较低版本的Spark上运行。您可以在本文末尾找到所有代码清单的源代码链接。</p><h2 id="eb7d" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">Spark源代码</h2><p id="9468" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Spark是一个开源项目，其源代码可以在<a class="ae mm" href="https://github.com/apache/spark" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。本文不是关于Spark内部的；然而，对于大多数方法，我在Spark源代码中放置了它们的定义的链接。Spark主要是用Scala写的，知道它是怎么设计的就能对它有更好的理解。因此，对于其中的一些方法，我已经简要地解释了它们在内部是如何工作的。我参考过Spark 3.1.1的源代码，但是其他版本的Spark源代码可以不一样。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="e7ef" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated">目录</h1><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1375" class="ms kz it nr b gy nv nw l nx ny"><a class="ae mm" href="#4738" rel="noopener ugc nofollow">Introduction</a></span><span id="fc48" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#1262" rel="noopener ugc nofollow">Spark RDDs</a></span><span id="bc8b" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#64b7" rel="noopener ugc nofollow">Datasets</a></span><span id="107a" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#47b8" rel="noopener ugc nofollow">Encoders</a><br/>-<a class="ae mm" href="#e4b2" rel="noopener ugc nofollow">Internal Tungsten binary format</a><br/>-<a class="ae mm" href="#80c5" rel="noopener ugc nofollow">InternalRow and UnsafeRow</a></span><span id="44a1" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#5938" rel="noopener ugc nofollow">Creating Datasets</a><br/>-<a class="ae mm" href="#9b85" rel="noopener ugc nofollow">createDataset()</a><em class="oa"><br/></em>-<a class="ae mm" href="#d114" rel="noopener ugc nofollow">toDS()</a></span><span id="f016" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#be00" rel="noopener ugc nofollow">DataFrames</a><br/>-<a class="ae mm" href="#5a56" rel="noopener ugc nofollow">Row objects</a><br/>-<a class="ae mm" href="http://a522" rel="noopener ugc nofollow" target="_blank">Schema</a><br/>-<a class="ae mm" href="#112d" rel="noopener ugc nofollow">RowEncoder</a><br/>-<a class="ae mm" href="#e77e" rel="noopener ugc nofollow">Creating a DataFrame from a Dataset</a><br/>-<a class="ae mm" href="#c74c" rel="noopener ugc nofollow">Creating a DataFrame from scratch</a><br/>-<a class="ae mm" href="#8069" rel="noopener ugc nofollow">createDataFrame()</a><em class="oa"><br/></em>-<a class="ae mm" href="#1ab2" rel="noopener ugc nofollow">toDF()</a><br/>-<a class="ae mm" href="#bf72" rel="noopener ugc nofollow">Creating a DataFrame using case classes</a><br/>-<a class="ae mm" href="#7658" rel="noopener ugc nofollow">DataFrames vs. Datasets</a></span><span id="174e" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#6ce0" rel="noopener ugc nofollow">Displaying DataFrames</a><br/>-<a class="ae mm" href="#9636" rel="noopener ugc nofollow">show()</a><em class="oa"><br/></em>-<a class="ae mm" href="#eabb" rel="noopener ugc nofollow">display()</a></span><span id="becb" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#fcc5" rel="noopener ugc nofollow">Importing a DataFrame</a><br/>-<a class="ae mm" href="#9f56" rel="noopener ugc nofollow">read()</a></span><span id="90a8" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#0bff" rel="noopener ugc nofollow">Showing the Schema</a><br/>-<a class="ae mm" href="#ea14" rel="noopener ugc nofollow">printSchema()</a><em class="oa"><br/></em>-<a class="ae mm" href="#cfbd" rel="noopener ugc nofollow">schema</a></span><span id="9933" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#4a33" rel="noopener ugc nofollow">Shape of a DataFrame</a><br/>-<a class="ae mm" href="#848c" rel="noopener ugc nofollow">count()</a><em class="oa"><br/></em>-<a class="ae mm" href="#78f6" rel="noopener ugc nofollow">columns</a></span><span id="7aeb" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#f925" rel="noopener ugc nofollow">Displaying the Rows</a><br/>-<a class="ae mm" href="#1132" rel="noopener ugc nofollow">head()</a><em class="oa"><br/></em>-<a class="ae mm" href="#84dd" rel="noopener ugc nofollow">take()</a><em class="oa"><br/></em>-<a class="ae mm" href="#2f5f" rel="noopener ugc nofollow">first()</a><em class="oa"><br/></em>-<a class="ae mm" href="#926b" rel="noopener ugc nofollow">tail()</a><em class="oa"><br/></em>-<a class="ae mm" href="#c6b8" rel="noopener ugc nofollow">limit()</a></span><span id="bb3e" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#28a7" rel="noopener ugc nofollow">Calculating the Statistics</a><br/>-<a class="ae mm" href="#2202" rel="noopener ugc nofollow">describe()</a></span><span id="9bed" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#684a" rel="noopener ugc nofollow">Null Values</a></span><span id="c8c5" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#bc4c" rel="noopener ugc nofollow">Columns</a><br/>-<a class="ae mm" href="#9958" rel="noopener ugc nofollow">col() and column()</a><em class="oa"><br/></em>-<a class="ae mm" href="#1b8f" rel="noopener ugc nofollow">$()</a><br/>-<a class="ae mm" href="#cc6a" rel="noopener ugc nofollow">Column symbols</a><br/>-<a class="ae mm" href="#6a55" rel="noopener ugc nofollow">Selecting columns</a><br/>   <a class="ae mm" href="#68e1" rel="noopener ugc nofollow">select()</a><br/>-<a class="ae mm" href="#dccc" rel="noopener ugc nofollow">Dropping duplicates</a><br/> <em class="oa">  </em><a class="ae mm" href="#776b" rel="noopener ugc nofollow">dropDuplicates()</a><em class="oa"><br/>   </em><a class="ae mm" href="#257d" rel="noopener ugc nofollow">distinct()</a><br/>-<a class="ae mm" href="#e18d" rel="noopener ugc nofollow">Column expressions</a><br/>-<a class="ae mm" href="#e7d1" rel="noopener ugc nofollow">concat()</a><em class="oa"><br/></em>-<a class="ae mm" href="#6731" rel="noopener ugc nofollow">Column aliases</a><em class="oa"><br/>   </em><a class="ae mm" href="#bcc3" rel="noopener ugc nofollow">name()</a><em class="oa"><br/>   </em><a class="ae mm" href="#02ab" rel="noopener ugc nofollow">alias() and as()</a><em class="oa"><br/></em>-<a class="ae mm" href="#9b07" rel="noopener ugc nofollow">expr()</a><br/>-<a class="ae mm" href="#d103" rel="noopener ugc nofollow">Math functions</a><br/>   <a class="ae mm" href="#bee5" rel="noopener ugc nofollow">log() and round()</a><em class="oa"><br/>   </em><a class="ae mm" href="#9ddf" rel="noopener ugc nofollow">corr()</a><br/>-<a class="ae mm" href="#75d5" rel="noopener ugc nofollow">Creating a new column</a><br/>   <a class="ae mm" href="#6f88" rel="noopener ugc nofollow">withColumn()</a><em class="oa"><br/></em>-<a class="ae mm" href="#5b13" rel="noopener ugc nofollow">Renaming columns</a><em class="oa"><br/>   </em><a class="ae mm" href="#95ca" rel="noopener ugc nofollow">withColumnRenamed()</a><em class="oa"><br/></em>-<a class="ae mm" href="#eee3" rel="noopener ugc nofollow">Changing data type of columns</a><em class="oa"><br/>   </em><a class="ae mm" href="#4def" rel="noopener ugc nofollow">cast()</a><br/>-<a class="ae mm" href="#c795" rel="noopener ugc nofollow">Canonical string representation</a><br/>-<a class="ae mm" href="#f3a2" rel="noopener ugc nofollow">Timestamp</a><br/>-<a class="ae mm" href="#3b1b" rel="noopener ugc nofollow">Dropping a column</a><br/>   <a class="ae mm" href="#564a" rel="noopener ugc nofollow">drop()</a></span><span id="75b6" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#4cd1" rel="noopener ugc nofollow">Rows</a><br/>-<a class="ae mm" href="#aaf9" rel="noopener ugc nofollow">getAs()</a><em class="oa"><br/></em>-<a class="ae mm" href="#3424" rel="noopener ugc nofollow">getInt(), getDouble() and getString()</a><br/>-<a class="ae mm" href="#6405" rel="noopener ugc nofollow">Splitting a column</a><br/>-<a class="ae mm" href="#62aa" rel="noopener ugc nofollow">split()</a><em class="oa"><br/></em>-<a class="ae mm" href="#d2ad" rel="noopener ugc nofollow">UDFs</a><em class="oa"><br/>   </em><a class="ae mm" href="#4853" rel="noopener ugc nofollow">udf()</a><br/>   <a class="ae mm" href="#bea6" rel="noopener ugc nofollow">Broadcast variables</a> <br/>   <a class="ae mm" href="#0aca" rel="noopener ugc nofollow">broadcast()</a><br/>   <a class="ae mm" href="#0c21" rel="noopener ugc nofollow">UDFs with non-Column parameters</a><br/>   <a class="ae mm" href="#f82e" rel="noopener ugc nofollow">Null values in UDFs</a><br/>-<a class="ae mm" href="#8099" rel="noopener ugc nofollow">Filtering the rows</a><br/>   <a class="ae mm" href="#e466" rel="noopener ugc nofollow">filter()</a><em class="oa"><br/>   </em><a class="ae mm" href="#a68d" rel="noopener ugc nofollow">where()</a><em class="oa"><br/>   </em><a class="ae mm" href="#936e" rel="noopener ugc nofollow">between()</a><br/>-<a class="ae mm" href="#bf3e" rel="noopener ugc nofollow">Methods for missing values</a><br/>   <a class="ae mm" href="#0b2d" rel="noopener ugc nofollow">isNull()</a><em class="oa"><br/>   </em><a class="ae mm" href="#7c56" rel="noopener ugc nofollow">isNotNull()</a><em class="oa"><br/>   </em><a class="ae mm" href="#d1c4" rel="noopener ugc nofollow">fill()</a><em class="oa"><br/>   </em><a class="ae mm" href="#4308" rel="noopener ugc nofollow">drop()</a><em class="oa"><br/></em>-<a class="ae mm" href="#91c7" rel="noopener ugc nofollow">when() and otherwise()</a></span><span id="7092" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#ce7b" rel="noopener ugc nofollow">Partitions</a><br/>-<a class="ae mm" href="#22af" rel="noopener ugc nofollow">getNumPartitions()</a><em class="oa"><br/></em>-<a class="ae mm" href="#a66b" rel="noopener ugc nofollow">partitions(</a><a class="ae mm" href="#a66b" rel="noopener ugc nofollow">)</a><em class="oa"> <br/></em>-<a class="ae mm" href="#8666" rel="noopener ugc nofollow">mapPartitionsWithIndex()</a><em class="oa"><br/></em>-<a class="ae mm" href="#6d19" rel="noopener ugc nofollow">repartition()</a><em class="oa"><br/></em>-<a class="ae mm" href="#9ee2" rel="noopener ugc nofollow">coalesce()</a><br/>-<a class="ae mm" href="#ef94" rel="noopener ugc nofollow">Adding an index</a><br/>   <a class="ae mm" href="#aff9" rel="noopener ugc nofollow">monotonically_increasing_id()</a><em class="oa"><br/>   </em><a class="ae mm" href="#a8ed" rel="noopener ugc nofollow">zipWithIndex()</a></span><span id="5384" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#a1ff" rel="noopener ugc nofollow">Saving a Dataset</a><br/>-<a class="ae mm" href="#cfa1" rel="noopener ugc nofollow">write()</a></span><span id="e992" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#9457" rel="noopener ugc nofollow">Sorting</a><br/>-<a class="ae mm" href="#0cbc" rel="noopener ugc nofollow">sort()</a><em class="oa"><br/></em>-<a class="ae mm" href="#8d24" rel="noopener ugc nofollow">desc()</a></span><span id="89f7" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#1513" rel="noopener ugc nofollow">Aggregation</a><br/>-<a class="ae mm" href="#66cb" rel="noopener ugc nofollow">groupBy()</a><em class="oa"><br/></em>-<a class="ae mm" href="#3934" rel="noopener ugc nofollow">count()</a><em class="oa"><br/></em>-<a class="ae mm" href="#54ff" rel="noopener ugc nofollow">agg()</a><br/>-<a class="ae mm" href="#af70" rel="noopener ugc nofollow">Aggregate functions</a><br/>-<a class="ae mm" href="#9afa" rel="noopener ugc nofollow">Pivoting</a><br/>   <a class="ae mm" href="#d29e" rel="noopener ugc nofollow">pivot()</a><br/>-<a class="ae mm" href="#e6a4" rel="noopener ugc nofollow">Unpivoting</a><br/>-<a class="ae mm" href="#dad8" rel="noopener ugc nofollow">Transpose</a><br/>-<a class="ae mm" href="#24cd" rel="noopener ugc nofollow">collect_list()</a></span><span id="34e0" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#4505" rel="noopener ugc nofollow">Window functions</a><br/>-<a class="ae mm" href="#8f69" rel="noopener ugc nofollow">orderBy()</a><em class="oa"><br/></em>-<a class="ae mm" href="#330d" rel="noopener ugc nofollow">partitionBy()</a><em class="oa"><br/></em>-<a class="ae mm" href="#f8e6" rel="noopener ugc nofollow">rowsBetween()</a><em class="oa"><br/></em>-<a class="ae mm" href="#07e4" rel="noopener ugc nofollow">over()</a><em class="oa"><br/></em>-<a class="ae mm" href="#b5a6" rel="noopener ugc nofollow">lag()</a><em class="oa"><br/></em>-<a class="ae mm" href="#464e" rel="noopener ugc nofollow">rangeBetween(</a><a class="ae mm" href="#464e" rel="noopener ugc nofollow"><em class="oa">)</em></a></span><span id="9b95" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#9550" rel="noopener ugc nofollow">Joins</a><br/>-<a class="ae mm" href="#51cb" rel="noopener ugc nofollow">join()</a><br/>-<a class="ae mm" href="#a5b6" rel="noopener ugc nofollow">Inner join</a><br/>-<a class="ae mm" href="#020f" rel="noopener ugc nofollow">Left outer and right outer joins</a><br/>-<a class="ae mm" href="#cd57" rel="noopener ugc nofollow">Full outer join</a><br/>-<a class="ae mm" href="http://4ef5" rel="noopener ugc nofollow" target="_blank">Left semi join</a><br/>-<a class="ae mm" href="#c9e3" rel="noopener ugc nofollow">Left anti join</a><br/>-<a class="ae mm" href="#9b77" rel="noopener ugc nofollow">Cross join</a><br/>   <a class="ae mm" href="#2ce5" rel="noopener ugc nofollow">crossJoin()</a><br/>-<a class="ae mm" href="#ca06" rel="noopener ugc nofollow">Type-preserving joins</a><br/>   <a class="ae mm" href="#0806" rel="noopener ugc nofollow">joinWith()</a></span><span id="e123" class="ms kz it nr b gy nz nw l nx ny"><a class="ae mm" href="#3ee0" rel="noopener ugc nofollow">Concatenating Datasets and DataFrames</a><br/>-<a class="ae mm" href="#c059" rel="noopener ugc nofollow">union()</a></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="4738" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">简介</strong></h1><p id="b42c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据集和数据框通常指的是表格数据结构。Spark数据集和数据帧分布在带有命名列和模式的内存表中，其中每一列都有特定的数据类型。火花数据帧类似于熊猫数据帧；然而，它们之间有一些重要的区别</p><ul class=""><li id="5182" class="ob oc it ls b lt mn lw mo lz od md oe mh of ml og oh oi oj bi translated"><strong class="ls iu"> Spark数据集/数据帧是不可变的</strong>:创建Spark数据帧(或数据集)后，您不能更改它。在Spark中，操作不会改变原始数据帧；相反，它们会将操作的结果作为新的数据帧返回。因此，Spark方法不会改变它们的参数或调用它们的对象。</li><li id="a576" class="ob oc it ls b lt ok lw ol lz om md on mh oo ml og oh oi oj bi translated"><strong class="ls iu"> Spark数据集/数据帧是分布式的</strong> : Pandas运行在单台机器上，但是Spark代码可以以分布式的方式执行，所以Spark数据集/数据帧在本质上是分布式的。</li><li id="7270" class="ob oc it ls b lt ok lw ol lz om md on mh oo ml og oh oi oj bi translated"><strong class="ls iu"> Spark是懒惰的:</strong>懒惰求值是一种求值策略，其中表达式的求值被延迟到需要它的值时进行。Spark有两种操作:<em class="oa">转换</em>和<em class="oa">动作</em>。转换是将Spark数据帧(或数据集)转换成新数据帧的操作。(当然，它们不会真正改变原始数据帧，因为它们是不可变的。相反，它们返回一个新的转换后的数据帧。)动作是不返回数据帧或数据集的任何其他操作。例如，它们可以在屏幕上显示一个数据帧，将其写入存储，或者触发一个计算并返回结果(比如计算数据帧中的行数)。在Spark中，转换是延迟计算的，所以转换的执行被延迟到一个动作被调用。</li></ul><p id="efe7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">本文主要关注Spark数据集和数据框架。然而，为了更好地理解它们，我们应该首先熟悉Spark RDDs。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="1262" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">火花RDDs </strong></h1><p id="a75d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">弹性分布式数据集(RDD)是Spark的基本数据抽象。RDD代表一个不可变的记录集合，它分布在集群中的多个节点上，可以并行操作。RDD具有弹性，这意味着如果保存部分RDD数据的节点出现故障，Spark可以恢复丢失的部分。rdd很重要，因为所有其他更高级的结构化数据抽象，如数据帧，都是基于它们构建的。与数据集和数据帧不同，rdd没有模式或命名列。</p><p id="34a0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以通过向<code class="fe op oq or nr b">SparkContext</code>的<code class="fe op oq or nr b">parallelize()</code>方法传递一个集合对象(比如Seq或List)来创建一个RDD。集合对象的元素可以是元组。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e8ca" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> someData = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>)<br/><strong class="nr iu">val</strong> rdd = spark.sparkContext.parallelize(someData)<br/>rdd.foreach(println)</span><span id="b34e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[5217] at parallelize at command-351177476716599:8</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="64b7" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">数据集</strong></h1><p id="2e9c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在Spark中，数据集是强类型、分布式、类似表格的对象集合，具有定义良好的行和列。数据集有一个<em class="oa">模式</em>，该模式<em class="oa"> </em>定义了列的名称及其数据类型。数据集提供编译时类型安全，这意味着Spark在编译时检查数据集元素的类型。数据集只在Java和Scala中可用，因为在这些语言中，类型在编译时被绑定到变量。另一方面，它们不存在于像Python这样的动态类型语言中，在Python中，变量的类型只在运行时检查。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="47b8" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">编码器</strong></h1><p id="052b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Java虚拟机(JVM)将内存分为<em class="oa">栈</em>和<em class="oa">堆</em>。新对象总是在堆空间中创建，对这些对象的引用存储在堆栈内存中。Java垃圾收集(GC)是Java程序执行自动内存管理的过程。当堆上的一些对象不再需要时，垃圾收集器会找到它们并删除它们以释放堆。Java垃圾收集是一个自动的过程。</p><p id="2c5d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于存储在堆上的所有数据都要进行垃圾收集，如果存储在堆上的数据变大，垃圾收集器消耗的时间就会长得多。垃圾收集器有一个垃圾收集暂停，也称为<em class="oa"> stop-the-world </em>事件，这意味着在某个时刻，所有的应用程序线程都将被挂起，直到垃圾收集器处理完堆中的所有对象。在暂停期间，所有操作都将暂停，因此在GC暂停期间，集群上的一个节点对于其他节点来说可能是关闭的。</p><p id="8485" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当堆大小大于1Gb时，您会得到明显的暂停。然而，今天的服务器应用程序可能需要远远超过4Gb的堆。为了克服这个问题，Spark不在堆上创建基于JVM的对象(这些对象会被垃圾收集)来存储数据集或数据帧。相反，它将堆外的Java内存分配给它们。堆外内存位于JVM之外，不受垃圾收集的影响。为了将任意对象保存到这个非托管堆外内存中，您必须使用<em class="oa">序列化</em>。序列化是一种将对象转换为字节流的机制。<em class="oa">反序列化</em>是相反的过程，其中字节流用于重新创建实际的JVM对象。因此，应用程序将对象序列化到堆外内存中，稍后，可以使用反序列化来读取对象。</p><p id="53e3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">此外，Spark基于分布式计算，因此数据经常在集群中的计算机节点之间通过网络传输，我们需要将数据序列化为二进制格式，以便能够通过网络共享数据。当JVM对象需要从一个节点发送到另一个节点时，发送方首先将其序列化为一个字节数组，然后将其发送到接收方节点。当接收节点接收到二进制格式的数据时，它会将其反序列化回一个JVM对象。</p><h2 id="e4b2" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">内部钨二进制格式</strong></h2><p id="9a21" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">JVM有自己内置的Java序列化器和反序列化器；但是，它创建的二进制格式开销很大，并且昂贵和缓慢。所以Spark使用自己的二进制格式，称为Spark的<em class="oa">内部钨二进制格式</em>。在这种二进制格式中，Spark将对象存储在Java堆内存之外，与Java二进制格式相比，对象的二进制表示需要更少的内存。</p><p id="7b95" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在Spark中，将JVM对象转换(序列化)为内部二进制格式称为<em class="oa">编码</em>，将内部二进制格式转换为JVM对象(反序列化)称为<em class="oa">解码</em>。为了在Spark中创建数据集，我们需要一个<em class="oa">编码器</em>，由<em class="oa"> </em>负责编码和解码JVM对象。编码器在Spark SQL中被建模为<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Encoder.scala#L69" rel="noopener ugc nofollow" target="_blank">Encoder[T]</a></code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Encoder.scala#L69" rel="noopener ugc nofollow" target="_blank">特征</a>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2d86" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">trait</strong> Encoder[T] <strong class="nr iu">extends</strong> Serializable {<br/>  <strong class="nr iu">def</strong> schema: StructType<br/>  <strong class="nr iu">def</strong> clsTag: ClassTag[T]<br/>}</span></pre><p id="86bc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">类型<code class="fe op oq or nr b">T</code>代表<code class="fe op oq or nr b">Encoder[T]</code>可以处理的记录类型。类型为<code class="fe op oq or nr b">T</code> ( <code class="fe op oq or nr b">Encoder[T]</code>)的编码器用于将任何类型为<code class="fe op oq or nr b">T</code>的JVM对象或原语转换(编码和解码)为Spark SQL的<code class="fe op oq or nr b">InternalRow</code>，后者表示内部二进制行格式。</p><p id="5509" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">包<code class="fe op oq or nr b">org.apache.spark.sql.catalyst.encoders</code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L232" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L232" rel="noopener ugc nofollow" target="_blank">ExpressionEncoder[T]</a></code>可用于创建编码器，用于将类型<code class="fe op oq or nr b">T</code>的JVM对象与内部二进制行相互转换。它扩展并实现了特征<code class="fe op oq or nr b">Encoder</code>，并且它是在创建<code class="fe op oq or nr b">Dataset</code>时使用的<code class="fe op oq or nr b">Encoder</code>的唯一实现。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8f53" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> ExpressionEncoder[T](objSerializer: Expression,      <br/>  objDeserializer: Expression, clsTag: ClassTag[T])<br/>  <strong class="nr iu">extends</strong> Encoder[T] { ...</span></pre><p id="8076" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">下面是一个使用这个类为case类(<code class="fe op oq or nr b">MyClass</code>)创建一个<code class="fe op oq or nr b">Encoder</code>的例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4c84" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.encoders.ExpressionEncoder</span><span id="778a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">case class</strong> MyClass(name: <strong class="nr iu">String</strong>, id: <strong class="nr iu">Int</strong>, role: <strong class="nr iu">String</strong>)<br/><strong class="nr iu">val</strong> myClassEncoder = ExpressionEncoder[MyClass]()</span><span id="52a8" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>myClassEncoder: org.apache.spark.sql.catalyst.encoders.ExpressionEncoder[MyClass] = class[name[0]: string, id[0]: int, role[0]: string]</strong></span></pre><h2 id="80c5" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">内部流动和取消流动</strong></h2><p id="3f22" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一旦创建了编码器，就可以使用它的方法<code class="fe op oq or nr b">createSerializer()</code>来创建一个序列化器。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6edc" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> toRow = myClassEncoder.createSerializer()<br/>toRow(MyClass("John", 7, "developer"))</span><span id="6939" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: org.apache.spark.sql.catalyst.InternalRow = [0,2000000004,7,2800000009,6e686f4a,65706f6c65766564,72]</strong></span></pre><p id="fc2b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">结果是一个<code class="fe op oq or nr b">InternalRow</code>，也称为<em class="oa">催化剂行</em>或<em class="oa"> Spark SQL行</em>(输出中数字的含义将在后面解释)。它是在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala#L29" rel="noopener ugc nofollow" target="_blank">包</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala#L29" rel="noopener ugc nofollow" target="_blank">org.apache.spark.sql.catalyst</a></code>中定义的抽象类，代表Spark SQL中一行(数据集)的内部二进制格式。在前面的例子中，<code class="fe op oq or nr b">MyClass</code>对象可以被认为是数据集的一行，有三列— <code class="fe op oq or nr b">name</code>、<code class="fe op oq or nr b">id</code>和<code class="fe op oq or nr b">role</code>—<code class="fe op oq or nr b">InternalRow</code>对象表示其内部二进制格式。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3a21" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">abstract class</strong> InternalRow <strong class="nr iu">extends<br/>  </strong>SpecializedGetters <strong class="nr iu">with</strong> Serializable { ...</span></pre><p id="d1c9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">得到二进制行格式，我们应该把它转换成一个<code class="fe op oq or nr b">UnsafeRow</code>对象。它在<code class="fe op oq or nr b">org.apache.spark.sql.catalyst.expressions</code>包中的文件<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java#L62" rel="noopener ugc nofollow" target="_blank">UnsafeRow.java</a></code>(这是Java文件，不是Scala文件)中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="89c0" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">public final class</strong> UnsafeRow <strong class="nr iu">extends</strong> InternalRow <strong class="nr iu">implements</strong> Externalizable, KryoSerializable { ...</span></pre><p id="7b4f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">UnsafeRow</code>是抽象类<code class="fe op oq or nr b">InternalRow</code>的具体实现。它表示行的内部钨二进制格式，这是Spark数据集和数据帧的内存存储格式。它被称为不安全的，因为它代表可变的内部原始内存。这里我们将<code class="fe op oq or nr b">InternalRow</code>对象转换为<code class="fe op oq or nr b">UnsafeRow</code>对象。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="34ba" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.expressions.UnsafeRow<br/><strong class="nr iu">val</strong> internalRow = toRow(MyClass("John", 7, "developer"))<br/><strong class="nr iu">val</strong> unsafeRow = internalRow.asInstanceOf[UnsafeRow]<br/>unsafeRow.getBytes</span><span id="13a3" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res4: Array[Byte] = res3: Array[Byte] = Array(0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 32, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 40, 0, 0, 0, 74, 111, 104, 110, 0, 0, 0, 0, 100, 101, 118, 101, 108, 111, 112, 101, 114, 0, 0, 0, 0, 0, 0, 0)</strong></span></pre><p id="4183" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">方法<code class="fe op oq or nr b">getBytes()</code>返回<code class="fe op oq or nr b">MyClass</code>行的底层二进制格式。这就是该行在内存中的存储方式。结果是一个<code class="fe op oq or nr b">Byte</code>的数组。所以每个数字都是一个字节数据的十进制值。为了理解这些字节的含义，我们应该将字节数组分成8个字节的组。所以每组有8个字节或64位，相当于一个<code class="fe op oq or nr b">Long</code>类型。这些组在字节数组中形成三个区域:<em class="oa">空位设置区域</em>、<em class="oa">固定长度值区域</em>和<em class="oa">可变长度值区域</em>(图1)。</p><p id="e984" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">第一组属于空比特集区域，用于跟踪行中的空字段。<em class="oa"> </em>如果该行有一些空值，则该区域的一些位将为1。<code class="fe op oq or nr b">UnsaferRow</code>字节数组中的下一组属于固定长度值区域。如果一个字段在一个固定长度的行中，并且可以放入8个字节，那么它将被存储在这个区域的一个组中。(这适用于类型为<code class="fe op oq or nr b">Long</code>、<code class="fe op oq or nr b">Double</code>或<code class="fe op oq or nr b">Int</code>的值。)</p><p id="35ab" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果行中的字段具有可变长度(如字符串或列表)，则引用(指针)和该字段的长度将存储在相应的组中(长度和引用各占4个字节)。在这种情况下，字段的实际值将存储在可变长度值区域中，并接受需要存储的任何组。该引用有一个相对偏移量(相对于行的基址),指向实际存储该字段的可变长度值区域中的组的开头。</p><p id="0799" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在让我们看看<code class="fe op oq or nr b">MyClass</code>的字段是如何存储在字节数组中的。图1显示了字节数组、三个区域及其对应的组。这里<code class="fe op oq or nr b">MyClass</code>的对象没有空值，所以第一组(空位设置区)只有零值(前8个零)。在这个区域中，对于该行包含的每个字段，都有一个位。如果行中的字段为空，则相应的位将为1；否则就是零。<code class="fe op oq or nr b">MyClass</code>的第一个字段包含字符串<code class="fe op oq or nr b">John</code>。在固定长度值区域，第一组与<code class="fe op oq or nr b">John</code>相关。它不适合8字节的组，所以它的长度加上一个引用存储在这个组中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/7eb8a6ee2147809dc7d65f7268377acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_lSfasOAW1HwV_F_3Qr0pA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1</p></figure><p id="35dc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该组的前4个字节存储<code class="fe op oq or nr b">John</code>的长度，为4 (4，0，0，0)。接下来的4个字节存储相对偏移量，等于32。<code class="fe op oq or nr b">John</code>中字符的ASCII码(十进制值)为:<code class="fe op oq or nr b">J: 74</code>、<code class="fe op oq or nr b">o: 111</code>、<code class="fe op oq or nr b">h: 104</code>、<code class="fe op oq or nr b">n: 110</code>。通过查看图1，您可以看到它们存储在可变长度值区域的一个组中编号为32、33、34和35的字节中，该组的最后4个字节正好为0(我们假设字节数组中的第一个字节的索引为0)。所以32是指向存储字符<code class="fe op oq or nr b">John</code>的可变长度值区域中的一个组的开始的偏移量。</p><p id="89f3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">MyClass</code>的下一个字段是7。它可以放入一个8字节的组，所以下一个组包含字节<code class="fe op oq or nr b">7,0,0,0,0,0,0,0</code>。类似地<code class="fe op oq or nr b">developer</code>有一个组存储它的长度(9)和偏移量(40)。在可变长度值区域中，两个8字节组用于存储其字符，第一个字节的索引是40。</p><p id="823d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">记得最初<code class="fe op oq or nr b">MyClass</code>的<code class="fe op oq or nr b">InternalRow</code>物体显示这些数字:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5411" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">res1: org.apache.spark.sql.catalyst.InternalRow = [0,2000000004,7,2800000009,6e686f4a,65706f6c65766564,72]</strong></span></pre><p id="751e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">InternalRow</code>的每个字段代表<code class="fe op oq or nr b">UnsafeRow</code>字节数组的一个8字节组。变长值组中字节的十六进制值以相反的顺序组合成一个数，并省略该数左边的零。例如，<code class="fe op oq or nr b">John</code>的组是[74，111，104，110，0，0，0，0]。这个数组中数字的十六进制值是[4a，6f，68，6e，0，0，0，0]。如果你把它们以相反的顺序组合，你会得到<code class="fe op oq or nr b">00006e686f4a</code>。所以<code class="fe op oq or nr b">6e686f4a</code>是该组<code class="fe op oq or nr b">InternalRow</code>的对应字段。同样[7，0，0，0，0，0，0，0，0]变成了<code class="fe op oq or nr b">7</code>。</p><p id="e119" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">第二组是[4，0，0，0，32，0，0，0，0]。这里，偏移值乘以16⁸，长度值加到其上。(在十六进制系统中，乘以16⁸等于在一个数的末尾加8个零，或者乘以10⁸.)32的十六进制值是20。所以这个组对应的十六进制字段是20x10⁸+4 = 20000000004。</p><p id="277f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在让我们看一个例子，其中有一个<code class="fe op oq or nr b">None</code>值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="073f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> MyClass(name: <strong class="nr iu">Option</strong>[<strong class="nr iu">String</strong>], id: <strong class="nr iu">Option</strong>[<strong class="nr iu">Int</strong>],<br/> role: <strong class="nr iu">Option</strong>[<strong class="nr iu">String</strong>])<br/><strong class="nr iu">val</strong> myClassEncoder = ExpressionEncoder[MyClass]()<br/><strong class="nr iu">val</strong> toRow = myClassEncoder.createSerializer()<br/><strong class="nr iu">val</strong> internalRow = toRow(MyClass(Some("John"), Some(7), None))<br/><strong class="nr iu">val</strong> unsafeRow = internalRow.asInstanceOf[UnsafeRow]<br/>unsafeRow.getBytes</span><span id="b8f6" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>unsafeRow: org.apache.spark.sql.catalyst.expressions.UnsafeRow = [4,2000000004,7,0,6e686f4a] res7: Array[Byte] = Array(4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 32, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 74, 111, 104, 110, 0, 0, 0, 0)</strong></span></pre><p id="e9a2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在，第一组(空位设置区域)包含一个非零值，它的第一个字节是4。该字节中的位跟踪<code class="fe op oq or nr b">MyClass</code>的<code class="fe op oq or nr b">None</code>值。<code class="fe op oq or nr b">MyClass</code>有三个字段，最后一个是<code class="fe op oq or nr b">None</code>。记住，我们为空值或<code class="fe op oq or nr b">None</code>值保留1位，为可用值保留0位。所以第一个字节的位是<code class="fe op oq or nr b">00000100</code>。最低有效位(位0)与<code class="fe op oq or nr b">name</code>相关，位1与<code class="fe op oq or nr b">id</code>相关，位2与<code class="fe op oq or nr b">role</code>相关(图2)。<code class="fe op oq or nr b">00000100</code>的十进制值为4，所以空位设置区的第一个字节为4。此外，由于<code class="fe op oq or nr b">role</code>有一个<code class="fe op oq or nr b">None</code>值，它在固定长度值区域中的组只包含零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/018857560472ff811f83795699a3e21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZSuadK49ymQ_N0CDEG9tw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2</p></figure></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="5938" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">创建数据集</strong></h1><p id="8453" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据集是类型为<code class="fe op oq or nr b">T</code>的对象的强类型集合。它也可以被认为是一个包含一些行和命名列的表，其中每一行都是类型为<code class="fe op oq or nr b">T</code>的对象，类型为<code class="fe op oq or nr b">T</code>的字段扮演列的角色(图3)。类型<code class="fe op oq or nr b">T</code>应该在创建数据集之前定义，因此该表中每个字段的类型是已知的。这就是数据集被视为强类型集合的原因。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/d03f81b8234c4ef24000a2f7d66f841c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhLNW_K1fdvhNQ2SI_5Y0w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3</p></figure><h2 id="9b85" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">createDataset()</h2><p id="9e63" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要创建数据集，您需要一个编码器。编码器将类型<code class="fe op oq or nr b">T</code>的JVM对象转换为Spark内部二进制格式，或者从Spark内部二进制格式转换过来。方法<code class="fe op oq or nr b">createDataset()</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L471" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L471" rel="noopener ugc nofollow" target="_blank">SparkSession</a></code>中定义，声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9bf4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> createDataset[T](data: <strong class="nr iu">Seq</strong>[T])(<strong class="nr iu">implicit</strong><em class="oa"> </em>arg0: Encoder[T]): <br/>  Dataset[T]</span></pre><p id="01a1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它从一系列给定类型<code class="fe op oq or nr b">T</code>的数据中创建一个数据集。编码器由<code class="fe op oq or nr b">SparkSession</code>通过<code class="fe op oq or nr b">implicits</code>自动生成；但是，也可以显式创建它。在spark-shell或<a class="ae mm" href="https://docs.databricks.com/notebooks/index.html" rel="noopener ugc nofollow" target="_blank"> Databricks </a>笔记本中，<code class="fe op oq or nr b">SparkSession</code>的一个实例被自动创建并作为变量<code class="fe op oq or nr b">spark</code>提供。否则，我们可以使用<code class="fe op oq or nr b">builder</code>方法创建一个新的会话。<code class="fe op oq or nr b">SparkSession</code>应该是从包org.apache.spark.sql导入的。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b5f1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.SparkSession<br/>spark = SparkSession.builder<br/>  .master("local")<br/>  .appName("nameOfApp")<br/>  .config("spark.some.config.option", "some-value")<br/>  .getOrCreate()</span></pre><p id="3d21" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下是创建数据集的示例:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="80e7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.Encoders</span><span id="b9fc" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Integer</strong>)<br/><strong class="nr iu">val</strong> employeeEncoder = Encoders.product[Employee]<br/><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", 4500),<br/>               Employee("James", "Data engineer", 3200),<br/>               Employee("Laura", "Data scientist", 4100),<br/>               Employee("Ali", "Data engineer", 3200),<br/>               Employee("Steve", "Developer", 3600))<br/><strong class="nr iu">val</strong> staffDS = spark.createDataset(data)(employeeEncoder)<br/>staffDS.show()</span><span id="8ae5" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  4500| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>|  Ali| Data engineer|  3200| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+<br/>staffDS: org.apache.spark.sql.Dataset[Employee] = [name: string, role: string ... 1 more field]</strong></span></pre><p id="51c0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们首先创建案例类<code class="fe op oq or nr b">Employee</code>。所以我们想创建一个类型为<code class="fe op oq or nr b">Employee</code> ( <code class="fe op oq or nr b">T=Employee</code>)的数据集。我们使用文件<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala#L36" rel="noopener ugc nofollow" target="_blank">Encoders.scala</a></code>中的对象<code class="fe op oq or nr b">Encoders</code>为<code class="fe op oq or nr b">Employee</code>创建一个编码器。这个对象提供了一些静态函数，可以用来生成不同类型的编码器。我们使用下面的方法为Scala的<code class="fe op oq or nr b">Product</code>类型(如元组、case类等)获取一个编码器。：</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9f78" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> product[T &lt;: Product : TypeTag]: Encoder[T] = <br/>  ExpressionEncoder()</span></pre><p id="ff0d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它返回前面介绍过的<code class="fe op oq or nr b">ExpressionEncoder</code>类的一个实例。我们也可以直接使用<code class="fe op oq or nr b">ExpressionEncoder</code>类来创建编码器:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9dfe" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<br/><strong class="nr iu">val</strong> employeeEncoder = ExpressionEncoder[Employee]()</span></pre><p id="4cb8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然后我们创建一系列<code class="fe op oq or nr b">Employee </code>对象，并将其分配给<code class="fe op oq or nr b">data</code>。这个序列中的每个对象在我们的数据集中扮演一行的角色。类别<code class="fe op oq or nr b">Employee</code> ( <code class="fe op oq or nr b">name</code>、<code class="fe op oq or nr b">role</code>和<code class="fe op oq or nr b">salary</code>)的每个字段都扮演这个数据集的一个列的角色。最后，我们使用<code class="fe op oq or nr b">data</code>和编码器创建数据集。如输出所示，结果是一个类型为<code class="fe op oq or nr b">Employee</code> ( <code class="fe op oq or nr b">Dataset[Employee]</code>)的数据集。其实就是一个<code class="fe op oq or nr b">Employee</code>对象的集合。这是一个强类型集合，因为应该预先知道<code class="fe op oq or nr b">name</code>、<code class="fe op oq or nr b">role</code>和<code class="fe op oq or nr b">salary</code>的类型。</p><p id="5943" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">可以通过<code class="fe op oq or nr b">implicits</code>自动创建编码器。对象<code class="fe op oq or nr b">implicits</code>在<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L697" rel="noopener ugc nofollow" target="_blank">SparkSession</a></code> ( <code class="fe op oq or nr b">org.apache.spark.sql.SparkSession</code>)类中定义，继承了<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L32" rel="noopener ugc nofollow" target="_blank">抽象类抽象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L32" rel="noopener ugc nofollow" target="_blank">SQLImplicits</a></code>(在org.apache.spark.sql包中)的一些方法，将常见的Scala对象转换成编码器。因此，要创建之前的数据集，我们可以编写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5fd8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Integer</strong>)<br/><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", 4500),<br/>               Employee("James", "Data engineer", 3200),<br/>               Employee("Laura", "Data scientist", 4100),<br/>               Employee("Ali", "Data engineer", 3200),<br/>               Employee("Steve", "Developer", 3600))<br/><strong class="nr iu">val</strong> staffDS = spark.createDataset(data)<br/>staffDS.show()</span></pre><p id="17e1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当<code class="fe op oq or nr b">createDataset()</code>的第二个参数表缺失时，Scala会在<code class="fe op oq or nr b">implicits</code>中寻找一个可以为类型<code class="fe op oq or nr b">T</code>的数据返回<code class="fe op oq or nr b">Encoder[T]</code>的方法。在这种情况下，合适的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L251" rel="noopener ugc nofollow" target="_blank">隐式方法</a>是:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1942" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">implicit def</strong> newProductEncoder[T &lt;: Product : TypeTag]: Encoder[T] =  <br/>  Encoders.product[T]</span></pre><p id="5dbd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以这个方法被编译器用来调用<code class="fe op oq or nr b">createDataset()</code>。如果我们想要手动完成，我们可以编写以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9f6f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.newProductEncoder<br/><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Integer</strong>)<br/><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", 4500),<br/>               Employee("James", "Data engineer", 3200),<br/>               Employee("Laura", "Data scientist", 4100),<br/>               Employee("Ali", "Data engineer", 3200),<br/>               Employee("Steve", "Developer", 3600))</span><span id="ec74" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> staffDS = spark.createDataset(data)(newProductEncoder[Employee])  </span></pre><h2 id="d114" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">托德()</h2><p id="f67a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们还可以使用<code class="fe op oq or nr b">toDS()</code>方法创建一个数据集:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="460b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Integer</strong>)<br/><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", 4500),<br/>               Employee("James", "Data engineer", 3200),<br/>               Employee("Laura", "Data scientist", 4100),<br/>               Employee("Ali", "Data engineer", 3200),<br/>               Employee("Steve", "Developer", 3600))<br/><strong class="nr iu">val</strong> staffDS = data.toDS()<br/>staffDS.show()</span></pre><p id="45b7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">方法<code class="fe op oq or nr b">toDS()</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala#L34" rel="noopener ugc nofollow" target="_blank">案例类</a> <code class="fe op oq or nr b">DatasetHolder</code>中定义。org.apache.spark.sql包中定义的case类是数据集的容器，用于spark中的隐式转换。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8510" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> DatasetHolder[T] private[sql](<strong class="nr iu">private val</strong> ds: Dataset[T]) {   <strong class="nr iu">def</strong> toDS(): Dataset[T] = ds<br/>...</span></pre><p id="8dfe" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">toDS()</code>将一个<code class="fe op oq or nr b">DatasetHolder</code>转换成一个<code class="fe op oq or nr b">Dataset</code>。在我们之前的例子中，<code class="fe op oq or nr b">data</code>是一个Seq对象，我们在其上调用了<code class="fe op oq or nr b">toDS()</code>，这需要一个<code class="fe op oq or nr b">DatasetHolder</code>。所以Spark使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L230" rel="noopener ugc nofollow" target="_blank">隐式方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L230" rel="noopener ugc nofollow" target="_blank">localSeqToDatasetHolder()</a></code>将Seq对象转换为<code class="fe op oq or nr b">DatasetHolder</code>。该方法声明如下:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="42b7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">implicit def</strong> localSeqToDatasetHolder[T : Encoder](s: <strong class="nr iu">Seq</strong>[T]): <br/>  DatasetHolder[T]</span></pre><p id="9e2f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">事实上，如果我们想要手动调用隐式方法，我们可以编写以下代码来获得相同的数据集:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="184a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.localSeqToDatasetHolder<br/><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", 4500),<br/>               Employee("James", "Data engineer", 3200),<br/>               Employee("Laura", "Data scientist", 4100),<br/>               Employee("Ali", "Data engineer", 3200),<br/>               Employee("Steve", "Developer", 3600))</span><span id="af0e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> staffDS = localSeqToDatasetHolder(data)<br/>  .toDS()</span></pre><p id="24e9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然而，我们不需要这样做，因为编译器可以自动为我们这样做。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="be00" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">数据帧</strong></h1><p id="6a44" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">到目前为止，我们已经看到了如何创建类型为<code class="fe op oq or nr b">T</code>的数据集。数据帧是数据集的一种特殊形式。DataFrame是一个类型为<code class="fe op oq or nr b">Row</code>的数据集。trait <code class="fe op oq or nr b">Row</code>在包org.apache.spark.sql的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L142" rel="noopener ugc nofollow" target="_blank">Row.scala</a></code>中定义，代表DataFrame的一行。如果您查看org.apache.spark包中的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L46" rel="noopener ugc nofollow" target="_blank">package.scala</a></code>,您会看到这一行:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4145" class="ms kz it nr b gy nv nw l nx ny"> <strong class="nr iu">type</strong> DataFrame = Dataset[Row]</span></pre><p id="36c6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以在Spark SQL中，<code class="fe op oq or nr b">DataFrame</code> type仅仅是<code class="fe op oq or nr b">Dataset[Row]</code>的类型别名。</p><p id="50dd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">数据帧也可以被认为是一个包含一些行和指定列的表。DataFrame还有一个<em class="oa">模式</em>(图4)，它定义了列名及其数据类型。然而，在创建数据帧时，您不需要定义它的模式。Spark可以根据每一列中的可用数据推断模式，并为每一列分配默认名称。当然，预先定义模式是一个很好的实践，因为对于大型数据文件来说，推断模式的计算代价可能很高，而且有了模式，Spark可以在运行时检测到任何不匹配的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/75a71387c2c51a9dfacf8ba67e82a8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lbXr1yT6PgcB7ODrrsOn6Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4</p></figure><h2 id="5a56" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">排物体</strong></h2><p id="1535" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在Spark中，数据帧的每一行都是一个通用的<code class="fe op oq or nr b">Row</code>对象。它在org.apache.spark.sql包中的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L44" rel="noopener ugc nofollow" target="_blank">Row.scala</a></code>中定义。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ff49" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">object</strong> Row {<br/>  ...  <br/>  <strong class="nr iu">def</strong> apply(values: <strong class="nr iu">Any*</strong>): Row = <strong class="nr iu">new</strong> GenericRow(values.toArray)<br/>  ...</span></pre><p id="9baf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这是同一文件中<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L142" rel="noopener ugc nofollow" target="_blank">特质</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L142" rel="noopener ugc nofollow" target="_blank">Row</a></code>的伴随对象。工厂方法创建一个新的<code class="fe op oq or nr b">GenericRow</code>对象。<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/rows.scala#L166" rel="noopener ugc nofollow" target="_blank">GenericRow</a></code>扩展了<code class="fe op oq or nr b">Row</code>特征并实现了它的方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6dcc" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">class</strong> GenericRow(<strong class="nr iu">protected</strong>[sql] <strong class="nr iu">val</strong> values: <strong class="nr iu">Array</strong>[Any]) <strong class="nr iu">extends</strong> Row {  <br/>  ...<br/>  <strong class="nr iu">override</strong> <strong class="nr iu">def</strong> get(i: <strong class="nr iu">Int</strong>): Any = values(i)<br/>  ...</span></pre><p id="031a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">GenericRow</code>的数据是<code class="fe op oq or nr b">Any</code>的数组。所以<code class="fe op oq or nr b">Row</code>是类型<code class="fe op oq or nr b">Any</code>的有序字段集合，可以通过从0开始的索引来访问。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6201" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.Row<br/><strong class="nr iu">val</strong> row = Row("John", "Data scientist", 4500)<br/>row(2)</span><span id="aa68" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">// Output:<br/>row: org.apache.spark.sql.Row = [John, Data scientist,4500]<br/>res0: Any = 4500</strong></span></pre><p id="056d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您可以使用带有索引的<code class="fe op oq or nr b">getAs()</code>来获得具有适当类型的行字段。<code class="fe op oq or nr b">getAs()</code>在<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L358" rel="noopener ugc nofollow" target="_blank">Row.scala</a></code>中定义，稍后将详细讨论:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2cda" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> getAs[T](i: Int): T = get(i).asInstanceOf[T]</span></pre><p id="5f1f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将<code class="fe op oq or nr b">get(i)</code>的返回值强制转换为类型<code class="fe op oq or nr b">T</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="361b" class="ms kz it nr b gy nv nw l nx ny">row.getAs[String](0)</span><span id="ab67" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">// Output:<br/>res0: String = John</strong></span></pre><h2 id="a522" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">模式</strong></h2><p id="047b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据帧的模式可以用两种方式定义:使用<code class="fe op oq or nr b">StructType</code>或使用DLL字符串。<code class="fe op oq or nr b">StructType</code>是内置的数据类型，它是<code class="fe op oq or nr b">StructField</code>的集合。每个<code class="fe op oq or nr b">StructType</code>用于为数据帧的一列定义模式。这些类型是从包<a class="ae mm" href="https://github.com/apache/spark/tree/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/types" rel="noopener ugc nofollow" target="_blank">org . Apache . spark . SQL . types</a><code class="fe op oq or nr b">.</code>中导入的。例如，我们可以为一个DataFrame定义一个模式，它有三列，分别为:<code class="fe op oq or nr b">name</code>、<code class="fe op oq or nr b">role</code>和<code class="fe op oq or nr b">salary</code>，其中这些列的数据类型分别为<code class="fe op oq or nr b">String</code>、<code class="fe op oq or nr b">String</code>和<code class="fe op oq or nr b">Integer</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4dfb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> someSchema = StructType(<strong class="nr iu">List</strong>(<br/>  StructField("name", StringType, true),<br/>  StructField("role", StringType, true),<br/>  StructField("salary", IntegerType, true))<br/>)</span></pre><p id="f89e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用数据定义语言(DDL)字符串以更简单的方式创建相同的模式:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bb7f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> someSchema = "`name` <strong class="nr iu">STRING</strong>, `role` <strong class="nr iu">STRING</strong>, `salary` <strong class="nr iu">INTEGER</strong>"</span></pre><p id="fa60" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们将在后面讨论DataFrame列。请注意，数据集也有一个模式，但是我们不能为此定义一个模式。从类型<code class="fe op oq or nr b">T</code>推断出<code class="fe op oq or nr b">Dataset[T]</code>的模式。如下所示，在使用方法<code class="fe op oq or nr b">schma</code>和<code class="fe op oq or nr b">printSchema()</code>创建数据集(或数据帧)之后，您可以看到它的模式。</p><h2 id="112d" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak"> RowEncoder </strong></h2><p id="bc26" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据帧也需要编码器。对象<code class="fe op oq or nr b">RowEncoder</code>充当数据帧的编码器。它在org . Apache . spark . SQL . catalyst . encoders包中的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala#L62" rel="noopener ugc nofollow" target="_blank">RowEncoder.scala</a></code>中定义。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4f58" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">object</strong> RowEncoder {  <br/>  <strong class="nr iu">def</strong> apply(schema: StructType): ExpressionEncoder[Row] = { ...</span></pre><p id="a211" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如您所见，它的工厂方法接受一个模式并创建一个类型为<code class="fe op oq or nr b">Row</code>的<code class="fe op oq or nr b">ExpressionEncoder</code>。这是<code class="fe op oq or nr b">RowEncoder</code>的一个重要特性，它允许你为一个数据帧定义一个模式。我们提到过一个<code class="fe op oq or nr b">Row</code>对象的字段属于<code class="fe op oq or nr b">Any</code>类型。但是这只适用于作为JVM对象的<code class="fe op oq or nr b">Row</code>对象。当<code class="fe op oq or nr b">RowEncoder</code>想要在运行时编码一个<code class="fe op oq or nr b">Row</code>对象时，它需要知道字段的实际类型。</p><p id="241b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">事实上，它不能将类型为<code class="fe op oq or nr b">Any</code>的字段序列化为内部二进制格式，并在这种情况下给出一个运行时错误(我们将在后面给出一个例子)。此外，它可以检查数据类型是否与给定的(或推断的)模式匹配。例如，如果模式说一个列有一个<code class="fe op oq or nr b">Integer</code>类型，并且为该列的数据给出了一个<code class="fe op oq or nr b">String</code>，那么<code class="fe op oq or nr b">RowEncoder</code>将给出一个运行时错误(我们将在后面给出一个例子)。当一个<code class="fe op oq or nr b">Row</code>对象的内部二进制格式被反序列化回JVM对象时，字段将再次具有类型<code class="fe op oq or nr b">Any</code>。所以当它是一个JVM对象时,<code class="fe op oq or nr b">Row</code>的字段具有类型<code class="fe op oq or nr b">Any</code>。</p><p id="1d2a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">但是为什么<code class="fe op oq or nr b">RowEncoder</code>需要知道<code class="fe op oq or nr b">Row</code>对象中字段的类型呢？记住，变量的类型改变了它在<code class="fe op oq or nr b">UnsafeRow</code>的字节数组中的存储方式。假设您有一个类型为<code class="fe op oq or nr b">Any</code>的字段，其值为7。如果7是一个整数，它的值将存储在固定长度值区域的一个组中。如果7是一个字符串(<code class="fe op oq or nr b">"7"</code>)，那么它的ASCII码将存储在变长值区域。因此，如果不知道字段的确切类型，编码器就不知道如何将其转换为内部二进制格式。</p><h2 id="e77e" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">从数据集创建数据帧</strong></h2><p id="b5d7" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如前所述，DataFrame是一个类型为<code class="fe op oq or nr b">Row</code>的数据集。因此，如果我们使用一系列的<code class="fe op oq or nr b">Row</code>对象和<code class="fe op oq or nr b">RowEncoder</code>，我们可以从数据集创建一个数据帧。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ae47" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.encoders.RowEncoder<br/><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> someSchema = StructType(<strong class="nr iu">List</strong>(<br/>  StructField("name", StringType, true),<br/>  StructField("role", StringType, true),<br/>  StructField("salary", IntegerType, true))<br/>)<br/><strong class="nr iu">val</strong> encoder = RowEncoder(someSchema)</span><span id="eef8" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Row("John", "Data scientist", 4500),<br/>               Row("James", "Data engineer", 3200),<br/>               Row("Laura", "Data scientist", 4100),<br/>               Row("Ali", "Data engineer", 3200),<br/>               Row("Steve", "Developer", 3600))<br/><strong class="nr iu">val</strong> staffDS = spark.createDataset(data)(encoder)</span><span id="2fff" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>staffDS: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, role: string ... 1 more field]</strong></span></pre><p id="15d0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里的输出是一个<code class="fe op oq or nr b">Dataset[Row]</code>，相当于一个数据帧。所以我们可以将<code class="fe op oq or nr b">staffDS</code>赋给一个<code class="fe op oq or nr b">DataFrame</code>类型的变量。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1069" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> newStaffDF: org.apache.spark.sql.DataFrame = staffDS</span><span id="423f" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>newStaffDF: org.apache.spark.sql.DataFrame = [name: string, role: string ... 1 more field]</strong></span></pre><p id="7825" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如您所见，我们没有创建<code class="fe op oq or nr b">Employee</code> case类，而是使用模式来定义DataFrame的列。为了创建一个数据帧，我们需要一个序列和一个模式。然而，模式是不必要的，因为它可以被推断出来。</p><h2 id="c74c" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">从头开始创建数据帧</strong></h2><p id="8069" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> createDataFrame() </strong></p><p id="23cd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">Spark有一些特殊的方法来创建数据帧。为此，我们可以使用<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L350" rel="noopener ugc nofollow" target="_blank">SparkSession</a></code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L350" rel="noopener ugc nofollow" target="_blank">类</a>中的方法<code class="fe op oq or nr b">createDataFrame()</code>。它被定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a513" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = withActive {    <br/>  <strong class="nr iu">val</strong> replaced = CharVarcharUtils.failIfHasCharVarchar(schema)<br/>    .asInstanceOf[StructType]   <br/>  <strong class="nr iu">val</strong> encoder = RowEncoder(replaced)<br/>  <strong class="nr iu">val</strong> toRow = encoder.createSerializer()    <br/>  <strong class="nr iu">val</strong> catalystRows = rowRDD.map(toRow)     <br/>  internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema)  }</span></pre><p id="5f7f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们需要向该方法传递一个RDD和一个模式来创建数据帧。如您所见，它使用带有给定<code class="fe op oq or nr b">schema</code>的<code class="fe op oq or nr b">RowEncoder</code>来编码数据。让我们看一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1f16" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> someData = <strong class="nr iu">Seq</strong>(<br/>  Row("John", "Data scientist", 4500),<br/>  Row("James", "Data engineer", 3200),<br/>  Row("Laura", "Data scientist", 4100),<br/>  Row("Ali", "Data engineer", 3200),<br/>  Row("Steve", "Developer", 3600)<br/>)</span><span id="9d51" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> someSchema = StructType(List(<br/>  StructField("name", StringType, true),<br/>  StructField("role", StringType, true),<br/>  StructField("salary", IntegerType, true))<br/>)</span><span id="87d9" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> staffDF = spark.createDataFrame(<br/>  spark.sparkContext.parallelize(someData),<br/>  someSchema<br/>)</span><span id="c394" class="ms kz it nr b gy nz nw l nx ny">staffDF.show()</span><span id="704e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+<br/>| name|          role|salary|<br/>+-----+--------------+------+<br/>| John|Data scientist|  4500|<br/>|James| Data engineer|  3200|<br/>|Laura|Data scientist|  4100|<br/>|  Ali| Data engineer|  3200|<br/>|Steve|     Developer|  3600|<br/>+-----+--------------+------+</strong></span></pre><p id="d15f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用DDL模式创建相同的数据帧:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b941" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> someData = <strong class="nr iu">Seq</strong>(<br/>  Row("John", "Data scientist", 4500),<br/>  Row("James", "Data engineer", 3200),<br/>  Row("Laura", "Data scientist", 4100),<br/>  Row("Ali", "Data engineer", 3200),<br/>  Row("Steve", "Developer", 3600)<br/>)</span><span id="548d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> someSchema = "`name` <strong class="nr iu">STRING</strong>, `role` <strong class="nr iu">STRING</strong>, `salary` <strong class="nr iu">INTEGER</strong>"</span><span id="8977" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> df = spark.createDataFrame(<br/>  spark.sparkContext.parallelize(someData),<br/>  StructType.fromDDL(someSchema)<br/>)</span></pre><p id="4875" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，我们不需要定义模式，因为它可以被推断出来。我们也可以使用不带模式的Seq对象，通过使用<code class="fe op oq or nr b">createDataFrame()</code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L312" rel="noopener ugc nofollow" target="_blank">重载版本</a>来创建数据帧:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="188d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> createDataFrame[A &lt;: Product : TypeTag](data: <strong class="nr iu">Seq</strong>[A]): DataFrame </span></pre><p id="b47b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">需要注意的是，您不能将一系列的<code class="fe op oq or nr b">Row</code>对象传递给这个版本的<code class="fe op oq or nr b">createDataFrame()</code>。该方法将一个<code class="fe op oq or nr b">A</code>的Seq作为<code class="fe op oq or nr b">scala.Product</code>的子类型，包含元组和case类，但<code class="fe op oq or nr b">Row</code>不是<code class="fe op oq or nr b">scala.Product</code>的子类型。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="797c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> someData = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>)</span><span id="473f" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> staffDF = spark.createDataFrame(someData)<br/>staffDF.show()</span><span id="b139" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+----+ <br/>|   _1|            _2|  _3| <br/>+-----+--------------+----+ <br/>| John|Data scientist|4500| <br/>|James| Data engineer|3200| <br/>|Laura|Data scientist|4100| <br/>|  Ali| Data engineer|3200| <br/>|Steve|     Developer|3600| <br/>+-----+--------------+----+</strong></span></pre><p id="74cc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这里，Spark自动为每一列命名。</p><p id="1ab2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> toDF() </strong></p><p id="b030" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">case类<code class="fe op oq or nr b">DatasetHolder</code>中的方法<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala#L44" rel="noopener ugc nofollow" target="_blank">toDF()</a></code>将<code class="fe op oq or nr b">DatasetHolder</code>对象转换为DataFrame:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="881b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> toDF(colNames: <strong class="nr iu">String*</strong>): DataFrame = ds.toDF(colNames : <strong class="nr iu">_*</strong>)</span></pre><p id="fa0a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">类似于<code class="fe op oq or nr b">toDS()</code>；但是，它可以接受列名列表。所以我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f00f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits._<br/><strong class="nr iu">val</strong> staffDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>  ).toDF("name", "role", "salary")</span></pre><p id="e042" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们需要导入对象<code class="fe op oq or nr b">implicits</code>，因为我们将<code class="fe op oq or nr b">toDF()</code>应用于一个序列对象，而不是一个<code class="fe op oq or nr b">DatasetHolder</code>。所以我们需要一个隐式的方法将序列对象转换成一个<code class="fe op oq or nr b">DatasetHolder</code>。如前所述，Spark使用对象<code class="fe op oq or nr b">implicits</code>中的方法<code class="fe op oq or nr b">localSeqToDatasetHolder()</code>将序列转换为<code class="fe op oq or nr b">DatasetHolder</code>。它返回一个类型为<code class="fe op oq or nr b">T</code>的<code class="fe op oq or nr b">DatasetHolder</code>。因此，如果我们想手动使用隐式方法，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b9fe" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.localSeqToDatasetHolder<br/><strong class="nr iu">val</strong> staffSeq = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>  )</span><span id="a00b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> staffDF = localSeqToDatasetHolder(staffSeq)<br/>  .toDF("name", "role", "salary")</span></pre><p id="160e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然而，编译器可以自动完成这项工作。数据帧列的名称作为<code class="fe op oq or nr b">toDF()</code>的参数给出；然而，我们也可以忽略它们。在这种情况下，Spark会自动为每一列命名:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="23d0" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> staffDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),  <br/>  ("James", "Data engineer", 3200), <br/>  ("Laura", "Data scientist", 4100), <br/>  ("Ali", "Data engineer", 3200),  <br/>  ("Steve", "Developer", 3600)  <br/>  ).toDF()</span><span id="5fc7" class="ms kz it nr b gy nz nw l nx ny">staffDF.show()</span><span id="a64a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+----+<br/>|_1   |_2            |_3  |<br/>+-----+--------------+----+<br/>|John |Data scientist|4500|<br/>|James|Data engineer |3200|<br/>|Laura|Data scientist|4100|<br/>|Ali  |Data engineer |3200|<br/>|Steve|Developer     |3600|<br/>+-----+--------------+----+</strong></span></pre><p id="2b82" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用<code class="fe op oq or nr b">_*</code>操作符从sequence对象中获取列名:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3fa7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> colList = <strong class="nr iu">List</strong>("name", "role", "salary")<br/><strong class="nr iu">val</strong> staffDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),  <br/>  ("James", "Data engineer", 3200), <br/>  ("Laura", "Data scientist", 4100), <br/>  ("Ali", "Data engineer", 3200),  <br/>  ("Steve", "Developer", 3600)  <br/>  ).toDF(colList : <strong class="nr iu">_*</strong>)</span></pre><h2 id="bf72" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">使用案例类</strong>创建数据框架 <strong class="ak"/></h2><p id="a497" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们也可以使用case类来创建一个数据框架。case类<code class="fe op oq or nr b">Employee</code>将获取数据帧的每一行。我们在Spark会话中使用方法<code class="fe op oq or nr b">createDataFrame()</code>将一系列案例类转换成数据帧。由于使用了case类，我们不再需要模式。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a73e" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.DataFrame</span><span id="e48e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Option</strong>[<strong class="nr iu">Integer</strong>])<br/><strong class="nr iu">val</strong> staffWithCaseDF: DataFrame = spark.createDataFrame(<br/>    <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", Some(4500)),<br/>        Employee("James", "Data engineer", None),<br/>        Employee("Laura", "Data scientist", Some(4100)),<br/>        Employee("Ali", "Data engineer", Some(3200)),<br/>        Employee("Steve", "Developer", None)<br/>        )) </span><span id="a1bd" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+<br/>| name|          role|salary|<br/>+-----+--------------+------+<br/>| John|Data scientist|  4500|<br/>|James| Data engineer|  null|<br/>|Laura|Data scientist|  4100|<br/>|  Ali| Data engineer|  3200|<br/>|Steve|     Developer|  null|<br/>+-----+--------------+------+</strong></span></pre><p id="0ab1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用<code class="fe op oq or nr b">toDF()</code>从一个案例类中创建数据帧:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d026" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> staffWithCaseDF = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", 4500),<br/>    Employee("James", "Data engineer", 3200),<br/>    Employee("Laura", "Data scientist", 4100),<br/>    Employee("Ali", "Data engineer", 3200),<br/>    Employee("Steve", "Developer", 3600)).toDF()</span></pre><h2 id="7658" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">数据帧与数据集</strong></h2><p id="75d7" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据集是强类型集合。在数据集中的每一行，字段都有特定的类型。然而，数据帧的每一行都是一个通用的<code class="fe op oq or nr b">Row</code>对象。<code class="fe op oq or nr b">Row</code>对象的每个字段都属于<code class="fe op oq or nr b">Any</code>类型。因此它们的行为类似于非类型化字段。(在Scala中，当变量的类型未知时，可以使用<code class="fe op oq or nr b">Any</code>来解决这个问题。)假设我们想从案例类<code class="fe op oq or nr b">Employee</code>中创建一个数据集，而我们在<code class="fe op oq or nr b">data</code>中有一个类型错误。例如，我们不用<code class="fe op oq or nr b">Integer</code>来表示<code class="fe op oq or nr b">salary</code>，而是用<code class="fe op oq or nr b">String</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1fc9" class="ms kz it nr b gy nv nw l nx ny">// This gives a compile-time error<br/><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Integer</strong>)<br/><strong class="nr iu">val</strong> data = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", "4500"),<br/>               Employee("James", "Data engineer", 3200),<br/>               Employee("Laura", "Data scientist", 4100),<br/>               Employee("Ali", "Data engineer", 3200),<br/>               Employee("Steve", "Developer", 3600))<br/><strong class="nr iu">val</strong> staffDS = spark.createDataset(data)</span><span id="6f2c" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>command-3975502924555582:3: error: type mismatch;  found   : String("4500")  required: Integer val data = Seq(Employee("John", "Data scientist", "4500"),</strong></span></pre><p id="59f7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这种情况下，Spark会给我们一个编译时错误。原因是在数据集中，一行中所有字段的类型在编译时都是已知的，所以编译器知道您需要一个整数作为<code class="fe op oq or nr b">salary</code>。现在假设我们在数据帧上犯了同样的错误:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3775" class="ms kz it nr b gy nv nw l nx ny">// This gives a run-time error<br/><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> someData = <strong class="nr iu">Seq</strong>(<br/>  Row("John", "Data scientist", "4500"),<br/>  Row("James", "Data engineer", 3200),<br/>  Row("Laura", "Data scientist", 4100),<br/>  Row("Ali", "Data engineer", 3200),<br/>  Row("Steve", "Developer", 3600)<br/>)</span><span id="3087" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> someSchema = StructType(<strong class="nr iu">List</strong>(<br/>  StructField("name", StringType, true),<br/>  StructField("role", StringType, true),<br/>  StructField("salary", IntegerType, true))<br/>)</span><span id="d5db" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> staffDF = spark.createDataFrame(<br/>   spark.sparkContext.parallelize(someData), someSchema)</span><span id="f8a1" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 677.0 failed 4 times, most recent failure: Lost task 2.3 in stage 677.0 (TID 111572, 10.139.64.9, executor 20): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int</strong></span></pre><p id="9908" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在Spark不能给我们一个编译时错误。只有在运行时才会检测到错误。原因是<code class="fe op oq or nr b">Row</code>对象中的字段属于类型<code class="fe op oq or nr b">Any</code>。所以你可以传递任何东西给它。现在记住，<code class="fe op oq or nr b">RowEncoder</code>将模式作为参数，因此在运行时，当<code class="fe op oq or nr b">RowEncoder</code>试图编码<code class="fe op oq or nr b">Row</code>对象时，它将检测到错误。模式将<code class="fe op oq or nr b">salary</code>定义为一个整数，但是<code class="fe op oq or nr b">RowEncoder</code>找到一个字符串并抛出一个错误。</p><p id="fd1e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如您在后面看到的，大多数操作数据集的方法都返回数据帧，所以如果您需要类似于SQL查询的关系转换，您应该使用数据帧。DataFrames还提供跨Spark组件的代码优化和API简化。另一方面，数据集提供编译时类型安全，当您使用数据集时，您可以受益于钨与编码器的高效序列化。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="6ce0" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">显示数据帧</strong></h1><h2 id="9636" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">显示()</h2><p id="0d78" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了显示Dataframe，我们可以使用类<code class="fe op oq or nr b">Dataset</code>中的方法<code class="fe op oq or nr b">show()</code>，它在前面的代码清单中使用过。该方法在org.apache.spark.sql包中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L824" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L824" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c9eb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> show(numRows: <strong class="nr iu">Int</strong>, truncate: <strong class="nr iu">Boolean</strong>): <strong class="nr iu">Unit</strong></span></pre><p id="149b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L784" rel="noopener ugc nofollow" target="_blank">numRows</a></code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L784" rel="noopener ugc nofollow" target="_blank">和</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L784" rel="noopener ugc nofollow" target="_blank">truncate</a></code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L784" rel="noopener ugc nofollow" target="_blank">默认值分别为20和<code class="fe op oq or nr b">true</code>:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="031e" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> show(numRows: <strong class="nr iu">Int</strong>): <strong class="nr iu">Unit</strong> = show(numRows, truncate = true)<br/><strong class="nr iu">def</strong> show(): <strong class="nr iu">Unit</strong> = show(20)<br/><strong class="nr iu">def</strong> show(truncate: <strong class="nr iu">Boolean</strong>): <strong class="nr iu">Unit</strong> = show(20, truncate)</span></pre><p id="df89" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">numRows</code>控制应该显示多少行，默认情况下，它以表格形式显示数据集或数据帧的前20行。例如，为了只显示<code class="fe op oq or nr b">staffDF</code>的前三行，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f537" class="ms kz it nr b gy nv nw l nx ny">staffDF.show(3)</span><span id="e85b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  4500| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>+-----+--------------+------+<br/> only showing top 3 rows</strong></span></pre><p id="441c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果一列超过20个字符，默认情况下将被截断。为了防止截断，我们可以将<code class="fe op oq or nr b">truncate</code>设置为<code class="fe op oq or nr b">false</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cc57" class="ms kz it nr b gy nv nw l nx ny">// Columns won't be truncated<br/>staffDF<!-- -->.show(false) </span></pre><h2 id="eabb" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">显示()</h2><p id="961e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在Databricks笔记本中，我们还可以使用功能显示来可视化数据帧:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="57e5" class="ms kz it nr b gy nv nw l nx ny">display(staffDF)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d96928f6faf235e4fc9b2d24911fdcbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkGA6SXpAN8mwNw3h-xbmg.png"/></div></div></figure><p id="ea55" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">需要注意的是，在Spark中，与Python中的Pandas库不同，您不能通过简单地键入数据帧的名称来显示其内容:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7fd3" class="ms kz it nr b gy nv nw l nx ny">staffDF</span><span id="8145" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res4: org.apache.spark.sql.DataFrame = [name: string, role: string ... 1 more field]</strong></span></pre><p id="1b3f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">另外，<code class="fe op oq or nr b">show()</code>的返回类型是<code class="fe op oq or nr b">Unit()</code>。所以不能将<code class="fe op oq or nr b">show()</code>的返回值赋给另一个数据帧。因此，如果我们编写以下代码，它不会将<code class="fe op oq or nr b">staffDF</code>复制到<code class="fe op oq or nr b">newDF</code>，并且<code class="fe op oq or nr b">newDF</code>的值将是<code class="fe op oq or nr b">()</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ed5c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> newDF = staffDF.show()</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="fcc5" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">导入数据帧</strong></h1><p id="2175" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">org.apache.spark.sql包中的类<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala" rel="noopener ugc nofollow" target="_blank">DataFrameReader</a></code>可用于从外部资源加载DataFrame。这个类有一个私有构造函数，只能在org.apache.spark.sql包中访问。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="01be" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">class</strong> DataFrameReader <strong class="nr iu">private</strong>[sql](sparkSession: SparkSession) <br/>  <strong class="nr iu">extends</strong> Logging { ...</span></pre><p id="cd52" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以您不能在这个包之外创建一个<code class="fe op oq or nr b">DataFrameReader</code>的实例。</p><h2 id="9f56" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">阅读()</h2><p id="950e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要获得它的实例句柄，应该使用类<code class="fe op oq or nr b">SparkSession</code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L655" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L655" rel="noopener ugc nofollow" target="_blank">read()</a></code>。这个方法返回一个句柄给<code class="fe op oq or nr b">DataFrameReader</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="480a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> read: DataFrameReader = <strong class="nr iu">new</strong> DataFrameReader(self)</span></pre><p id="8393" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">DataFrameReader</code>有一些装载数据的公共方法:</p><ul class=""><li id="117e" class="ob oc it ls b lt mn lw mo lz od md oe mh of ml og oh oi oj bi translated"><code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L63" rel="noopener ugc nofollow" target="_blank">format()</a></code>:该方法控制外部资源的类型，声明如下:</li></ul><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="845d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> format(source: <strong class="nr iu">String</strong>): DataFrameReader</span></pre><p id="eee5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">参数<code class="fe op oq or nr b">source</code>可以是<code class="fe op oq or nr b">"parquet"</code>、<code class="fe op oq or nr b">"csv"</code>、<code class="fe op oq or nr b">"txt"</code>、<code class="fe op oq or nr b">"json"</code>等。如果你没有指定这个方法，那么默认是<code class="fe op oq or nr b">parquet</code>或者在<code class="fe op oq or nr b">spark.sql.sources.default</code>中设置的任何东西。</p><ul class=""><li id="361a" class="ob oc it ls b lt mn lw mo lz od md oe mh of ml og oh oi oj bi translated"><code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L124" rel="noopener ugc nofollow" target="_blank">option()</a></code>:为底层数据源增加一个输入选项，声明如下:</li></ul><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="eafa" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> option(key: <strong class="nr iu">String</strong>, value: <strong class="nr iu">String</strong>): DataFrameReader</span></pre><p id="1dd0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它需要一个键和一个值。<code class="fe op oq or nr b">value</code>代表一个布尔值，在<code class="fe op oq or nr b">option()</code>的重载版本中也可以是<code class="fe op oq or nr b">boolean</code>、<code class="fe op oq or nr b">long</code>或<code class="fe op oq or nr b">double</code>。您可以在文档中找到所有可在<code class="fe op oq or nr b">option()</code>中使用的<a class="ae mm" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html" rel="noopener ugc nofollow" target="_blank">键列表。<code class="fe op oq or nr b">option()</code>使用的密钥可以特定于一种文件类型(如CSV)。</a></p><ul class=""><li id="76f6" class="ob oc it ls b lt mn lw mo lz od md oe mh of ml og oh oi oj bi translated"><code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L235" rel="noopener ugc nofollow" target="_blank">load()</a></code>:取数据源的路径，声明如下:</li></ul><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e30c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> load(path: <strong class="nr iu">String</strong>): DataFrame</span></pre><p id="c314" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，要读取CSV文件，我们可以编写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="45ba" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> csvFile = "databricks-datasets/learning-spark- <br/>  v2/mnm_dataset.csv"</span><span id="e93e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> df = spark.read.format("csv")<br/>  .option("inferSchema", "true")<br/>  .option("header", "true")<br/>  .load(csvFile)<br/>df.show(3)</span><span id="6527" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-----+-----+ <br/>|State|Color|Count| <br/>+-----+-----+-----+ <br/>|   TX|  Red|   20| <br/>|   NV| Blue|   66| <br/>|   CO| Blue|   79| <br/>+-----+-----+-----+ <br/>only showing top 3 rows</strong></span></pre><p id="f238" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们将文件路径放在<code class="fe op oq or nr b">csvFile</code>中。Spark session的<code class="fe op oq or nr b">read()</code>方法用于获得<code class="fe op oq or nr b">DataFrameReader</code>的实例句柄。然后我们在<code class="fe op oq or nr b">format()</code>中确定资源的类型。当<code class="fe op oq or nr b">infershema</code>设置为<code class="fe op oq or nr b">true</code>时，从数据中自动推断输入模式。当<code class="fe op oq or nr b">header</code>设置为<code class="fe op oq or nr b">true</code>时，文件的第一行将用于命名列，不会包含在数据中。最后，我们使用文件路径加载数据。由于<code class="fe op oq or nr b">option()</code>和<code class="fe op oq or nr b">format()</code>都返回<code class="fe op oq or nr b">DataFrameReader</code>，它们的顺序并不重要；然而，<code class="fe op oq or nr b">load()</code>应该总是在最后，因为它返回一个数据帧。</p><p id="a3cb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以加载一个拼花文件作为另一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1e83" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> filePath = "databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet"</span><span id="546d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> df = spark.read.load(filePath)</span></pre><p id="1e0b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们不需要写<code class="fe op oq or nr b">format()</code>,因为默认的源是Parquet。类<code class="fe op oq or nr b">DataFrameReader</code>有一些其他方法来加载特定类型的文件，可以用来代替<code class="fe op oq or nr b">load()</code>。例如，要加载前面的Parquet文件，我们还可以编写以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="aca0" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df = spark.read.parquet(filePath)</span></pre><p id="595c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">方法<code class="fe op oq or nr b">parquet()</code>获取文件路径并返回一个数据帧。我们可以使用<code class="fe op oq or nr b">csv()</code>、<code class="fe op oq or nr b">json()</code>等方法。，以加载其他类型的文件。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="0bff" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">显示模式</strong></h1><h2 id="ea14" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">printSchema()</h2><p id="58a2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">类<code class="fe op oq or nr b">Dataset</code>有两个方法来显示模式。方法<code class="fe op oq or nr b">printSchema()</code>以漂亮的树格式将模式打印到控制台。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3eeb" class="ms kz it nr b gy nv nw l nx ny">staffDF.printSchema()</span><span id="221e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>root  <br/> |-- name: string (nullable = true)  <br/> |-- role: string (nullable = true)  <br/> |-- salary: integer (nullable = true)</strong></span></pre><h2 id="cfbd" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">(计划或理论的)纲要</h2><p id="56d2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8e14" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> schema: StructType</span></pre><p id="3fe1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在类<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L510" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中，将数据集的模式作为<code class="fe op oq or nr b">StructType</code>对象返回。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c990" class="ms kz it nr b gy nv nw l nx ny">println(staffDF.schema)</span><span id="4c2b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>StructType(StructField(name,StringType,true), StructField(role,StringType,true), StructField(salary,IntegerType,true))</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="4a33" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">数据帧的形状</strong></h1><h2 id="848c" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">计数()</h2><p id="abe7" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7e6a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> count(): Long</span></pre><p id="9cb9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">属于<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3005" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3005" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>并返回Dataset或DataFrame中的行数。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6bd6" class="ms kz it nr b gy nv nw l nx ny">println(staffDF.count)</span><span id="b8d1" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>5</strong></span></pre><p id="ee49" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，要显示数据帧的所有行，我们可以编写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e3e2" class="ms kz it nr b gy nv nw l nx ny">staffDF.show(staffDF.count.toInt)</span></pre><p id="c418" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，<code class="fe op oq or nr b">show()</code>中<code class="fe op oq or nr b">numRows</code>的类型是<code class="fe op oq or nr b">Int</code>，但是<code class="fe op oq or nr b">count()</code>返回一个<code class="fe op oq or nr b">Long</code>，所以我们需要使用<code class="fe op oq or nr b">toInt()</code>方法将其转换为<code class="fe op oq or nr b">Int</code>。</p><h2 id="78f6" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">列</h2><p id="d73c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="59e9" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> columns: <strong class="nr iu">Array</strong>[<strong class="nr iu">String</strong>]</span></pre><p id="c88d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L597" rel="noopener ugc nofollow" target="_blank">类中</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L597" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>以数组形式返回所有列名。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c9e6" class="ms kz it nr b gy nv nw l nx ny">staffDF.columns</span><span id="84a8" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Ouput<br/>res10: Array[String] = Array(name, role, salary)</strong></span></pre><p id="73d2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因此，要获得数据帧中的列数，我们可以编写以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0552" class="ms kz it nr b gy nv nw l nx ny">print(staffDF.columns.size)</span><span id="68e0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>3</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="f925" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">显示行</strong></h1><h2 id="1132" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">头部()</h2><p id="c196" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f309" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> <!-- -->head(n: <strong class="nr iu">Int</strong>)<!-- -->: <strong class="nr iu">Array</strong>[T]</span></pre><p id="16c4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">in <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2722" rel="noopener ugc nofollow" target="_blank"> class </a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2722" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>以数组形式返回Dataset或DataFrame的前<code class="fe op oq or nr b">n</code>行。 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2729" rel="noopener ugc nofollow" target="_blank">n</a></code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2729" rel="noopener ugc nofollow" target="_blank">的</a><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2729" rel="noopener ugc nofollow" target="_blank">默认值为1 </a>。所以为了显示<code class="fe op oq or nr b">staffDF</code>的前三行，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="760d" class="ms kz it nr b gy nv nw l nx ny">staffDF.head(3)</span><span id="5567" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res4: Array[org.apache.spark.sql.Row] = Array([John,Data scientist,4500], [James,Data engineer,3200], [Laura,Data scientist,4100])</strong></span></pre><h2 id="8d44" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">采取()</h2><p id="8519" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们也可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2929" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2929" rel="noopener ugc nofollow" target="_blank">take()</a></code>得到同样的结果:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4a90" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> take(n: <strong class="nr iu">Int</strong>): Array[T] = head(n)</span></pre><h2 id="2f5f" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">首先()</h2><p id="7ba8" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了得到第一行，我们也可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2736" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2736" rel="noopener ugc nofollow" target="_blank">first()</a></code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8cb8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> first(): T = head()</span></pre><h2 id="926b" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">尾部()</h2><p id="8efa" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2940" rel="noopener ugc nofollow" target="_blank">下面的方法</a>返回Dataset或DataFrame中最后的<code class="fe op oq or nr b">n</code>行。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d3a6" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> tail(n: <strong class="nr iu">Int</strong>): Array[T]</span></pre><h2 id="c6b8" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">极限()</h2><p id="8406" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">所有这些方法(<code class="fe op oq or nr b">head()</code>、<code class="fe op oq or nr b">first()</code>、<code class="fe op oq or nr b">take()</code>、<code class="fe op oq or nr b">tail()</code>)都返回一行或一组行。如果需要返回一个DataFrame，可以使用<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1959" rel="noopener ugc nofollow" target="_blank">limit()</a></code>。该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6855" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> limit(n: <strong class="nr iu">Int</strong>): Dataset[T]</span></pre><p id="0a6a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">通过获取第一个<code class="fe op oq or nr b">n</code>行返回新的数据集。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ea80" class="ms kz it nr b gy nv nw l nx ny">staffDF.limit(3).show()</span><span id="f531" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  4500| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>+-----+--------------+------+</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="28a7" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">计算统计数据</strong></h1><h2 id="2202" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">描述()</h2><p id="1bbc" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2647" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2647" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的以下方法计算数字和字符串列的基本统计数据，包括计数、平均值、标准差、最小值和最大值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5f8f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> describe(cols: <strong class="nr iu">String*</strong>)<!-- -->: DataFrame</span></pre><p id="66ea" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它可用于探索性数据分析。这个方法的参数是我们想要计算统计数据的列的名称，如果我们不传递任何列名，它将计算整个数据帧的统计数据。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="418d" class="ms kz it nr b gy nv nw l nx ny">staffDF.describe().show()</span><span id="1994" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------+-----+-------------+-----------------+ <br/>|summary| name|         role|           salary| <br/>+-------+-----+-------------+-----------------+ <br/>|  count|    5|            5|                5| <br/>|   mean| null|         null|           3720.0| <br/>| stddev| null|         null|571.8391382198319| <br/>|    min|  Ali|Data engineer|             3200| <br/>|    max|Steve|    Developer|             4500| <br/>+-------+-----+-------------+-----------------+</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="684a" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">空值</strong></h1><p id="c506" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">数据集或数据帧中缺少值或值为空是很常见的。Spark可以处理空值，因此我们可以在创建数据集或数据帧时使用空值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="030b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> staffWithNullDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", null, 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", null, 4100),<br/>  (null, "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>  ).toDF("name", "role", "salary")</span><span id="1e74" class="ms kz it nr b gy nz nw l nx ny">staffWithNullDF.show()</span><span id="68d2" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+ <br/>| name|         role|salary| <br/>+-----+-------------+------+ <br/>| John|         null|  4500| <br/>|James|Data engineer|  3200| <br/>|Laura|         null|  4100| <br/>| null|Data engineer|  3200| <br/>|Steve|    Developer|  3600| <br/>+-----+-------------+------+</strong></span></pre><p id="a2e3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当一个列可以接受空值时，它的<code class="fe op oq or nr b">nullable</code>参数是<code class="fe op oq or nr b">true</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e9fa" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.printSchema()</span><span id="154b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>root  <br/> |-- name: string (nullable = true)  <br/> |-- role: string (nullable = true)  <br/> |-- salary: integer (nullable = false)</strong></span></pre><p id="417e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">nullable</code>确实是<code class="fe op oq or nr b">Column</code>类的构造函数的一个参数，在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala#L97" rel="noopener ugc nofollow" target="_blank">包org.apache.spark.sql.catalog中的</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala#L97" rel="noopener ugc nofollow" target="_blank">interface.scala</a></code>文件中定义</p><p id="ff2d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在前面的例子中，<code class="fe op oq or nr b">staffDF</code>中的列<code class="fe op oq or nr b">role</code>的两个元素是<code class="fe op oq or nr b">null</code>。现在让我们向列<code class="fe op oq or nr b">salary</code>添加一些空值:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c843" class="ms kz it nr b gy nv nw l nx ny">//This won't run<strong class="nr iu"><br/>val </strong>someRows = <strong class="nr iu">Seq</strong>(<br/>   ("John", "Data scientist", null),<br/>   ("James", "Data engineer", 3200),<br/>   ("Laura", "Data scientist", 4100),<br/>   ("Ali", "Data engineer", 3200),<br/>   ("Steve", "Developer", 3600)<br/>   )</span><span id="d0f3" class="ms kz it nr b gy nz nw l nx ny">someRows.toDF("name", "role", "salary").show()</span><span id="e29f" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>nsupportedOperationException: No Encoder found for Any</strong></span></pre><p id="f6c9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果您尝试运行这段代码，将会得到一个运行时错误。在类型为<code class="fe op oq or nr b">Integer</code>的列中放置空值会将类型更改为<code class="fe op oq or nr b">Any</code>，这将不再可序列化。我们已经提到过<code class="fe op oq or nr b">RowEncoder</code>不能编码类型<code class="fe op oq or nr b">Any</code>，所以它不能序列化或编码这个列，并且您会得到一个运行时错误。事实上，如果你看一下<code class="fe op oq or nr b">printSchema()</code>对<code class="fe op oq or nr b">staffWithNullDF</code>的输出，你会注意到它不是可空的。为了克服这个问题，我们要么需要明确指定每一列的类型，要么使用Scala的<code class="fe op oq or nr b">Option</code>。要指定每个列的类型，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="928d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> someRows: <strong class="nr iu">Seq</strong>[(<strong class="nr iu">String</strong>, <strong class="nr iu">String</strong>, <strong class="nr iu">Integer</strong>)] = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", null),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>)</span><span id="5008" class="ms kz it nr b gy nz nw l nx ny">someRows.toDF("name", "role", "salary").show()</span><span id="b5cc" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  null| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>|  Ali| Data engineer|  3200| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre><p id="6e8b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们也可以使用<code class="fe op oq or nr b">Option</code>来获得带有<code class="fe op oq or nr b">None</code>值的数据帧。这里对于空值，我们使用<code class="fe op oq or nr b">None</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="14d4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> someRows: <strong class="nr iu">Seq</strong>[(<strong class="nr iu">Option</strong>[<strong class="nr iu">String</strong>], <strong class="nr iu">Option</strong>[<strong class="nr iu">String</strong>], <strong class="nr iu">Option</strong>[<strong class="nr iu">Integer</strong>])] = <strong class="nr iu">Seq</strong>(<br/>  (Some("John"), Some("Data scientist"), None),<br/>  (Some("James"), Some("Data engineer"), Some(3200)),<br/>  (Some("Laura"), Some("Data scientist"), Some(4100)),<br/>  (Some("Ali"), Some("Data engineer"), Some(3200)),<br/>  (Some("Steve"), Some("Developer"), Some(3600))<br/>)</span><span id="d9da" class="ms kz it nr b gy nz nw l nx ny">someRows.toDF("name", "role", "salary").show()</span><span id="05f1" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  null| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>|  Ali| Data engineer|  3200| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre><p id="ebbf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">请注意，在这两种情况下，我们对<code class="fe op oq or nr b">salary</code>使用了类型<code class="fe op oq or nr b">Integer</code>，而不是<code class="fe op oq or nr b">Int</code>。如果使用<code class="fe op oq or nr b">Int</code>，代码不会编译:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="352b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> someRows: <strong class="nr iu">Seq</strong>[(<strong class="nr iu">String</strong>, <strong class="nr iu">String</strong>, <strong class="nr iu">Int</strong>)] = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", <strong class="nr iu">null</strong>),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>)</span><span id="faa9" class="ms kz it nr b gy nz nw l nx ny">someRows.toDF("name", "role", "salary").show()</span><span id="79a3" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>error: an expression of type Null is ineligible for implicit conversion   ("John", "Data scientist", null),</strong></span></pre><p id="372b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">类<code class="fe op oq or nr b">Null</code>是一个<code class="fe op oq or nr b">null</code>引用的类型。在Scala中，<code class="fe op oq or nr b">Null</code>是所有引用类型(所有从<code class="fe op oq or nr b">AnyRef</code>继承的类型)的子类型。我们可以在任何需要引用类型的地方使用<code class="fe op oq or nr b">null</code>。但是，<code class="fe op oq or nr b">Null</code>不是值类型的子类型。这些是从<code class="fe op oq or nr b">AnyVal</code>继承的类型。因为<code class="fe op oq or nr b">Int</code>是值类型，所以不能在<code class="fe op oq or nr b">Seq</code>中给<code class="fe op oq or nr b">Int</code>变量赋值空值。另一方面，<code class="fe op oq or nr b">Integer</code>是一个引用类型，所以我们可以将<code class="fe op oq or nr b">null</code>赋给一个<code class="fe op oq or nr b">Integer</code>变量。</p><p id="141c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您也可以在case类中使用<code class="fe op oq or nr b">Option</code>来创建一个数据帧。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="279f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> Employee(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>, salary: <strong class="nr iu">Option</strong>[<strong class="nr iu">Integer</strong>])</span><span id="30a0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> someRows = <strong class="nr iu">Seq</strong>(Employee("John", "Data scientist", Some(4500)),<br/>               Employee("James", "Data engineer", None),<br/>               Employee("Laura", "Data scientist", Some(4100)),<br/>               Employee("Ali", "Data engineer", Some(3200)),<br/>               Employee("Steve", "Developer", None))<br/>someRows.toDF("name", "role", "salary").show()</span></pre><p id="a16f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在Scala社区中，使用<code class="fe op oq or nr b">Option</code>显然比使用<code class="fe op oq or nr b">null</code>更受欢迎。然而，<a class="ae mm" href="https://github.com/databricks/scala-style-guide#perf-option" rel="noopener ugc nofollow" target="_blank">data bricks Scala style guide</a>建议对性能敏感的代码使用<code class="fe op oq or nr b">null</code>而不是<code class="fe op oq or nr b">Option</code>来避免虚拟方法调用和装箱。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="bc4c" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">栏目</strong></h1><h2 id="9958" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">列()和栏()</h2><p id="cd51" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在Spark中，类<code class="fe op oq or nr b">Column</code>代表数据帧或数据集的一列。org.apache.spark.sql包中定义了<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L142" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L142" rel="noopener ugc nofollow" target="_blank">Column</a></code>，org.apache.spark.sql包中<code class="fe op oq or nr b">functions.scala</code>文件中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L77" rel="noopener ugc nofollow" target="_blank">对象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L77" rel="noopener ugc nofollow" target="_blank">functions</a></code>提供了几个内置的标准函数来处理列。所有这些函数都返回一个<code class="fe op oq or nr b">Column</code>对象。您可以使用方法<code class="fe op oq or nr b">col()</code>和<code class="fe op oq or nr b">column()</code>来创建一个与数据集无关的自由列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bf42" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/><strong class="nr iu">val</strong> colName = F.col("c1")</span><span id="0145" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colName: org.apache.spark.sql.Column = c1</strong></span></pre><p id="dc15" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们导入了<code class="fe op oq or nr b">functions</code>，并将其重命名为<code class="fe op oq or nr b">F</code>。在本文的其余部分，我们将使用这个名称。我们也可以这样写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c8d5" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> colName = F.column("c1")</span><span id="02dd" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colName: org.apache.spark.sql.Column = c1</strong></span></pre><h2 id="1b8f" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi">$()</h2><p id="401d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">你也可以使用Scala简写<code class="fe op oq or nr b">$</code>来创建一个<code class="fe op oq or nr b">Column</code>对象:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c475" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> colName = $"c1"</span><span id="1240" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colName: org.apache.spark.sql.ColumnName = c1</strong></span></pre><p id="4214" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当你写<code class="fe op oq or nr b">$colName</code>的时候，作用域中有一个<code class="fe op oq or nr b">implicit</code>为你将一个字符串转换成一个<code class="fe op oq or nr b">Column</code>对象。在org.apache.spark.sql包的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L41" rel="noopener ugc nofollow" target="_blank">SQLImplicits.scala</a></code>中定义了<code class="fe op oq or nr b">implicit</code>类。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="68f1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">implicit class</strong> StringToColumn(<strong class="nr iu">val</strong> sc: StringContext) { <br/>  <strong class="nr iu">def</strong> $(args: Any*): ColumnName = { <br/>     <strong class="nr iu">new</strong> ColumnName(sc.s(args: <strong class="nr iu">_</strong>*)) <br/>  }<br/>}</span></pre><p id="e73d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它使用Scala类<code class="fe op oq or nr b">StringContext</code>定义了一个定制的字符串插值器。方法<code class="fe op oq or nr b">$()</code>将其每个参数传递给<code class="fe op oq or nr b">s-</code>方法。插入的字符串被用作由<code class="fe op oq or nr b">$()</code>返回的新<code class="fe op oq or nr b">ColumnName</code>对象的名称。所以字符串确实被转换成了同名的<code class="fe op oq or nr b">ColumnName</code>对象，这也是<code class="fe op oq or nr b"><strong class="ls iu">val</strong> colName = $"c1"</code>将<code class="fe op oq or nr b">colName</code>定义为<code class="fe op oq or nr b">ColumnName </code>对象的原因。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1390" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1390" rel="noopener ugc nofollow" target="_blank">ColumnName</a></code>在<code class="fe op oq or nr b">Column.scala</code>文件中定义。它扩展了类<code class="fe op oq or nr b">Column</code>，因此可以作为<code class="fe op oq or nr b">Column</code>对象传递:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4171" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">class</strong> ColumnName(name: <strong class="nr iu">String</strong>) <strong class="nr iu">extends</strong> Column(name) {...</span></pre><p id="0b5e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因为Spark创建了一个自定义的字符串插值器来定义<code class="fe op oq or nr b">Column</code>对象，所以我们可以使用字符串插值来定义一个列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4f76" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> number ="5"<br/><strong class="nr iu">val</strong> colName = $"column${number}"</span><span id="3883" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colName: org.apache.spark.sql.ColumnName = column5</strong></span></pre><h2 id="cc6a" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">列符号</strong></h2><p id="4e1a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">您可以使用<code class="fe op oq or nr b">'</code>符号来引用<code class="fe op oq or nr b">Column</code>对象。Scala中的<code class="fe op oq or nr b">'</code>是创建<code class="fe op oq or nr b">Symbol</code>类实例的语法糖。比如，<code class="fe op oq or nr b">'mysym</code>这个词会调用<code class="fe op oq or nr b">Symbol</code>类的构造函数，扩展成<code class="fe op oq or nr b">Symbol("mysym")</code>。现在假设您有一个Spark方法，它需要一个<code class="fe op oq or nr b">Column</code>类型作为它的参数。相反，您可以传递一个符号，并且有另一个隐式in作用域将该符号转换为一个<code class="fe op oq or nr b">Column</code>对象。隐式方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4f53" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">implicit def</strong> symbolToColumn(s: Symbol): ColumnName = <strong class="nr iu">new</strong>  <br/>  ColumnName(s.name)</span></pre><p id="7f9b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala#L238" rel="noopener ugc nofollow" target="_blank">SQLImplicits.scala</a></code>中定义，并将一个符号转换成一个<code class="fe op oq or nr b">ColumnName</code>对象(也可以认为是一个<code class="fe op oq or nr b">Column</code>对象)。请注意，您不能使用<code class="fe op oq or nr b">'</code>来定义一个新的<code class="fe op oq or nr b">Column</code>对象。当您使用<code class="fe op oq or nr b">$"colname"</code>定义一个<code class="fe op oq or nr b">Column</code>对象时，编译器会在它面对<code class="fe op oq or nr b">$"colname"</code>的任何地方应用字符串插值。所以你可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9b1a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> colName = $"c1"</span></pre><p id="c00b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">创建一个<code class="fe op oq or nr b">Column</code>对象。然而，<code class="fe op oq or nr b">'</code>的隐式转换只在编译器需要一个<code class="fe op oq or nr b">Column</code>对象时有效。所以如果你写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3d74" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> colName = 'c1</span><span id="e406" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colName: Symbol = 'c1</strong></span></pre><p id="e1dc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">你得到的是一个符号而不是一个<code class="fe op oq or nr b">Column</code>对象，因为编译器在这里不期望有一个<code class="fe op oq or nr b">Column</code>对象。</p><p id="93e8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，类<code class="fe op oq or nr b">Dataset</code>中的方法<code class="fe op oq or nr b">columns</code>将所有列名作为一个<code class="fe op oq or nr b">String</code>数组返回。例如，要获得前面定义的<code class="fe op oq or nr b">staffDF</code>的列名，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e47f" class="ms kz it nr b gy nv nw l nx ny">staffDF.columns</span><span id="69ca" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res10: Array[String] = Array(name, role, salary)</strong></span></pre><p id="4db2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在DataFrame中，您可以使用列名作为参数，以获得<code class="fe op oq or nr b">Column</code>对象。所以要得到<code class="fe op oq or nr b">staffDF</code>中<code class="fe op oq or nr b">name</code>列的<code class="fe op oq or nr b">Column</code>对象，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1646" class="ms kz it nr b gy nv nw l nx ny">staffDF("name")</span><span id="ce77" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res13: org.apache.spark.sql.Column = name</strong></span></pre><h2 id="6a55" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">选择栏目</strong></h2><p id="68e1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">选择()</strong></p><p id="dc84" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">类 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1443" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1443" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1443" rel="noopener ugc nofollow" target="_blank">select</a></code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1443" rel="noopener ugc nofollow" target="_blank">可用于选择数据集或数据帧中的一组列。该方法接受许多<code class="fe op oq or nr b">Column</code>对象，并返回包含这些列的数据帧:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ccb2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> select(cols: Column*): DataFrame</span></pre><p id="5c02" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，我们可以使用下面的代码来显示<code class="fe op oq or nr b">staffDF</code>中的列<code class="fe op oq or nr b">name</code>和<code class="fe op oq or nr b">role</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8a62" class="ms kz it nr b gy nv nw l nx ny">staffDF.select("name", "role").show()</span><span id="5887" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+ <br/>| name|          role| <br/>+-----+--------------+ <br/>| John|Data scientist| <br/>|James| Data engineer| <br/>|Laura|Data scientist| <br/>|  Ali| Data engineer| <br/>|Steve|     Developer| <br/>+-----+--------------+</strong></span></pre><p id="16d8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们也可以使用生成一个<code class="fe op oq or nr b">Column</code>对象的其他方法来选择它。以下代码显示了选择<code class="fe op oq or nr b">staffDF</code>中的列<code class="fe op oq or nr b">name</code>的三种不同方式:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c866" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.select(F.col("name")).show()<br/>staffDF.select(staffDF("name")).show()<br/>staffDF.select($"name").show()</span></pre><p id="c767" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您也可以简单地使用一列(或多列)的名称来选择它:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5b2e" class="ms kz it nr b gy nv nw l nx ny">staffDF.select("name").show()</span></pre><p id="a2c6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里使用了一个 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1479" rel="noopener ugc nofollow" target="_blank">select</a></code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1479" rel="noopener ugc nofollow" target="_blank">重载变量，它接受<code class="fe op oq or nr b">String</code>参数:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e2e4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> select(col: <strong class="nr iu">String</strong>, cols: <strong class="nr iu">String</strong>*): DataFrame = select((col +: cols).map(Column(_)) : _*)</span></pre><p id="3096" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因此，使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L35" rel="noopener ugc nofollow" target="_blank">私有对象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L35" rel="noopener ugc nofollow" target="_blank">Column</a></code>将作为<code class="fe op oq or nr b">String</code>的每个列名映射到一个<code class="fe op oq or nr b">Column</code>对象。该对象的工厂方法(<code class="fe op oq or nr b">Column</code>)使用其参数名(<code class="fe op oq or nr b">colName</code>)创建一个新的<code class="fe op oq or nr b">Column</code>对象:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e252" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">private</strong>[sql] <strong class="nr iu">object</strong> Column {   <br/>  <strong class="nr iu">def</strong> apply(colName: <strong class="nr iu">String</strong>): Column = <strong class="nr iu">new</strong> Column(colName)<br/>  ...</span></pre><p id="a9ae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">此私有对象不能在SQL包之外使用。</p><p id="7838" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以要选择<code class="fe op oq or nr b">staffDF</code>中的列<code class="fe op oq or nr b">name</code>，我们也可以使用一个符号:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ec97" class="ms kz it nr b gy nv nw l nx ny">staffDF.select('name).show()</span></pre><p id="d66a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">需要注意的是，以<code class="fe op oq or nr b">$</code>开头的符号或字符串只能转换成<code class="fe op oq or nr b">ColumnName</code>对象(它也是<code class="fe op oq or nr b">Column</code>对象，因为它扩展了<code class="fe op oq or nr b">Column</code>)。因此，只有当参数的类型为<code class="fe op oq or nr b">Column</code>时，才能使用它们。如果一个方法需要一个列的名字作为<code class="fe op oq or nr b">String</code>，你不能使用它们。所以下面的代码不会编译，因为需要一个<code class="fe op oq or nr b">String</code>参数:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="847c" class="ms kz it nr b gy nv nw l nx ny">//won't compile<br/>staffDF('name)<br/>staffDF($"name")</span></pre><p id="38e7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如您之前看到的，<code class="fe op oq or nr b">select</code>将列名作为参数。要从<code class="fe op oq or nr b">List</code>中选择列，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="474a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> cols = <strong class="nr iu">List</strong>("name", "role", "salary")<br/>staffDF.select(cols.head, cols.tail: <strong class="nr iu">_</strong>*).show()</span></pre><p id="50ec" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们需要分别写<code class="fe op oq or nr b">cols</code>的头和尾，因为select是这样定义的(只需看看上面给出的<code class="fe op oq or nr b">select()</code>和字符串<code class="fe op oq or nr b">parameters</code>的定义)。我们也可以使用这个定义，并将<code class="fe op oq or nr b">cols</code>的元素直接映射到<code class="fe op oq or nr b">Column</code>对象中，以便能够选择它们:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5ed8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.select(cols.map(c =&gt; F.col(c)): <strong class="nr iu">_*</strong>).show()</span></pre><p id="30bc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">或者更简洁地说:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3bec" class="ms kz it nr b gy nv nw l nx ny">staffDF.select(cols.map(F.col): <strong class="nr iu">_*</strong>).show()</span></pre><p id="fbd2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们也可以通过直接从<code class="fe op oq or nr b">functions</code>导入<code class="fe op oq or nr b">col()</code>方法来省略<code class="fe op oq or nr b">F</code>。我们可以通过将<code class="fe op oq or nr b">"*"</code>传递到<code class="fe op oq or nr b">select()</code>来选择数据帧的所有列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ae2c" class="ms kz it nr b gy nv nw l nx ny">staffDF.select(F.col("*"))</span></pre><p id="8bd4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果我们想使用<code class="fe op oq or nr b">select()</code>保留数据帧的所有列，这可能会很有用。我们也可以选择数据帧的列的一部分。Scala标准库中的方法<code class="fe op oq or nr b">slice</code>可以用于此目的。例如，要选择<code class="fe op oq or nr b">staffDF</code>的前两列，我们可以使用下面的代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f3bc" class="ms kz it nr b gy nv nw l nx ny">staffDF.select(staffDF.columns.slice(0, 2).head, staffDF.columns.slice(0, 2).tail:<strong class="nr iu"> _*</strong>).show()</span><span id="5344" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+ <br/>| name|          role| <br/>+-----+--------------+ <br/>| John|Data scientist| <br/>|James| Data engineer| <br/>|Laura|Data scientist| <br/>|  Ali| Data engineer| <br/>|Steve|     Developer| <br/>+-----+--------------+</strong></span></pre><p id="569f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后，您可以将一个<code class="fe op oq or nr b">Column</code>对象的元素收集成一个数组:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8db3" class="ms kz it nr b gy nv nw l nx ny">staffDF.select("name").rdd.map(r =&gt; r(0)).collect()</span><span id="0c65" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res28: Array[Any] = Array(John, James, Laura, Ali, Steve)</strong></span></pre><h2 id="dccc" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">删除重复</strong></h2><p id="776b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> dropDuplicates() </strong></p><p id="5d7a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8691" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> dropDuplicates(col1: <strong class="nr iu">String</strong>, cols: <strong class="nr iu">String</strong>*): Dataset[T]</span></pre><p id="e0ff" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2613" rel="noopener ugc nofollow" target="_blank">中，类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2613" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>删除调用它的数据集的重复行，并将结果作为新的数据集返回(因此原始数据集不会改变)。如果两行或更多行的列名作为参数给出，则认为它们具有相同的值(<code class="fe op oq or nr b">col1</code>和<code class="fe op oq or nr b">cols*</code>中的其他列)。在这种情况下，Spark保留该行的第一次出现，所有重复的行都被删除。该方法的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2566" rel="noopener ugc nofollow" target="_blank">重载变量</a>有:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1e4f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> dropDuplicates(colNames: <strong class="nr iu">Array</strong>[<strong class="nr iu">String</strong>]): Dataset[T]<br/><strong class="nr iu">def</strong> dropDuplicates(colNames: <strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]): Dataset[T]</span></pre><p id="5ac7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f7e2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> staffWithDuplicatesDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600),<br/>  ("John", "Data scientist", 4500)<br/>  ).toDF("name", "role", "salary")</span><span id="4747" class="ms kz it nr b gy nz nw l nx ny">staffWithDuplicatesDF.dropDuplicates("role").show()</span><span id="a4a8" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>|James| Data engineer|  3200| <br/>|Steve|     Developer|  3600| <br/>| John|Data scientist|  4500| <br/>+-----+--------------+------+</strong></span></pre><p id="d229" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，所有具有相同值<code class="fe op oq or nr b">role</code>的重复行都被删除。如果没有向<code class="fe op oq or nr b">dropDuplicates()</code>传递列名，那么它将返回一个新的数据集，该数据集只包含调用它的原始数据集中的惟一行。这里，如果两行的所有列都具有相同的值，则认为这两行是相同的。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1bca" class="ms kz it nr b gy nv nw l nx ny">staffWithDuplicatesDF.dropDuplicates().show()</span><span id="8c9f" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>|  Ali| Data engineer|  3200| <br/>| John|Data scientist|  4500| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre><p id="257d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">截然不同()</strong></p><p id="c00e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3854" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> distinct(): Dataset[T] = dropDuplicates()</span></pre><p id="3307" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3156" rel="noopener ugc nofollow" target="_blank">阶层</a>中<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3156" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>是<code class="fe op oq or nr b">dropDuplicates()</code>的别名。例如，要获得列<code class="fe op oq or nr b">role</code>的不同值，我们可以编写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d33f" class="ms kz it nr b gy nv nw l nx ny">staffWithDuplicatesDF.select("role").distinct.show()</span><span id="bd8d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+ <br/>|          role| <br/>+--------------+ <br/>| Data engineer| <br/>|     Developer| <br/>|Data scientist| <br/>+--------------+</strong></span></pre><h2 id="e18d" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">列表达式</h2><p id="5dbf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">您可以使用<code class="fe op oq or nr b">Column</code>类的重载操作符来创建包含列的新表达式:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="683a" class="ms kz it nr b gy nv nw l nx ny">staffDF.select($"salary" * 1.2 + 100).show()</span><span id="c991" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+----------------------+ <br/>|((salary * 1.2) + 100)| <br/>+----------------------+ <br/>|                5500.0| <br/>|                3940.0| <br/>|                5020.0| <br/>|                3940.0| <br/>|                4420.0| <br/>+----------------------+</strong></span></pre><p id="5118" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，表达式<code class="fe op oq or nr b">$"salary" * 1.2 + 100</code>创建了一个名为<code class="fe op oq or nr b">((salary * 1.2) + 100)</code>的新列。所以新列的名称等于表达式字符串。我们也可以这样创建它:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a560" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> colExpr  = $"salary" * 1.2 + 100</span><span id="60b3" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colExpr: org.apache.spark.sql.Column = ((salary * 1.2) + 100)</strong></span></pre><p id="5491" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然后使用<code class="fe op oq or nr b">select()</code>显示它，得到相同的结果:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d8b8" class="ms kz it nr b gy nv nw l nx ny">staffDF.select(colExpr).show()</span></pre><p id="59ec" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">需要注意的是，在<code class="fe op oq or nr b">select()</code>中不能同时使用<code class="fe op oq or nr b">Column</code>和<code class="fe op oq or nr b">String</code>两个参数。例如，以下代码不会编译:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="da95" class="ms kz it nr b gy nv nw l nx ny">staffDF.select("name", $"salary" * 1.2 + 100).show()</span></pre><p id="364f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里第一个参数是一个<code class="fe op oq or nr b">String</code>，但是第二个是一个<code class="fe op oq or nr b">Column</code>对象。所以你需要把第一个转换成一个<code class="fe op oq or nr b">Column</code>对象:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ca7b" class="ms kz it nr b gy nv nw l nx ny">staffDF.select($"name", $"salary" * 1.2 + 100).show()</span></pre><p id="af23" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以使用列表达式创建布尔列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="20e8" class="ms kz it nr b gy nv nw l nx ny">staffDF.select($"salary" &gt; 4000).show()</span><span id="8e11" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------------+ <br/>|(salary &gt; 4000)| <br/>+---------------+ <br/>|           true| <br/>|          false| <br/>|           true| <br/>|          false| <br/>|          false| <br/>+---------------+</strong></span></pre><h2 id="e7d1" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">concat()</h2><p id="08f6" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要组合字符串列，我们可以使用<code class="fe op oq or nr b">functions</code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L3561" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L3561" rel="noopener ugc nofollow" target="_blank">concat()</a></code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2388" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.select(F.concat($"name", F.lit("-"), $"role")).show()</span><span id="1d86" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------------------+ <br/>|concat(name, -, role)| <br/>+---------------------+ <br/>|  John-Data scientist| <br/>|  James-Data engineer| <br/>| Laura-Data scientist| <br/>|    Ali-Data engineer| <br/>|      Steve-Developer| <br/>+---------------------+</strong></span></pre><p id="4726" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里使用了<code class="fe op oq or nr b">functions</code>中的方法<code class="fe op oq or nr b">lit()</code>来创建一个文字值的<code class="fe op oq or nr b">Column</code>。</p><h2 id="6731" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">列别名</h2><p id="bcc3" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">姓名()</strong></p><p id="7a89" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，用于创建新列的表达式决定了它的名称。然而，我们可以自由地为它取一个更好的名字。类<code class="fe op oq or nr b">Column</code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1166" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1166" rel="noopener ugc nofollow" target="_blank">name()</a></code>给列起了一个名字(别名)。事实上，当您在一个<code class="fe op oq or nr b">Column</code>对象上调用它时，它会返回具有新名称(别名)的相同列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9c09" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> name(alias: <strong class="nr iu">String</strong>): Column</span></pre><p id="a779" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，参数<code class="fe op oq or nr b">alias</code>将是返回的<code class="fe op oq or nr b">Column</code>对象的别名。例如，我们可以给<code class="fe op oq or nr b">colExpr</code>一个别名:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3247" class="ms kz it nr b gy nv nw l nx ny">val colExpr = ($"salary" * 1.2 + 100).name("bonus")</span><span id="4ebb" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>colExpr: org.apache.spark.sql.Column = ((salary * 1.2) + 100) AS 'bonus'</strong></span></pre><p id="5201" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在我们可以使用<code class="fe op oq or nr b">select</code>显示该列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a088" class="ms kz it nr b gy nv nw l nx ny">staffDF.select(colExpr).show()</span><span id="f751" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+ <br/>| bonus| <br/>+------+ <br/>|5500.0| <br/>|3940.0| <br/>|5020.0| <br/>|3940.0| <br/>|4420.0| <br/>+------+</strong></span></pre><p id="02ab" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">别名()和as() </strong></p><p id="c9e7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">类<code class="fe op oq or nr b">Column</code>有另外两个方法，<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1096" rel="noopener ugc nofollow" target="_blank">as()</a></code>和<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1080" rel="noopener ugc nofollow" target="_blank">alias()</a></code>，它们可以做同样的事情:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="02a8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> alias(alias: <strong class="nr iu">String</strong>): Column = name(alias)<br/><strong class="nr iu">def</strong> as(alias: <strong class="nr iu">String</strong>): Column = name(alias)</span></pre><p id="5367" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果当前列有关联的元数据，<code class="fe op oq or nr b">name()</code>会将其传播到新列。但是，您可以使用 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1148" rel="noopener ugc nofollow" target="_blank">as()</a></code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1148" rel="noopener ugc nofollow" target="_blank">重载版本给它显式元数据:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e991" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> as(alias: <strong class="nr iu">String</strong>, metadata: Metadata): Column</span></pre><p id="2390" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们看一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f7a5" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types.Metadata</span><span id="b7e2" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> colExpr  = ($"salary" * 1.2 + 100).as("bonus", <br/>  Metadata.fromJson("""{"desc": "2021 bonus"}"""))</span><span id="e81d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> newStaffDF = staffDF.select(colExpr)<br/>newStaffDF.schema.foreach{c =&gt; println(s"${c.name}, <br/>  ${c.metadata.toString}")}</span><span id="9c67" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>bonus, {"desc": "2021 bonus"}</strong></span></pre><p id="dba4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们使用类<code class="fe op oq or nr b">Metadata</code>创建元数据。它的方法<code class="fe op oq or nr b">fromJson()</code>从JSON字符串中读取元数据。<code class="fe op oq or nr b">select()</code>使用新的列创建一个新的数据帧，我们使用它的<code class="fe op oq or nr b">schema</code>显示这个数据帧的元数据。</p><h2 id="9b07" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">表达式()</h2><p id="bf64" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们也可以用一个字符串作为列表达式。为此，我们需要<code class="fe op oq or nr b">functions</code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L1394" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L1394" rel="noopener ugc nofollow" target="_blank">expr()</a></code>。该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="25fb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> expr(expr: <strong class="nr iu">String</strong>): Column</span></pre><p id="0522" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">将它的参数<code class="fe op oq or nr b">expr</code>解析成它所代表的列。所以我们可以用它重写前面所有的列表达式。例如:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4006" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.select(F.expr("salary * 1.2 + 100").alias("bonus")).show()<br/>staffDF.select(F.expr("salary &gt; 4000")).show()</span></pre><h2 id="d103" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">数学函数</strong></h2><p id="8f75" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们可以对一列中的每一行应用一个数学函数，并获得一个新的<code class="fe op oq or nr b">Column</code>对象。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L77" rel="noopener ugc nofollow" target="_blank">对象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L77" rel="noopener ugc nofollow" target="_blank">functions</a></code>中的数学函数可用于此目的。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="90b6" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> df = <strong class="nr iu">Seq</strong>(1, 10, 20, 30, 40).toDF("number")<br/>df.select(F.round(F.log("number"), 3)).show()</span><span id="19fc" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>--------------------+ <br/>|round(ln(number), 3)| <br/>+--------------------+ <br/>|                 0.0| <br/>|               2.303| <br/>|               2.996| <br/>|               3.401| <br/>|               3.689| <br/>+--------------------+</strong></span></pre><p id="bee5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> log()和round() </strong></p><p id="4add" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">功能:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6721" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> log(columnName: <strong class="nr iu">String</strong>): Column</span></pre><p id="c0f5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L1914" rel="noopener ugc nofollow" target="_blank">functions</a></code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L1914" rel="noopener ugc nofollow" target="_blank">计算给定列的自然对数，</a><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2087" rel="noopener ugc nofollow" target="_blank">函数</a>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7238" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> round(e: Column, scale: <strong class="nr iu">Int</strong>): Column</span></pre><p id="a1a4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果<code class="fe op oq or nr b">scale</code>大于或等于0，用HALF_UP舍入模式将<code class="fe op oq or nr b">e</code>的值舍入到<code class="fe op oq or nr b">scale</code>的小数位。如果<code class="fe op oq or nr b">scale</code>为负数，<code class="fe op oq or nr b">e</code>四舍五入到<code class="fe op oq or nr b">scale</code>的绝对值指定的整数部分的位置。的确，如果<code class="fe op oq or nr b">scale</code>等于<code class="fe op oq or nr b">-n</code>，这就好比<code class="fe op oq or nr b">e</code>除以<code class="fe op oq or nr b">10^n</code>，用HALF_UP舍入模式舍入到零个小数位，再乘以<code class="fe op oq or nr b">10^n </code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5f38" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/><strong class="nr iu">val</strong> df = Seq(50.6892, 206.892, 1268).toDF("number")<br/>df.select(F.round($"number", -2)).show()</span><span id="9202" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----------------+ <br/>|round(number, -2)| <br/>+-----------------+ <br/>|            100.0| <br/>|            200.0| <br/>|           1300.0| <br/>+-----------------+</strong></span></pre><p id="0260" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">还有一些其他函数，如<code class="fe op oq or nr b">exp()</code>、<code class="fe op oq or nr b">sin()</code>、<code class="fe op oq or nr b">cos()</code>、…，你可以在<a class="ae mm" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html" rel="noopener ugc nofollow" target="_blank"> Spark手册</a>中看到数学函数的完整列表。</p><p id="9ddf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> corr() </strong></p><p id="e85e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">功能:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="80d7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> corr(column1: Column, column2: Column): Column<br/><strong class="nr iu">def</strong> corr(columnName1: <strong class="nr iu">String</strong>, columnName2: <strong class="nr iu">String</strong>): Column</span></pre><p id="694e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L351" rel="noopener ugc nofollow" target="_blank">返回<code class="fe op oq or nr b">columnName1</code>和<code class="fe op oq or nr b">columnName2</code>的皮尔逊相关系数。</a></p><h2 id="75d5" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">创建新列</strong></h2><p id="6f88" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> withColumn() </strong></p><p id="4c00" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f089" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> withColumn(colName: <strong class="nr iu">String</strong>, col: Column): DataFrame</span></pre><p id="17cd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2394" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2394" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中，通过添加一列或替换同名的现有列来返回新的数据帧。事实上，它将名为<code class="fe op oq or nr b">colName</code>的<code class="fe op oq or nr b">Column</code>对象<code class="fe op oq or nr b">col</code>添加到它被调用的数据集。您也可以对<code class="fe op oq or nr b">col</code>使用列表达式。作为一个例子，我们用它向<code class="fe op oq or nr b">staffDF</code>添加一个新的常量列<code class="fe op oq or nr b">vacation</code>，并将其作为<code class="fe op oq or nr b">newStaffDF</code>返回:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d823" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> newStaffDF = staffDF.withColumn("vacation", F.lit(15))<br/>newStaffDF.show()</span><span id="8067" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+--------+ <br/>| name|          role|salary|vacation| <br/>+-----+--------------+------+--------+ <br/>| John|Data scientist|  4500|      15| <br/>|James| Data engineer|  3200|      15| <br/>|Laura|Data scientist|  4100|      15| <br/>|  Ali| Data engineer|  3200|      15| <br/>|Steve|     Developer|  3600|      15| <br/>+-----+--------------+------+--------+</strong></span></pre><p id="293b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">或者我们可以用它来改变<code class="fe op oq or nr b">staffDF</code>中的现有列<code class="fe op oq or nr b">salary</code>，使用一个列表达式:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2bcb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> newStaffDF = staffDF.withColumn("salary", $"salary" *  1.2 + <br/>  100)<br/>newStaffDF.show()</span><span id="b8df" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+------+ <br/>| name|          role|salary| bonus| <br/>+-----+--------------+------+------+ <br/>| John|Data scientist|  4500|5500.0| <br/>|James| Data engineer|  3200|3940.0| <br/>|Laura|Data scientist|  4100|5020.0| <br/>|  Ali| Data engineer|  3200|3940.0| <br/>|Steve|     Developer|  3600|4420.0| <br/>+-----+--------------+------+------+</strong></span></pre><p id="2f77" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用<code class="fe op oq or nr b">select()</code>来添加一个新列，方法是保留数据帧的原始列。我们可以用<code class="fe op oq or nr b">select()</code>来写前面的例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4805" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> newStaffDF = staffDF.select(F.col("*"),<br/>  ($"salary" * 1.2 + 100).name("bonus"))</span></pre><p id="0e2b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，<code class="fe op oq or nr b">withColumn()</code>是在类<code class="fe op oq or nr b">Dataset</code>中定义的，因此您可以将其用于数据集，并且由于DataFrame是带有<code class="fe op oq or nr b">RowEncoder</code>的数据集，因此您也可以将其用于data frame。对于我们将在本文中看到的大多数方法来说都是如此。但是<code class="fe op oq or nr b">withColumn()</code>的返回类型是<code class="fe op oq or nr b">DataFrame</code>。因此，它将原始数据集转换为新的数据帧。Spark的评论在这里有点混乱。这个函数的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2380" rel="noopener ugc nofollow" target="_blank">注释声明它返回一个数据集，这是不准确的。在类<code class="fe op oq or nr b">Dataset</code>中有一些返回数据帧的其他函数(<code class="fe op oq or nr b">select()</code>是另一个例子)。</a></p><p id="afc9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">请注意，<code class="fe op oq or nr b">withColumn()</code>在内部引入了一个投影，因此多次调用它来添加多个列会导致大的计划，这会导致性能问题，甚至<code class="fe op oq or nr b">StackOverflowException</code>。您可以一次对多个列使用<code class="fe op oq or nr b">select()</code>来避免这个问题。</p><h2 id="5b13" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">重命名列</strong></h2><p id="95ca" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> withColumnRenamed() </strong></p><p id="19b3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您也可以使用<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2459" rel="noopener ugc nofollow" target="_blank">withColumnRenamed()</a></code>来重命名现有的列。它被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="64ee" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> withColumnRenamed(existingName: <strong class="nr iu">String</strong>, newName: <strong class="nr iu">String</strong>): DataFrame</span></pre><p id="5d4d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这个方法的每次调用中，我们只能重命名一列。因此，对于重命名多个列，您需要链接它。例如，要将列<code class="fe op oq or nr b">name</code>和<code class="fe op oq or nr b">role</code>的名称分别改为<code class="fe op oq or nr b">first name</code>和<code class="fe op oq or nr b">job</code>，我们可以这样写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5294" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> newStaffDF = staffDF.withColumnRenamed("name", "first <br/>  name").withColumnRenamed("role", "job").show()</span></pre><h2 id="eee3" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">改变列的数据类型</strong></h2><p id="4def" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> cast() </strong></p><p id="1a14" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以使用<code class="fe op oq or nr b">cast()</code>方法改变列的数据类型:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="22e4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> cast(to: DataType): Column</span></pre><p id="21ff" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1188" rel="noopener ugc nofollow" target="_blank">类</a>T6中定义。它将调用它的列的数据类型更改为<code class="fe op oq or nr b">DataType</code>，并将其作为新列返回。例如，我们可以使用此方法将列的数据类型从<code class="fe op oq or nr b">String</code>更改为<code class="fe op oq or nr b">Double</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="30ae" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>import</strong> org.apache.spark.sql.types.DoubleType<br/><strong class="nr iu">val</strong> temp = <strong class="nr iu">Seq</strong>(<br/>  ("2020-01-01 07:30:00", "17.0"), <br/>  ("2020-01-02 07:30:00", "25.5"),  <br/>  ("2020-01-03 07:30:00", "19.5"),  <br/>  ("2020-01-04 07:30:00", "21.2"),  <br/>  ("2020-01-05 07:30:00", "18.0"), <br/>  ("2020-01-06 07:30:00", "20.5")<br/>  ).toDF("time", "temperature")</span><span id="e8fc" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> temperatureDF = temp.withColumn("temperature", <br/>  F.col("temperature").cast(DoubleType))<br/>temperatureDF.show()</span></pre><p id="487f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">图5显示了Spark中的类型层次结构。Spark SQL中的所有内置数据类型都是从私有抽象<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala#L28" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala#L28" rel="noopener ugc nofollow" target="_blank">AbstractDataType</a></code>中派生出来的，该类是在org.apache.spark.sql.types包中定义的，保留供内部使用。<code class="fe op oq or nr b">DataType</code>是从<code class="fe op oq or nr b">AbstractDataType</code>派生的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala#L51" rel="noopener ugc nofollow" target="_blank">同一个包</a>中的抽象类。这个类可以用作Spark SQL中所有内置数据类型的基本类型。受保护的抽象类<code class="fe op oq or nr b">AtomicType</code>扩展了<code class="fe op oq or nr b">DataType</code>。它是一种内部类型，用于表示所有非空的内容，包括用户定义的类型、数组、结构和映射。抽象类<code class="fe op oq or nr b">NumericType</code>扩展了<code class="fe op oq or nr b">AtomicType</code>并表示数字数据类型。</p><p id="9c7e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它有两个抽象子类:<code class="fe op oq or nr b">IntegralType</code>和<code class="fe op oq or nr b">FractionalType</code>。<code class="fe op oq or nr b">IntegralType</code>表示不带小数点的数字，类型(类)<code class="fe op oq or nr b">ByteType</code>、<code class="fe op oq or nr b">ShortType</code>、<code class="fe op oq or nr b">IntegerType</code>、<code class="fe op oq or nr b">LongType</code>都是由此派生出来的。<code class="fe op oq or nr b">FractionalType</code>表示带小数部分的数字，包含一位或多位小数。<code class="fe op oq or nr b">FloatType</code>、<code class="fe op oq or nr b">DoubleType</code>、<code class="fe op oq or nr b">DecimalType</code>都是从中衍生出来的。所有这些类都在包org.apache.spark.sql.types中定义，抽象类在这个包的文件<code class="fe op oq or nr b">AbstractDataType.scala</code>中定义。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/37a7eae39bb3111546f01b83b0676566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FB31BOE_oKPzH6yK5I94zg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5(作者图片)</p></figure><h2 id="c795" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">规范字符串表示</strong></h2><p id="f4dd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">方法<code class="fe op oq or nr b">cast()</code>有一个<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1204" rel="noopener ugc nofollow" target="_blank">重载变量</a>，它使用类型的规范字符串表示:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ccbc" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> cast(to: <strong class="nr iu">String</strong>): Column</span></pre><p id="7275" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，<code class="fe op oq or nr b">DoubleType</code>的规范表示是<code class="fe op oq or nr b">double</code>。所以前面的例子也可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5490" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> temperatureDF = temp.withColumn("temperature",  <br/>  F.col("temperature").cast("double"))<br/>temperatureDF.show()</span></pre><p id="5095" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在你不需要进口<code class="fe op oq or nr b">DoubleType</code>。表1显示了Spark中不同类型的规范表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/c6a865896b59233dd3a12d247fbcf0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ON3ZOdMaPzCj3PxzfUVHBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1</p></figure><h2 id="f3a2" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">时间戳</strong></h2><p id="73f4" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们可以创建一个时间戳列作为使用<code class="fe op oq or nr b">cast()</code>的另一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4b2b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> temp = <strong class="nr iu">Seq</strong>(("2020-01-01 07:30:00", 17.0), <br/>  ("2020-01-02 07:30:00", 25.5),  <br/>  ("2020-01-03 07:30:00", 19.5),  <br/>  ("2020-01-04 07:30:00", 21.2),  <br/>  ("2020-01-05 07:30:00", 18.0), <br/>  ("2020-01-06 07:30:00", 20.5)<br/>  ).toDF("time", "temperature")</span><span id="e0ea" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> temperatureDF = temp.withColumn("time", <br/>  F.col("time").cast("timestamp"))<br/>temperatureDF.show()</span><span id="f59e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+ <br/>|               time|temperature| <br/>+-------------------+-----------+ <br/>|2020-01-01 07:30:00|       17.0| <br/>|2020-01-02 07:30:00|       25.5| <br/>|2020-01-03 07:30:00|       19.5| <br/>|2020-01-04 07:30:00|       21.2| <br/>|2020-01-05 07:30:00|       18.0| <br/>|2020-01-06 07:30:00|       20.5| <br/>+-------------------+-----------+</strong></span></pre><h2 id="3b1b" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">删除一列</strong></h2><p id="564a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> drop() </strong></p><p id="b440" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">类 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2524" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">drop()</code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2524" rel="noopener ugc nofollow" target="_blank">用于从数据帧中删除特定的列:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2beb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> drop(col: Column): DataFrame</span></pre><p id="eaf1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它接受一个<code class="fe op oq or nr b">Column</code>对象，并返回一个删除了该列的新数据集。我们也可以使用列名来删除它:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2cfd" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> drop(colName: <strong class="nr iu">String</strong>): DataFrame</span></pre><p id="bbb8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">或者删除多个列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="dba2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> drop(colNames: <strong class="nr iu">String*</strong>): DataFrame</span></pre><p id="c907" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，要从<code class="fe op oq or nr b">staffDF</code>中删除列<code class="fe op oq or nr b">role</code>和<code class="fe op oq or nr b">salary</code>，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f22f" class="ms kz it nr b gy nv nw l nx ny">staffDF.drop("role", "salary").show()</span><span id="5022" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+ <br/>| name| <br/>+-----+ <br/>| John| <br/>|James| <br/>|Laura| <br/>|  Ali| <br/>|Steve| <br/>+-----+</strong></span></pre><p id="a271" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用<code class="fe op oq or nr b">_*</code>操作符从列表中删除列，将列表解包到<code class="fe op oq or nr b">drop()</code>的参数中:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ba21" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> dropList = <strong class="nr iu">List</strong>("role", "salary")<br/>staffDF.drop(dropList :<strong class="nr iu">_*</strong>).show()</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="4cd1" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">行</strong></h1><p id="8bf1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">到目前为止，我们已经讨论了与数据集或数据帧的列相关的操作，现在我们可以专注于它的行。如前所述，数据帧的每一行都是一个通用的<code class="fe op oq or nr b">Row</code>对象。您可以使用<code class="fe op oq or nr b">map()</code>函数迭代数据帧的行。</p><h2 id="aaf9" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">格塔斯()</h2><p id="03be" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如前所述，DataFrame <code class="fe op oq or nr b">Row</code>对象中的项目具有类型<code class="fe op oq or nr b">Any</code>。我们可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L358" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L358" rel="noopener ugc nofollow" target="_blank">getAs()</a></code>来获取DataFrame的一行中的一个项目，并将其转换为一个类型。该方法被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9981" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> getAs[T](i: <strong class="nr iu">Int</strong>): T</span></pre><p id="cdcd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它返回给定索引<code class="fe op oq or nr b">i</code>处的<code class="fe op oq or nr b">Row</code>对象(它在其上被调用)的字段值，作为类型<code class="fe op oq or nr b">T</code>。它的另一个定义是:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5c0a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> getAs[T](fieldName: <strong class="nr iu">String</strong>): T</span></pre><p id="b1d6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它返回列名为<code class="fe op oq or nr b">fieldName</code>的<code class="fe op oq or nr b">Row</code>对象的字段值，作为类型<code class="fe op oq or nr b">T</code>。所以您可以通过<code class="fe op oq or nr b">getAs()</code>使用列名和索引。下面是一个使用<code class="fe op oq or nr b">map()</code>函数迭代DataFrame的行并使用<code class="fe op oq or nr b">getAs()</code>获取每行中的项目的示例:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="293e" class="ms kz it nr b gy nv nw l nx ny">staffDF.map {row =&gt; <br/>  (row.getAs[<strong class="nr iu">String</strong>](0), row.getAs[<strong class="nr iu">Integer</strong>]("salary") * 1.2)}<br/>  .toDF("name", "bonus").show()</span><span id="a894" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+------+ <br/>| name| bonus| <br/>+-----+------+ <br/>| John|5400.0| <br/>|James|3840.0| <br/>|Laura|4920.0| <br/>|  Ali|3840.0| <br/>|Steve|4320.0| <br/>+-----+------+</strong></span></pre><h2 id="3424" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">getInt()、getDouble()和getString()</h2><p id="6a1c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">也可以用类似<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L245" rel="noopener ugc nofollow" target="_blank">getInt()</a></code>、<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L270" rel="noopener ugc nofollow" target="_blank">getDouble()</a></code>的以<code class="fe op oq or nr b">get</code>开头的一系列方法。这些方法返回特定类型的给定索引<code class="fe op oq or nr b">i</code>处的列值。例如，<code class="fe op oq or nr b">getString()</code>将其作为string对象返回。所以前面的例子可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="415a" class="ms kz it nr b gy nv nw l nx ny">staffDF.map {row =&gt; <br/>  (row.getString(0), row.getAs[<strong class="nr iu">Integer</strong>]("salary") * 1.2)<br/>  }.toDF("name", "bonus").show()</span></pre><p id="b3fb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用这些方法来获取数据帧的单行中的字段值:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9b5a" class="ms kz it nr b gy nv nw l nx ny">staffDF.filter($"name" === "John").head().getAs[String]("name")</span><span id="8cc9" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: String = John</strong></span></pre><p id="0d16" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们首先过滤所需的行。结果是一个DataFrame，所以我们使用<code class="fe op oq or nr b">head()</code>来获取它的<code class="fe op oq or nr b">Row</code>对象。然后我们可以使用适当的get函数来获取该行的一个字段。</p><h2 id="6405" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">分裂一列</strong></h2><p id="62aa" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">拆分()</strong></p><p id="5f7c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9487" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> split(str: Column, pattern: <strong class="nr iu">String</strong>): Column</span></pre><p id="bce1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在类<code class="fe op oq or nr b">Column</code>中，围绕<code class="fe op oq or nr b">pattern</code>的匹配项拆分列<code class="fe op oq or nr b">str</code>，这里<code class="fe op oq or nr b">pattern</code>表示一个正则表达式。该方法在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2683" rel="noopener ugc nofollow" target="_blank">对象</a>中定义<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2683" rel="noopener ugc nofollow" target="_blank">functions</a></code>。返回的<code class="fe op oq or nr b">Column</code>的数据类型为<code class="fe op oq or nr b">Array</code>。所以这一列的每一行都是一个包含所有拆分结果的数组。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bf54" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> rawDF = <strong class="nr iu">Seq</strong>(<br/>  ("John, Data scientist", 4500),<br/>  ("James, Data engineer", 3200),<br/>  ("Laura, Data scientist", 4100)).toDF("name-role", "salary")<br/>rawDF.select($"name-role", F.split($"name-role", <br/>  ",").as("split_col")).show(false)</span><span id="54db" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------------------+------------------------+ <br/>|name-role            |split_col               | <br/>+---------------------+------------------------+ <br/>|John, Data scientist |[John,  Data scientist] | <br/>|James, Data engineer |[James,  Data engineer] | <br/>|Laura, Data scientist|[Laura,  Data scientist]| <br/>+---------------------+------------------------+</strong></span></pre><p id="dab1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">但是我们如何访问这个数组中的元素呢？该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="77dc" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> getItem(key: <strong class="nr iu">Any</strong>): Column</span></pre><p id="fa18" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L871" rel="noopener ugc nofollow" target="_blank">类</a>中定义<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L871" rel="noopener ugc nofollow" target="_blank">Column</a></code>。您可以在包含数组的<code class="fe op oq or nr b">Column</code>对象上调用这个方法。它从数组中获取一个位置为<code class="fe op oq or nr b">ordinal</code>的项目，并将其作为一个新的<code class="fe op oq or nr b">Column</code>对象返回。因此，我们可以使用下面的代码来拆分一列，并将拆分的项提取为单独的列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="98e1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> rawDF = <strong class="nr iu">Seq</strong>(<br/>  ("John, Data scientist", 4500),<br/>  ("James, Data engineer", 3200),<br/>  ("Laura, Data scientist", 4100)).toDF("name-role", "salary")<br/><strong class="nr iu">val</strong> splitDF = rawDF.select(F.split($"name-role", <br/>  ",").as("split_col"), $"salary")<br/>splitDF.select(<br/>  $"split_col".getItem(0).as("name"),<br/>  $"split_col".getItem(1).as("role"),<br/>  $"salary").show()</span><span id="8e9c" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+---------------+------+ <br/>| name|           role|salary| <br/>+-----+---------------+------+ <br/>| John| Data scientist|  4500| <br/>|James|  Data engineer|  3200| <br/>|Laura| Data scientist|  4100| <br/>+-----+---------------+------+</strong></span></pre><p id="8367" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以以编程方式创建新列的名称。当我们获得大量新列时，这很有用。以下代码拆分一列，将新列命名为<code class="fe op oq or nr b">col1</code>、<code class="fe op oq or nr b">col2</code>、… <code class="fe op oq or nr b">coln</code>，然后删除它:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="87bb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>rawDF.select(F.col("*"),<br/>  F.split($"name-role",  ",").as("split_col"))<br/>  .select((0 to 1).map(i =&gt; <br/>    F.col("split_col").getItem(i)<br/>    .as(s"col${i+1}")) :+ F.col("*") : <strong class="nr iu">_*</strong>)<br/>  .drop("name-role", "split_col").show()</span><span id="dfe6" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+---------------+------+ <br/>| col1|           col2|salary| <br/>+-----+---------------+------+ <br/>| John| Data scientist|  4500| <br/>|James|  Data engineer|  3200| <br/>|Laura| Data scientist|  4100| <br/>+-----+---------------+------+</strong></span></pre><h2 id="d2ad" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">自定义项</strong></h2><p id="4853" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">自定义项()</strong></p><p id="934c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">Spark允许您定义用户定义函数(UDF)来扩展其内置函数。UDF是基于列的函数，接受一个或多个列作为输入参数，并返回一个新列作为输出。你可以通过定义一个Scala函数并用<code class="fe op oq or nr b">udf()</code>方法包装它来定义一个UDF。例如，下面的代码定义了一个UDF来确定一列中的数字是否为偶数。定义UDF后，我们可以使用<code class="fe op oq or nr b">withColumn()</code>将其应用于一个列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a994" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> numDF = <strong class="nr iu">Seq</strong>(6, 9, 12, 3, 10).toDF("number")</span><span id="04af" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> isEven = (number: <strong class="nr iu">Int</strong>) =&gt; number % 2 == 0<br/><strong class="nr iu">val</strong> isEvenUDF = F.udf(isEven)<br/><strong class="nr iu">val </strong>newDF = numDF.withColumn("even", isEvenUDF($"number"))<br/>newDF.show()</span><span id="7e46" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+-----+ <br/>|number| even| <br/>+------+-----+ <br/>|     6| true| <br/>|     9|false| <br/>|    12| true| <br/>|     3|false| <br/>|    10| true| <br/>+------+-----+</strong></span></pre><p id="526d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为此，我们可以使用<code class="fe op oq or nr b">select()</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2c38" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val </strong>newDF = numDF.select(F.col("*"), isEvenUDF($"number") as "even")</span></pre><p id="daac" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在对象<code class="fe op oq or nr b">functions</code>中定义了<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L4648" rel="noopener ugc nofollow" target="_blank">方法</a>T10。它接受多达15个参数的Scala函数。对于每个参数数量，都有一个重载的<code class="fe op oq or nr b">udf()</code>函数。例如，对于带有一个参数的函数，<code class="fe op oq or nr b">udf()</code>定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a15f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> udf[RT: TypeTag, A1: TypeTag](f: Function1[A1, RT]): UserDefinedFunction = {    <br/>  <strong class="nr iu">val </strong>outputEncoder = Try(ExpressionEncoder[RT]()).toOption    <br/>  <strong class="nr iu">val </strong>ScalaReflection.Schema(dataType, nullable) =  <br/>    outputEncoder.map(UDFRegistration.outputSchema)<br/>    .getOrElse(ScalaReflection.schemaFor[RT])    <br/>  <strong class="nr iu">val </strong>inputEncoders = Try(ExpressionEncoder[A1]()).toOption :: Nil<br/>  <strong class="nr iu">val </strong>udf = SparkUserDefinedFunction(f, dataType, inputEncoders, <br/>    outputEncoder)    <br/>  <strong class="nr iu">if </strong>(nullable) udf <strong class="nr iu">else </strong>udf.asNonNullable()<br/>}</span></pre><p id="26f5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在Scala中，编译器在编译时移除所有泛型类型信息。因此类型参数<code class="fe op oq or nr b">A1</code>和<code class="fe op oq or nr b">RT</code>在运行时将不可见。<code class="fe op oq or nr b">udf()</code>使用<code class="fe op oq or nr b">TypeTag</code>从Scala反射中识别出<code class="fe op oq or nr b">RT</code>和<code class="fe op oq or nr b">A1</code>的数据类型，封装了类型的运行时类型表示。<code class="fe op oq or nr b">udf()</code>的返回类型为<code class="fe op oq or nr b">UserDefinedFunction</code>。</p><p id="8cd8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">udf()</code>为函数(<code class="fe op oq or nr b">RT</code>)的返回类型创建编码器，并将其分配给<code class="fe op oq or nr b">outputEncoder</code>。它还创建一个列表，其中包含它所采用的函数的每个参数的编码器(<code class="fe op oq or nr b">f</code>)。这里<code class="fe op oq or nr b">f</code>只有一个参数，所以列表只有一个元素。该列表被分配给<code class="fe op oq or nr b">inputEncoders</code>。<code class="fe op oq or nr b">f</code>的返回类型被分配给<code class="fe op oq or nr b">dataType</code>。最后，case类<code class="fe op oq or nr b">SparkUserDefinedFunction</code>的一个实例被创建并作为<code class="fe op oq or nr b">udf()</code>的输出返回。这个类是在org . Apache . spark . SQL . expressions包中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala#L90" rel="noopener ugc nofollow" target="_blank">文件</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala#L90" rel="noopener ugc nofollow" target="_blank">UserDefinedFunction.scala</a></code>中定义的。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b0e2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">private</strong>[spark] <strong class="nr iu">case class</strong> SparkUserDefinedFunction(    <br/> f: AnyRef, dataType: DataType,    <br/> inputEncoders: <strong class="nr iu">Seq</strong>[<strong class="nr iu">Option</strong>[ExpressionEncoder[<strong class="nr iu">_</strong>]]] = <strong class="nr iu">Nil</strong>, <br/> outputEncoder: <strong class="nr iu">Option</strong>[ExpressionEncoder[<strong class="nr iu">_</strong>]] = None,<br/> name: <strong class="nr iu">Option</strong>[<strong class="nr iu">String</strong>] = None, nullable: <strong class="nr iu">Boolean </strong>= true,   <br/> deterministic: <strong class="nr iu">Boolean</strong> = true) <strong class="nr iu">extends </strong>UserDefinedFunction {    </span><span id="fb51" class="ms kz it nr b gy nz nw l nx ny">  @scala.annotation.varargs  <br/>  <strong class="nr iu">override def</strong> apply(exprs: Column*): Column = {    <br/>    Column(createScalaUDF(exprs.map(<strong class="nr iu">_</strong>.expr))) <br/>}<br/>...</span></pre><p id="fb96" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们只提供了这个类的前四个参数，所以其他参数采用它们的默认值。我们可以在前面的例子(<code class="fe op oq or nr b">isEvenUDF</code>)中检查这个类的参数:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4b18" class="ms kz it nr b gy nv nw l nx ny">isEvenUDF</span><span id="4d9f" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res0: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4279/486689149@634ff1c9,BooleanType,List(Some(class[value[0]: int])),Some(class[value[0]: boolean]),None,false,true)</strong></span></pre><p id="9586" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">事实上，我们可以使用<code class="fe op oq or nr b">SparkUserDefinedFunction</code>直接创建我们的UDF:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="25dc" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<br/><strong class="nr iu">import</strong> org.apache.spark.sql.expressions.SparkUserDefinedFunction<br/><strong class="nr iu">import</strong> org.apache.spark.sql.types.BooleanType<br/><strong class="nr iu">import</strong> scala.util.Try</span><span id="a42b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> outputEncoder = Try(ExpressionEncoder[<strong class="nr iu">Boolean</strong>]()).toOption<br/><strong class="nr iu">val</strong> inputEncoders = Try(ExpressionEncoder[<strong class="nr iu">Int</strong>]()).toOption :: <strong class="nr iu">Nil</strong><br/><strong class="nr iu">val</strong> isEvenUDF = SparkUserDefinedFunction((<strong class="nr iu">_</strong>: <strong class="nr iu">Int</strong>) % 2 == 0, <br/>  BooleanType, inputEncoders, outputEncoder)</span></pre><p id="2c7c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们使用<code class="fe op oq or nr b">ExpressionEncoder</code>类来创建输入和输出编码器。我们还知道<code class="fe op oq or nr b">isEven()</code>的返回类型是Boolean。</p><p id="0681" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">SparkUserDefinedFunction</code>的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala#L100" rel="noopener ugc nofollow" target="_blank">apply()</a></code> <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala#L100" rel="noopener ugc nofollow" target="_blank">方法</a>接受多个<code class="fe op oq or nr b">Column</code>对象并返回一个<code class="fe op oq or nr b">Column</code>对象。事实上，它将原始函数转换为基于列的函数。因此，我们只能将<code class="fe op oq or nr b">Column</code>对象传递给从<code class="fe op oq or nr b">udf()</code>返回的UDF。(如果我们传递列名，它不会编译)。<code class="fe op oq or nr b">apply()</code>使用<code class="fe op oq or nr b">expr</code>将方法<code class="fe op oq or nr b">createScalaUDF()</code>应用于输入列的内部催化剂表达式序列。该方法在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala#L104" rel="noopener ugc nofollow" target="_blank">相同的类</a>中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ee0d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">private</strong>[sql] <strong class="nr iu">def</strong> createScalaUDF(exprs: <strong class="nr iu">Seq</strong>[Expression]): ScalaUDF = {    <br/>  ScalaUDF(f, dataType, exprs, inputEncoders,<br/>    outputEncoder, udfName = name,<br/>    nullable = nullable,<br/>    udfDeterministic = deterministic)<br/>}</span></pre><p id="b2a5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它返回一个<code class="fe op oq or nr b">ScalaUDF</code>对象。在org . Apache . spark . SQL . catalyst . expressions包中定义了<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L47" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L47" rel="noopener ugc nofollow" target="_blank">ScalaUDF</a></code></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1973" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> ScalaUDF(<br/>  function: AnyRef, dataType: DataType,<br/>  children: <strong class="nr iu">Seq</strong>[<strong class="nr iu">Expression</strong>],<br/>  inputEncoders: <strong class="nr iu">Seq</strong>[<strong class="nr iu">Option</strong>[ExpressionEncoder[<strong class="nr iu">_</strong>]]] = <strong class="nr iu">Nil</strong>,       <br/>  outputEncoder: <strong class="nr iu">Option</strong>[ExpressionEncoder[<strong class="nr iu">_</strong>]] = None,<br/>  udfName: <strong class="nr iu">Option</strong>[<strong class="nr iu">String</strong>] = None, nullable: <strong class="nr iu">Boolean</strong> = true,      <br/>  udfDeterministic: <strong class="nr iu">Boolean</strong> = true) <strong class="nr iu">extends</strong> Expression <strong class="nr iu">with</strong> NonSQLExpression <strong class="nr iu">with</strong> UserDefinedExpression</span></pre><p id="9005" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它采用UDF函数、其返回数据类型、编码器、输入Catalyst表达式和一些其他默认值。对于我们的例子，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9754" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<br/><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.expressions.Literal<br/><strong class="nr iu">import</strong> org.apache.spark.sql.catalyst.expressions.ScalaUDF</span><span id="cff0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> isEvenUDF = ScalaUDF((<strong class="nr iu">_</strong>: <strong class="nr iu">Int</strong>) % 2 == 0, <strong class="nr iu">BooleanType</strong>, Literal(6)   <br/>    :: <strong class="nr iu">Nil</strong>, <strong class="nr iu">Option</strong>(ExpressionEncoder[<strong class="nr iu">Int</strong>]()) :: <strong class="nr iu">Nil</strong>)</span></pre><p id="6024" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们用一个文字值来测试它。方法<code class="fe op oq or nr b">eval()</code>对输入的内部行执行UDF函数。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b3ff" class="ms kz it nr b gy nv nw l nx ny">isEvenUDF.eval()</span><span id="3ff4" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: Any = true</strong></span></pre><p id="eda1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在下一个例子中，我们用<code class="fe op oq or nr b">udf()</code>包装了一个带有两个参数的函数:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8fcf" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>def</strong> func(salary: <strong class="nr iu">Integer</strong>, role: <strong class="nr iu">String</strong>): <strong class="nr iu">String</strong> = <br/>  (salary, role) <strong class="nr iu">match</strong> {<br/>    <strong class="nr iu">case</strong> (x, "Data scientist") <strong class="nr iu">if</strong> x &gt;= 4000 =&gt; "Senior"<br/>    <strong class="nr iu">case</strong> (x, "Data engineer") <strong class="nr iu">if</strong> x &gt; 3800  =&gt; "Senior"<br/>    <strong class="nr iu">case</strong> (x, <strong class="nr iu">_</strong>) <strong class="nr iu">if</strong> x &gt;= 3500  =&gt; "Senior"<br/>    <strong class="nr iu">case _</strong>          =&gt; "Junior"<br/>  }</span><span id="3c48" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> levelUDF = F.udf(func <strong class="nr iu">_</strong>)</span><span id="c962" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> newStaffDF = staffDF.withColumn("level",                                                            <br/>  levelUDF(F.col("salary"), F.col("role")))<br/>newStaffDF.show()</span><span id="e65e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+------+ <br/>| name|          role|salary| level| <br/>+-----+--------------+------+------+ <br/>| John|Data scientist|  4500|Senior| <br/>|James| Data engineer|  3200|Junior| <br/>|Laura|Data scientist|  4100|Senior| <br/>|  Ali| Data engineer|  3200|Junior| <br/>|Steve|     Developer|  3600|Senior| <br/>+-----+--------------+------+------+</strong></span></pre><p id="45bf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们首先定义带两个参数的Scala函数<code class="fe op oq or nr b">func</code>。然后用<code class="fe op oq or nr b">udf()</code>包起来。我们也可以这样定义它:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bd6d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> levelUDF = F.udf[<strong class="nr iu">String</strong>, <strong class="nr iu">Integer</strong>, <strong class="nr iu">String</strong>](func)</span></pre><p id="ad8e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们需要提供函数参数的返回类型和数据类型。如果您查看1到15个参数的<code class="fe op oq or nr b">udf()</code>函数的定义，您会看到返回类型排在第一位。</p><p id="3959" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，<code class="fe op oq or nr b">udf()</code>返回的UDF函数只能将<code class="fe op oq or nr b">Column</code>对象作为参数。因此，如果Scala函数接受一个不是<code class="fe op oq or nr b">Column</code>对象的参数，我们需要将其转换为<code class="fe op oq or nr b">Column</code>对象。例如，假设我们需要一个函数，它将一个整数作为第二个参数，并将它添加到一个列中。我们像往常一样定义UDF。但是当我们想要使用<code class="fe op oq or nr b">withColumn()</code>内部的UDF时，我们需要用<code class="fe op oq or nr b">lit()</code>包装第二个参数，它是一个整数:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f427" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>import</strong> org.apache.spark.sql.functions.lit</span><span id="11c0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> numDF = <strong class="nr iu">Seq</strong>(6, 9, 12, 3, 10).toDF("<strong class="nr iu">number</strong>")<br/><strong class="nr iu">val</strong> addConst = (number: <strong class="nr iu">Int</strong>, const: <strong class="nr iu">Int</strong>) =&gt; number + const<br/><strong class="nr iu">val</strong> addConstUDF = F.udf(addConst)<br/><strong class="nr iu">val</strong> newDF = numDF.withColumn("new_number", addConstUDF($"number", lit(5)))<br/>newDF.show()</span><span id="7cb4" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+----------+ <br/>|number|new_number| <br/>+------+----------+ <br/>|     6|        11| <br/>|     9|        14| <br/>|    12|        17| <br/>|     3|         8| <br/>|    10|        15| <br/>+------+----------+</strong></span></pre><p id="dc0b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L114" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L114" rel="noopener ugc nofollow" target="_blank">lit()</a></code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6f2a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> lit(literal: Any): Column</span></pre><p id="7f06" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">创建一列文字值。所以<code class="fe op oq or nr b">lit(5)</code>创建一个所有元素都等于5的列。该列中的行数将等于<code class="fe op oq or nr b">$"number"</code>中的行数。</p><p id="bea6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">广播变量</strong></p><p id="8d4a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在下一节中，我们将讨论一些参数不是<code class="fe op oq or nr b">Column</code>对象的UDF。为了使用这样的UDF，我们需要广播它们的参数。我们将在这里解释广播变量。广播变量允许我们以有效的方式与每个节点共享变量的只读副本。因此，将没有必要航运它的副本与任务。Spark可以使用高效的广播算法来分发广播变量，以降低通信成本。</p><p id="0aca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">广播()</strong></p><p id="93dc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以使用<code class="fe op oq or nr b">SparkContext</code>类中的方法<code class="fe op oq or nr b">broadcast()</code>来创建一个广播变量。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8891" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> v = Array(1, 2, 3)<br/><strong class="nr iu">val</strong> broadcastV = <!-- -->spark.sparkContext.broadcast<!-- -->(v)</span><span id="0c80" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output<br/>broadcastV: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(7762)</strong></span></pre><p id="4b38" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如您所见，原始变量用一个<code class="fe op oq or nr b">Broadcast</code>对象包装。创建广播变量后，应该在集群上运行的任何函数中使用它来代替原始变量，以便原始变量不会被多次发送到节点。此外，原始变量在广播后不应被修改，以确保所有节点获得相同的广播变量的值。</p><p id="83e1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">广播变量不会通过调用<code class="fe op oq or nr b">spark.sparkContext.broadcast()</code>发送给节点上的执行器。而是会在第一次使用的时候发货。当一个方法需要处理一个广播变量时，它不能直接使用广播变量。相反，应该通过调用<code class="fe op oq or nr b">value()</code>方法来访问它的<code class="fe op oq or nr b">value</code>。该方法返回由<code class="fe op oq or nr b">broadcast()</code>包装的实际变量。因此，要使用前一个示例中的广播数组，我们应该编写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d471" class="ms kz it nr b gy nv nw l nx ny">broadcastV.values</span></pre><p id="0c21" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">带非列参数的自定义项</strong></p><p id="bd63" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在假设您有一个将<code class="fe op oq or nr b">Map</code>作为参数的函数，因此您不能再用<code class="fe op oq or nr b">lit()</code>包装它。这里我们不包括那些不能被转换成被<code class="fe op oq or nr b">udf()</code>包装的Scala函数的参数列表中的<code class="fe op oq or nr b">Column</code>对象的参数。UDF将把这些参数放在一个单独的参数表中，作为<em class="oa">控制的</em>函数。让我们看一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2ef6" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>import</strong> org.apache.spark.broadcast.Broadcast<strong class="nr iu"><br/>val</strong> jobCodes = <strong class="nr iu">Map</strong>("Data scientist" -&gt; 0,<br/>                   "Data engineer" -&gt; 1,<br/>                   "Developer" -&gt; 2)</span><span id="9136" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> broadcastJobCodes = spark.sparkContext.broadcast(jobCodes)<strong class="nr iu"><br/>def</strong> mapJobCodeUDF(m: Broadcast[<strong class="nr iu">Map</strong>[<strong class="nr iu">String</strong>, <strong class="nr iu">Int</strong>]]) = <br/>  udf((role: <strong class="nr iu">String</strong>) =&gt; m.value.getOrElse(role, -1))</span><span id="d326" class="ms kz it nr b gy nz nw l nx ny">staffDF.select($"role",<br/>  mapJobCodeUDF(broadcastJobCodes)($"role")<br/>  .as("job_code")).show()</span><span id="2bdd" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+--------+ <br/>|          role|job_code| <br/>+--------------+--------+ <br/>|Data scientist|       0| <br/>| Data engineer|       1| <br/>|Data scientist|       0| <br/>| Data engineer|       1| <br/>|     Developer|       2| <br/>+--------------+--------+</strong></span></pre><p id="12e3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这里，我们首先需要广播<code class="fe op oq or nr b">jobCodes</code>以将其缓存在每个节点上。广播变量叫做<code class="fe op oq or nr b">broadcastJobCodes</code>。函数文字<code class="fe op oq or nr b">(role: String) =&gt; m.getOrElse(role, -1)</code>是一个<em class="oa">开放项</em>，因为它有一个自由变量<code class="fe op oq or nr b">m</code>。该变量不能转换成<code class="fe op oq or nr b">Column</code>对象，因为它是一个广播<code class="fe op oq or nr b">Map</code>。运行时从这个函数文字创建的对象是一个<em class="oa">闭包</em>。实际上，<code class="fe op oq or nr b">udf()</code>里面的闭包只带一个参数，就是一个<code class="fe op oq or nr b">Column</code>对象。</p><p id="5001" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">mapJobUDF</code>是一个简化的UDF函数，因为它有两个独立的参数列表。的确可以写成<code class="fe op oq or nr b">mapJobUDF(m: Map[String, Int])(role: String)</code>。在<code class="fe op oq or nr b">select()</code>内部，我们首先将广播变量传递给它。现在<code class="fe op oq or nr b">mapJobUDF(broadcastJobCodes)</code>是一个只有一个参数(<code class="fe op oq or nr b">$"role"</code>)的UDF函数，这个参数是一个<code class="fe op oq or nr b">Column</code>对象。请注意，我们需要在UDF函数中使用广播变量的值(<code class="fe op oq or nr b">m.value</code>)。</p><p id="f82e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">UDF中的空值</strong></p><p id="bf4f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">UDF不能将<code class="fe op oq or nr b">Option</code>作为参数。不过，你可以用它们来处理<code class="fe op oq or nr b">null</code> s。例如，以下代码不会运行，因为UDF不能对空值进行双精度运算:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1f3e" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">//This won't run<br/>import</strong> org.apache.spark.sql.{functions=&gt;F}<br/><strong class="nr iu">val</strong> someRow: <strong class="nr iu">Seq</strong>[<strong class="nr iu">Integer</strong>] = Seq(6, 9, 12, <strong class="nr iu">null</strong>, 10)<br/><strong class="nr iu">val</strong> numDF = someRow.toDF("number")</span><span id="0aab" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">def</strong> makeDouble(num: <strong class="nr iu">Integer</strong>): <strong class="nr iu">Integer </strong>= 2*num<br/><strong class="nr iu">val</strong> makeDoubleUDF = F.udf[<strong class="nr iu">Integer</strong>, <strong class="nr iu">Integer</strong>](makeDouble)</span><span id="431f" class="ms kz it nr b gy nz nw l nx ny">numDF.withColumn("number doubled", <br/>  makeDoubleUDF(F.col("number"))).show()</span></pre><p id="6106" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">但是，我们可以修改UDF来处理空值，并用<code class="fe op oq or nr b">Option</code>包装返回值:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0e49" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>def </strong>makeDouble(num: <strong class="nr iu">Integer</strong>): <strong class="nr iu">Option</strong>[<strong class="nr iu">Integer</strong>] = num <strong class="nr iu">match </strong>{<br/>  <strong class="nr iu">case null </strong>=&gt; None<br/>  <strong class="nr iu">case </strong>x =&gt; Some(2*x)<br/>}<br/><strong class="nr iu">val </strong>makeDoubleUDF = F.udf[Option[<strong class="nr iu">Integer</strong>], <strong class="nr iu">Integer</strong>](makeDouble)</span><span id="cd68" class="ms kz it nr b gy nz nw l nx ny">numDF.withColumn("number doubled", makeDoubleUDF(F.col("number"))).show()</span><span id="8269" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+--------------+ <br/>|number|number doubled| <br/>+------+--------------+ <br/>|     6|            12| <br/>|     9|            18| <br/>|    12|            24| <br/>|  null|          null| <br/>|    10|            20| <br/>+------+--------------+</strong></span></pre><p id="d8eb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后一点，UDF就像Spark SQL的黑盒，所以它不能优化它们。因此，您应该尽可能避免使用它们，以提高代码的性能。</p><h2 id="8099" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">过滤行</strong></h2><p id="e466" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">过滤器()</strong></p><p id="3b4c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1596" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1596" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">filter()</code>使用给定的条件过滤数据集或数据帧的行:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8b89" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> filter(condition: Column): Dataset[T]</span></pre><p id="71bb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">条件是返回布尔列的列表达式，因此不能简单地使用列名，除非该列的类型是布尔。<code class="fe op oq or nr b">filter()</code>将结果作为新的<code class="fe op oq or nr b">Dataset</code>返回。在下面的示例中，它用于过滤<code class="fe op oq or nr b">staffDF</code>中<code class="fe op oq or nr b">salary</code>等于<code class="fe op oq or nr b">3200</code>的行。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9612" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> filteredDF = staffDF.filter($"salary" === 3200)  <br/>filteredDF.show()</span><span id="3bdc" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+<br/>| name|         role|salary|<br/>+-----+-------------+------+<br/>|James|Data engineer|  3200|<br/>|  Ali|Data engineer|  3200|<br/>+-----+-------------+------+</strong></span></pre><p id="934e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">表2给出了可用于列表达式的布尔和逻辑方法的列表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/06d8c5b778376477a61e721467c21a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01Qj8kAO5YJFRXd2P9Vimw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表2</p></figure><p id="9f3a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这是另一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="205a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> filteredDF = staffDF.filter($"salary" &gt; 3200 &amp;&amp; $"name" =!= "Ali")  <br/>filteredDF.show()</span><span id="366b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  4500| <br/>|Laura|Data scientist|  4100| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre><p id="1f52" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如表2所示，您可以使用每个操作符可用的替代方法。因此，前面的例子可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="eed8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> filteredDF = staffDF.filter(($"salary" gt 3200) and ($"name" notEqual "Ali"))</span></pre><p id="256b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">但是，您需要在比较方法周围添加括号，因为它们的评估优先级不会高于<code class="fe op oq or nr b">and</code>。如果去掉括号，表达式</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bd8b" class="ms kz it nr b gy nv nw l nx ny">$"salary" gt 3200 and $"name" notEqualTo "Ali"</span></pre><p id="a36c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">被评估为</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d104" class="ms kz it nr b gy nv nw l nx ny"> <!-- -->(($"salary" gt 3200) and $"name") notEqualTo "Ali"<!-- --> </span></pre><p id="ffcb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以<code class="fe op oq or nr b">and</code>的右操作数将是一个<code class="fe op oq or nr b">Int</code>列，而不是一个<code class="fe op oq or nr b">Boolean</code>列，这样你就会得到一个错误。</p><p id="1367" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">请注意，这里我们使用<code class="fe op oq or nr b">===</code>作为等式运算符，而不是<code class="fe op oq or nr b">==</code>。在Spark中，<code class="fe op oq or nr b">===</code>和<code class="fe op oq or nr b">==</code>都是相等运算符，但返回的类型不同。列表达式<code class="fe op oq or nr b">a==b</code>返回一个<code class="fe op oq or nr b">Column</code>对象，该对象包含列<code class="fe op oq or nr b">a</code>和<code class="fe op oq or nr b">b</code>的单个元素的比较结果。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="79bc" class="ms kz it nr b gy nv nw l nx ny">staffDF.select($"salary" === 3200).show()</span><span id="3d6c" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------------+ <br/>|(salary = 3200)| <br/>+---------------+ <br/>|          false| <br/>|           true| <br/>|          false| <br/>|           true| <br/>|          false| <br/>+---------------+</strong></span></pre><p id="d793" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">事实上，表2中的所有比较操作符都返回这样一个<code class="fe op oq or nr b">Column</code>对象。另一方面，列表达式<code class="fe op oq or nr b">a==b</code>返回一个布尔值。当列<code class="fe op oq or nr b">a</code>等于列<code class="fe op oq or nr b">b</code>时，它返回<code class="fe op oq or nr b">true</code>。当两列的所有对应元素都相等时，认为这两列相等。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a88d" class="ms kz it nr b gy nv nw l nx ny">staffDF("salary") == 3200</span><span id="3473" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res0: Boolean = false</strong></span></pre><p id="e353" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">===</code>的一个问题是它消除了空值。例如，在下面的代码中</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a278" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df = Seq(<br/>  ("a", "a"),<br/>  ("b", "b"),<br/>  (<strong class="nr iu">null</strong>, <strong class="nr iu">null</strong>)).toDF("c1", "c2")</span><span id="3cf0" class="ms kz it nr b gy nz nw l nx ny">df.filter($"c1" === $"c2").show()</span><span id="f2e5" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+---+ <br/>| c1| c2| <br/>+---+---+ <br/>|  a|  a| <br/>|  b|  b| <br/>+---+---+</strong></span></pre><p id="2ff9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">$"c1" === $"c2"</code>删除<code class="fe op oq or nr b">c1</code>和<code class="fe op oq or nr b">c2</code>列中的空值。在这种情况下，我们可以使用等式运算符<code class="fe op oq or nr b">&lt;=&gt;</code>，它对于空值是安全的，并且不会消除它们:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b936" class="ms kz it nr b gy nv nw l nx ny">df.filter($"c1" &lt;=&gt; $"c2").show()</span><span id="5beb" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+----+----+ <br/>|  c1|  c2| <br/>+----+----+ <br/>|   a|   a| <br/>|   b|   b| <br/>|null|null| <br/>+----+----+</strong></span></pre><p id="46ca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">或者，<code class="fe op oq or nr b">filter()</code>的条件可以是一个字符串:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0592" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> filter(conditionExpr: <strong class="nr iu">String</strong>): Dataset[T]</span></pre><p id="8514" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里的<code class="fe op oq or nr b">conditionExpr</code>是一个布尔SQL表达式。表3给出了可以用于SQL表达式的布尔和逻辑运算符的列表。所以下面的代码</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="eb49" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> filteredDF = staffDF.filter($"salary" &gt; 3200 &amp;&amp; $"name" =!= "Ali")</span></pre><p id="09fc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">也可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9c22" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> filteredDF = staffDF.filter("salary &gt; 3200 and name != 'Ali'")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/4a521cc158cde7de047d9a03c2424f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7eikJ_M81wlxYUXYtrlNew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表3</p></figure><p id="a68d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">哪里()</strong></p><p id="8be6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1624" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1624" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">where()</code>也可用于使用给定条件过滤数据集或数据帧的行。相当于<code class="fe op oq or nr b">filter()</code>，使用方法相同。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="58db" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> where(condition: Column): Dataset[T] = filter(condition)</span></pre><p id="936e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">间()</strong></p><p id="ae00" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">方法<code class="fe op oq or nr b">between()</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L571" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L571" rel="noopener ugc nofollow" target="_blank">Column</a></code>中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b8b7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> between(lowerBound: Any, upperBound: Any): Column = {<br/>  (<strong class="nr iu">this</strong> &gt;= lowerBound) &amp;&amp; (<strong class="nr iu">this</strong> &lt;= upperBound)<br/>}</span></pre><p id="2f15" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它返回一个布尔型<code class="fe op oq or nr b">Column</code>对象。如果调用该列的<code class="fe op oq or nr b">Column</code>对象的相应行在下限和上限之间，则该列的每一行都是<code class="fe op oq or nr b">true</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ca1f" class="ms kz it nr b gy nv nw l nx ny">staffDF.select($"salary".between(3200,3600)).show()</span><span id="641a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------------------------------------+ <br/>|((salary &gt;= 3200) AND (salary &lt;= 3600))| <br/>+---------------------------------------+ <br/>|                                  false| <br/>|                                   true| <br/>|                                  false| <br/>|                                   true| <br/>|                                   true| <br/>+---------------------------------------+</strong></span></pre><h2 id="bf3e" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">缺失值的处理方法</strong></h2><p id="0b2d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> isNull() </strong></p><p id="2b5c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以使用类<code class="fe op oq or nr b">Column</code>中的方法<code class="fe op oq or nr b">isNull()</code>来过滤空值。还记得数据帧<code class="fe op oq or nr b">staffWithNullDF</code>，它被定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="06e1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> staffWithNullDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", null, 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", null, 4100),<br/>  (null, "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>  ).toDF("name", "role", "salary")</span><span id="886a" class="ms kz it nr b gy nz nw l nx ny">staffWithNullDF.show()</span><span id="09f8" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+ <br/>| name|         role|salary| <br/>+-----+-------------+------+ <br/>| John|         null|  4500| <br/>|James|Data engineer|  3200| <br/>|Laura|         null|  4100| <br/>| null|Data engineer|  3200| <br/>|Steve|    Developer|  3600| <br/>+-----+-------------+------+</strong></span></pre><p id="19d0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了过滤空值，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="77d5" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.filter($"role".isNull).show()</span><span id="9925" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+----+------+ <br/>| name|role|salary| <br/>+-----+----+------+ <br/>| John|null|  4500| <br/>|Laura|null|  4100| <br/>+-----+----+------+</strong></span></pre><p id="4ee1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您也可以使用SQL表达式来获得相同的结果:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6e5f" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.filter("role IS NULL").show()</span></pre><p id="7c56" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> isNotNull() </strong></p><p id="f887" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了过滤非<code class="fe op oq or nr b">null</code>的行，使用<code class="fe op oq or nr b">isNotNull()</code>方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3517" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.filter($"role".isNotNull).show()</span></pre><p id="9fa3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在包org.apache.spark.sql中定义了<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L36" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L36" rel="noopener ugc nofollow" target="_blank">DataFrameNaFunctions</a></code>，以添加处理数据帧中缺失数据的功能。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="be23" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">final</strong> <strong class="nr iu">class</strong> DataFrameNaFunctions <strong class="nr iu">private</strong>[sql](df: DataFrame) </span></pre><p id="4054" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将DataFrame作为其参数，并且该类中的方法可以处理丢失的数据。方法<code class="fe op oq or nr b">na</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L912" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L912" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中定义，返回<code class="fe op oq or nr b">DataFrameNaFunctions</code>的一个实例:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cdf3" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> na: DataFrameNaFunctions = <strong class="nr iu">new</strong> DataFrameNaFunctions(toDF())</span></pre><p id="4161" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将调用它的<code class="fe op oq or nr b">Dataset</code>对象转换为<code class="fe op oq or nr b">DataFrame</code>，并返回一个用它实例化的<code class="fe op oq or nr b">DataFrameNaFunctions</code>对象。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8236" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.na</span><span id="4c5a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res4: org.apache.spark.sql.DataFrameNaFunctions = org.apache.spark.sql.DataFrameNaFunctions@3e8f1f05</strong></span></pre><p id="1201" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当我们得到<code class="fe op oq or nr b">DataFrameNaFunctions</code>对象时，我们可以使用它的方法来处理丢失的数据。</p><p id="d1c4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> fill() </strong></p><p id="e0b5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了填充空值，我们可以使用<code class="fe op oq or nr b">fill()</code>的不同重载版本，它们在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L139" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L139" rel="noopener ugc nofollow" target="_blank">DataFrameNaFunctions</a></code>中可用。例如:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1397" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> fill(value: <strong class="nr iu">String</strong>): DataFrame</span></pre><p id="e6b4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">返回一个新的DataFrame，用<code class="fe op oq or nr b">value</code>替换字符串列中的空值。或者</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="04b7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> fill(value: <strong class="nr iu">Double</strong>): DataFrame</span></pre><p id="713c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">返回一个新的DataFrame，用<code class="fe op oq or nr b">value</code>替换数值列中的<code class="fe op oq or nr b">null</code>或<code class="fe op oq or nr b">NaN</code>值。例如，以下代码将所有字符串列中的空值替换为<code class="fe op oq or nr b">N/A</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0c8f" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.na.fill("N/A").show()</span><span id="48dd" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+ <br/>| name|         role|salary| <br/>+-----+-------------+------+ <br/>| John|          N/A|  4500| <br/>|James|Data engineer|  3200| <br/>|Laura|          N/A|  4100| <br/>|  N/A|Data engineer|  3200| <br/>|Steve|    Developer|  3600| <br/>+-----+-------------+------+</strong></span></pre><p id="b4f7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以指定应该替换空值的列的名称。列名列表可以以序列或数组的形式给出。例如，在前面的示例中，我们只能填充<code class="fe op oq or nr b">role</code>列的空值:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1f4e" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.na.fill("N/A", <strong class="nr iu">List</strong>("role")).show()</span><span id="5e21" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+ <br/>| name|         role|salary| <br/>+-----+-------------+------+ <br/>| John|          N/A|  4500| <br/>|James|Data engineer|  3200| <br/>|Laura|          N/A|  4100| <br/>| null|Data engineer|  3200| <br/>|Steve|    Developer|  3600| <br/>+-----+-------------+------+</strong></span></pre><p id="4308" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> drop() </strong></p><p id="054b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L91" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L91" rel="noopener ugc nofollow" target="_blank">DataFrameNaFunctions</a></code>中的方法<code class="fe op oq or nr b">drop()</code>来删除<code class="fe op oq or nr b">null</code>或<code class="fe op oq or nr b">NaN</code>值。此方法的一般形式声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e0a1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> drop(how: <strong class="nr iu">String</strong>, cols: <strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]): DataFrame</span></pre><p id="5b90" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当您在DataFrame上调用它时，它会删除指定列中包含<code class="fe op oq or nr b">null</code>或<code class="fe op oq or nr b">NaN</code>值的行，并将结果作为新的DataFrame返回。如果<code class="fe op oq or nr b">how</code>设置为<code class="fe op oq or nr b">any</code>，则删除指定列中至少包含一个<code class="fe op oq or nr b">null</code>或<code class="fe op oq or nr b">NaN</code>值的行。如果<code class="fe op oq or nr b">how</code>设置为<code class="fe op oq or nr b">all</code>，则仅删除所有指定列为<code class="fe op oq or nr b">null</code>或<code class="fe op oq or nr b">NaN</code>的行。</p><p id="7057" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果不指定列，那么它将应用于DataFrame的所有列，如果不指定<code class="fe op oq or nr b">how</code>，那么它的默认值是<code class="fe op oq or nr b">any</code>。列名也可以是一个<code class="fe op oq or nr b">Array</code>对象。这里有几个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d1c1" class="ms kz it nr b gy nv nw l nx ny">staffWithNullDF.na.drop().show()</span><span id="1612" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+ <br/>| name|         role|salary| <br/>+-----+-------------+------+ <br/>|James|Data engineer|  3200| <br/>|Steve|    Developer|  3600| <br/>+-----+-------------+------+</strong></span></pre><p id="b78e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">和</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a1fb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val </strong>someRows = <strong class="nr iu">Seq</strong>(<br/>  (<strong class="nr iu">null</strong>, <strong class="nr iu">null</strong>, 4500),<br/>  ("James", <strong class="nr iu">null</strong>, 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  (<strong class="nr iu">null</strong>, "Developer", 3600)<br/>)</span><span id="29d4" class="ms kz it nr b gy nz nw l nx ny">someRows.toDF("name", "role", "salary").na.drop("all", <strong class="nr iu">List</strong>("name", "role"))show()</span><span id="8a36" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>|James|          null|  3200| <br/>|Laura|Data scientist|  4100| <br/>|  Ali| Data engineer|  3200| <br/>| null|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre><h2 id="91c7" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">when()和otherwise()</h2><p id="8558" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">方法<code class="fe op oq or nr b">when()</code>和<code class="fe op oq or nr b">otherwise()</code>在类<code class="fe op oq or nr b">Column</code>中定义，通常一起使用。<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L522" rel="noopener ugc nofollow" target="_blank">When()</a></code>被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="848d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def </strong>when(condition: Column, value: Any): Column</span></pre><p id="667a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它对布尔列表达式<code class="fe op oq or nr b">condition</code>求值，并基于此返回一个新的<code class="fe op oq or nr b">Column</code>对象。如果<code class="fe op oq or nr b">condition</code>的对应行是<code class="fe op oq or nr b">true</code>，则结果列的每一行被设置为<code class="fe op oq or nr b">value</code>，如果是<code class="fe op oq or nr b">false</code>，则被设置为<code class="fe op oq or nr b">null</code>。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8065" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.select(F.when(staffDF("role") === "Data scientist", <br/>  3).alias("role code")).show()</span><span id="287d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------+ <br/>|role code| <br/>+---------+ <br/>|        3| <br/>|     null| <br/>|        3| <br/>|     null| <br/>|     null| <br/>+---------+</strong></span></pre><p id="3f9b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，当一行<code class="fe op oq or nr b">role</code>等于<code class="fe op oq or nr b">Data scientist</code>时，结果对应的一行是<code class="fe op oq or nr b">3</code>。否则就是<code class="fe op oq or nr b">null</code>。您可以链接<code class="fe op oq or nr b">when()</code>方法。因此<code class="fe op oq or nr b">when()</code>可以应用于之前由另一个<code class="fe op oq or nr b">when()</code>方法生成的列。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L554" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L554" rel="noopener ugc nofollow" target="_blank">otherwise()</a></code>只能在<code class="fe op oq or nr b">when()</code>链的末端使用。它被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="929d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> otherwise(value: Any): Column</span></pre><p id="3540" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将用<code class="fe op oq or nr b">value</code>替换由<code class="fe op oq or nr b">when()</code>链产生的列的所有剩余的<code class="fe op oq or nr b">null</code>元素。所以你不会得到任何空值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b7e7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.select(F.when(staffDF("role") === "Data scientist", 3)<br/>  .when(staffDF("role") === "Data engineer", 2)<br/>  .otherwise(1).alias("role code"))<br/>  .show()</span><span id="7039" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------+ <br/>|role code| <br/>+---------+ <br/>|        3| <br/>|        2| <br/>|        3| <br/>|        2| <br/>|        1| <br/>+---------+</strong></span></pre><p id="1bdd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">下面是另一个当<code class="fe op oq or nr b">role</code>为<code class="fe op oq or nr b">Data scientist</code>时将<code class="fe op oq or nr b">50</code>加到<code class="fe op oq or nr b">salary</code>的例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="561f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.withColumn("salary", F.when($"role" === "Data scientist", <br/>  $"salary" + 50).otherwise($"salary")).show()</span><span id="7ed2" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Otherwise<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  4550| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4150| <br/>|  Ali| Data engineer|  3200| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="ce7b" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">分区</strong></h1><p id="4f6c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如前所述，在Spark RDDs中，数据集和数据帧是分布式数据结构，因此Spark将它们分开并分布在集群的工作节点中。这种分布式数据集的每个分割被称为一个<em class="oa">分区</em>。事实上，在Spark中，分区是存储在集群中一个节点上的逻辑数据块。分区是Spark中并行的基本单位，rdd、数据集和数据帧是分区的集合。Spark集群中的每个节点都包含一个或多个分区。但是，一个分区不能跨越多个节点。Spark中的分区数量是可配置的，太少或太多的分区都会降低性能。</p><p id="0560" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">Spark可以为RDD的每个分区运行一个并发任务。然而，并发任务的总数受到集群中核心总数的限制。(每个节点可以有多个核心。)比如一个集群有十个核心，那么Spark最多可以运行十个并发任务。因此，这样一个集群上的RDD应该至少有十个分区。(实际上是两到四倍。)如果您的分区少于10个，一些内核会保持空闲，这会导致并发性降低。</p><p id="07f2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">另一方面，如果您有太多的分区，那么Spark中许多小任务的任务调度可能会比实际执行时间花费更多的时间。此外，节点之间的数据通信非常昂贵，所以Spark变得很慢。因此，在决定分区数量时，总会有一个权衡。根据经验，分区的数量应该是集群中核心数量的一到四倍。</p><h2 id="22af" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">getNumPartitions()</h2><p id="1be2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要获得一个数据帧的分区数量，您应该首先使用在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3239" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3239" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中定义的方法<code class="fe op oq or nr b">rdd()</code>将其转换为RDD。它被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cbd4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">lazy val</strong> rdd: RDD[T]</span></pre><p id="5f84" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将数据集的内容表示为类型为<code class="fe op oq or nr b">T</code>的<code class="fe op oq or nr b">RDD</code>。然后，可以使用方法<code class="fe op oq or nr b">getNumPartitions()</code>来获得该RDD中的分区数量:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="dcb2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> x  = (1 to 12).toList<br/><strong class="nr iu">val</strong> numberDF = x.toDF("number")<br/>numberDF.rdd.getNumPartitions</span><span id="e78a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res3: Int = 8</strong></span></pre><p id="bb6f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以这个数据帧有八个分区。(在不同的集群上，该数字可能会有所不同。)</p><h2 id="a66b" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">分区()</h2><p id="f7bf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">您也可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L295" rel="noopener ugc nofollow" target="_blank">抽象类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L295" rel="noopener ugc nofollow" target="_blank">RDD</a></code>中的方法<code class="fe op oq or nr b">partitions()</code>来获得相同的结果。它返回一个RDD的分区数组:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cada" class="ms kz it nr b gy nv nw l nx ny">numberDF.rdd.partitions.size</span></pre><p id="7c2c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">事实上，八是这个集群的默认分区数。<code class="fe op oq or nr b">sparkContext</code>中的方法<code class="fe op oq or nr b">defaultParallelism</code>给出了默认的分区数量:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1f7e" class="ms kz it nr b gy nv nw l nx ny">spark.sparkContext.defaultParallelism</span><span id="6f15" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res3: Int = 8</strong></span></pre><h2 id="8666" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">mapPartitionsWithIndex()</h2><p id="520a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们还可以看到数据帧的行是如何在分区之间分布的。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L909" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L909" rel="noopener ugc nofollow" target="_blank">RDD</a></code>中的方法<code class="fe op oq or nr b">mapPartitionsWithIndex()</code>通过将函数应用于被调用的RDD的每个分区来返回新的RDD。同时跟踪原始分区的索引。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e2fd" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> mapPartitionsWithIndex[U: ClassTag](f: (<strong class="nr iu">Int</strong>, Iterator[T]) =&gt; <br/>  Iterator[U], preservesPartitioning: <strong class="nr iu">Boolean</strong> = false): RDD[U]</span></pre><p id="135e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了给前面定义的<code class="fe op oq or nr b">numberDF</code>的每一行添加一个分区ID，我们可以使用下面的代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f349" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">import</strong> org.apache.spark.sql.Row<br/><strong class="nr iu">val</strong> rows = numberDF.rdd.mapPartitionsWithIndex{ <br/>  (index, itr) =&gt; <br/>    itr.toList.map(x =&gt; Row.fromSeq(index +: x.toSeq)).iterator }.collect()<br/><strong class="nr iu">val</strong> someSchema = StructField("partitionIndex", IntegerType, true) +: <br/>  numberDF.schema.fields<br/><strong class="nr iu">val</strong> numberDFWithPartitionID = <br/>  spark.createDataFrame(spark.sparkContext.parallelize(rows), <br/>  StructType(someSchema))<br/>numberDFWithPartitionID.show()</span><span id="3c56" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+------+ <br/>|partitionIndex|number| <br/>+--------------+------+ <br/>|             0|     1| <br/>|             1|     2| <br/>|             1|     3| <br/>|             2|     4| <br/>|             3|     5| <br/>|             3|     6| <br/>|             4|     7| <br/>|             5|     8| <br/>|             5|     9| <br/>|             6|    10| <br/>|             7|    11| <br/>|             7|    12| <br/>+--------------+------+</strong></span></pre><p id="5f09" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，<code class="fe op oq or nr b">mapPartitionsWithIndex()</code>中的文字函数遍历了<code class="fe op oq or nr b">numberDF</code>的RDD的所有分区。它需要两个参数。第一个是分区索引，第二个是每个分区中的行。我们将每个分区中的行转换成一个列表，并将分区索引添加到每个对象行中。我们还需要更改模式，为分区索引添加一个新列。最后，我们创建一个带有<code class="fe op oq or nr b">partitionIndex</code>列的新DataFrame。该列给出了该行所属分区的索引。如输出所示，每个分区包含两行<code class="fe op oq or nr b">numberDF</code>。</p><h2 id="6d19" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">重新分区()</h2><p id="079d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">您可以更改数据集或数据帧的分区数量。该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a673" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> repartition(numPartitions: <strong class="nr iu">Int</strong>): Dataset[T] = withTypedPlan {    <br/>  Repartition(numPartitions, shuffle = true, logicalPlan)  }</span></pre><p id="d9bb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">in <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#3015" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#3015" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>返回一个新的数据集，它正好有<code class="fe op oq or nr b">numPartitions</code>个分区。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="dc1c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> repNumberDF = numberDF.repartition(3)<br/>repNumberDF.rdd.partitions.size</span><span id="7529" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: Int = 3</strong></span></pre><p id="bf6a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以看到行是如何在这三个分区之间分布的:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e553" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_</strong><br/><strong class="nr iu">import</strong> org.apache.spark.sql.Row<br/><strong class="nr iu">val</strong> rows = repNumberDF.rdd.mapPartitionsWithIndex{ <br/>  (index, itr) =&gt; <br/>    itr.toList.map(x =&gt; Row.fromSeq(index +: x.toSeq)).iterator }.collect()<br/><strong class="nr iu">val</strong> someSchema = StructField("partitionIndex", IntegerType, true) +: <br/>  repNumberDF.schema.fields<br/><strong class="nr iu">val</strong> repNumberDFWithPartitionID = <br/>  spark.createDataFrame(spark.sparkContext.parallelize(rows), <br/>  StructType(someSchema))<br/>repNumberDFWithPartitionID.show()</span><span id="3a1e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+------+ <br/>|partitionIndex|number| <br/>+--------------+------+ <br/>|             0|     5| <br/>|             0|     7| <br/>|             0|     8| <br/>|             0|    12| <br/>|             1|     1| <br/>|             1|     3| <br/>|             1|     6| <br/>|             1|     9| <br/>|             2|     2| <br/>|             2|     4| <br/>|             2|    10| <br/>|             2|    11| <br/>+--------------+------+</strong></span></pre><h2 id="9ee2" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">联合()</h2><p id="a976" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们还可以使用类<code class="fe op oq or nr b">Dataset</code>中的方法<code class="fe op oq or nr b">coalesce()</code>来返回一个新的数据集，它正好有<code class="fe op oq or nr b">numPartitions</code>个分区:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8444" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> coalesce(numPartitions: <strong class="nr iu">Int</strong>): Dataset[T] = withTypedPlan {    <br/>  Repartition(numPartitions, shuffle = false, logicalPlan)  }</span></pre><p id="3129" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如你所见，<code class="fe op oq or nr b">coalesce()</code>和<code class="fe op oq or nr b">repartition()</code>的唯一区别是<code class="fe op oq or nr b">shuffle</code>的值。在<code class="fe op oq or nr b">coalesce()</code>中，<code class="fe op oq or nr b">shuffle</code>为<code class="fe op oq or nr b">false</code>，而在<code class="fe op oq or nr b">partition()</code>中为真。所以<code class="fe op oq or nr b">repartition()</code>打乱了数据。当您将其应用于数据帧时，行的顺序可能会改变。它可以增加和减少原始数据集中的分区数量。但是，<code class="fe op oq or nr b">coalesce()</code>只能减少分区的数量。如果请求更大数量的分区，它将保持当前的分区数量。原因是它不能混洗数据。让我们用<code class="fe op oq or nr b">coalesce()</code>来减少<code class="fe op oq or nr b">numberDF</code>中的分区数量。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="458e" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> coalNumberDF = numberDF.coalesce(3)<br/><strong class="nr iu">val</strong> rows = coalNumberDF.rdd.mapPartitionsWithIndex{ <br/>  (index, itr) =&gt; <br/>    itr.toList.map(x =&gt; Row.fromSeq(index +: x.toSeq)).iterator }.collect()<br/><strong class="nr iu">val</strong> someSchema = StructField("partitionIndex", IntegerType, true) +: <br/>  coalNumberDF.schema.fields<br/><strong class="nr iu">val</strong> coalNumberDFWithPartitionID = <br/>  spark.createDataFrame(spark.sparkContext.parallelize(rows), <br/>  StructType(someSchema))<br/>coalNumberDFWithPartitionID.show()</span><span id="fedf" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+------+ <br/>|partitionIndex|number| <br/>+--------------+------+ <br/>|             0|     1| <br/>|             0|     2| <br/>|             0|     3| <br/>|             1|     4| <br/>|             1|     5| <br/>|             1|     6| <br/>|             1|     7| <br/>|             2|     8| <br/>|             2|     9| <br/>|             2|    10| <br/>|             2|    11| <br/>|             2|    12| <br/>+--------------+------+</strong></span></pre><p id="16e3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果您将原始数据帧(<code class="fe op oq or nr b">numberDFWithPartitionID</code>)的分区与重新分区的数据帧(<code class="fe op oq or nr b">repNumberDFWithPartitionID</code>)和合并的数据帧(<code class="fe op oq or nr b">coalNumberDFWithPartitionID</code>)的分区进行比较，您会看到<code class="fe op oq or nr b">repartition()</code>对原始行进行了洗牌。它创建新的分区，并在这些分区中平均分配被打乱的数据。这是一个相当昂贵的操作，因为混洗的数据应该在节点之间传送。</p><p id="41b4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">coalesce()</code>类似于<code class="fe op oq or nr b">repartition()</code>的优化版本，因为它最大限度地减少了集群节点之间的数据移动。它不会打乱数据，只是简单地按照原始顺序组合行，以获得更少的分区。比如在原来的<code class="fe op oq or nr b">numberDF</code>中，可以将分区5的数据保留在它们原来的节点中，只添加分区6和分区7的数据，以尽量减少节点间的数据移动。当然，你只能使用<code class="fe op oq or nr b">coalesce()</code>来减少分区数量。</p><h2 id="ef94" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">添加索引</strong></h2><p id="40c6" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Spark数据集和数据帧本质上是无序的。它们不支持随机访问，所以它们没有像Python中的Pandas库那样的内置索引。数据帧是Spark中的分布式数据结构，Spark集群的每个节点都可以存储和处理其中的一部分，而不用担心行的顺序。事实上，数据帧的每一行都被认为是一个独立的结构化数据集合，从而能够支持分布式并行处理。但是，可以向数据帧中添加一个新列，起到索引的作用。</p><p id="aff9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">单调_递增_id() </strong></p><p id="1f13" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L1214" rel="noopener ugc nofollow" target="_blank">对象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L1214" rel="noopener ugc nofollow" target="_blank">functions</a></code>中的方法<code class="fe op oq or nr b">monotonically_increasing_id()</code>返回一个64位整数单调递增的<code class="fe op oq or nr b">Column</code>对象。该列的行保证是单调递增且唯一的，但不是连续的。该方法定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2fd9" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> monotonically_increasing_id(): Column = withExpr { MonotonicallyIncreasingID() }</span></pre><p id="6e04" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它调用方法<code class="fe op oq or nr b">withExpr()</code>，该方法也在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L80" rel="noopener ugc nofollow" target="_blank">对象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L80" rel="noopener ugc nofollow" target="_blank">functions</a></code>中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="90eb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">private def</strong> withExpr(expr: Expression): Column = Column(expr)</span></pre><p id="0cd9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它使用<code class="fe op oq or nr b">expr</code>创建一个<code class="fe op oq or nr b">Column</code>对象。所以<code class="fe op oq or nr b">monotonically_increasing_id()</code>使用case类<code class="fe op oq or nr b">MonotonicallyIncreasingID </code>作为表达式创建一个新的<code class="fe op oq or nr b">Column</code>对象。这个case类在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/MonotonicallyIncreasingID.scala#L50" rel="noopener ugc nofollow" target="_blank">包org . Apache . spark . SQL . catalyst . expressions</a>中定义，它是非确定性方法的一个例子。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4c37" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> MonotonicallyIncreasingID() <strong class="nr iu">extends</strong> LeafExpression <strong class="nr iu">with</strong> <br/>  Stateful</span></pre><p id="fe61" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在Spark中，如果一个表达式对于相同的输入得到相同的结果，那么它就是<em class="oa">确定性的</em>。<code class="fe op oq or nr b">monotonically_increasing_id()</code>不是确定性的，因为它的结果取决于分区id。它扩展了特征<code class="fe op oq or nr b">Stateful</code>，该特征在包org . Apache . spark . SQL . catalyst . expressions中的<code class="fe op oq or nr b">Expression.scala</code>中定义<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L433" rel="noopener ugc nofollow" target="_blank">特征</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L433" rel="noopener ugc nofollow" target="_blank">Stateful</a></code>扩展了特征<code class="fe op oq or nr b">Nondeterministic</code>，该特征用于不确定的表达式。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5f0f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">trait</strong> Stateful <strong class="nr iu">extends</strong> Nondeterministic { ...</span></pre><p id="5f68" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所有扩展<code class="fe op oq or nr b">Nondeterministic</code>的类都应该实现两个抽象方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="29c6" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">protected def</strong> initializeInternal(partitionIndex: Int): Unit <br/><strong class="nr iu">protected def</strong> evalInternal(input: InternalRow): Any</span></pre><p id="3d83" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">第一个函数初始化一个不确定的表达式，第二个函数对它求值。<code class="fe op oq or nr b">MonotonicallyIncreasingId</code>有一个<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/MonotonicallyIncreasingID.scala#L56" rel="noopener ugc nofollow" target="_blank">计数器字段</a>对记录进行计数:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="97f5" class="ms kz it nr b gy nv nw l nx ny">@transient <strong class="nr iu">private</strong>[<strong class="nr iu">this</strong>] <strong class="nr iu">var </strong>count: Long = _</span></pre><p id="50f7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">作为一个非确定性表达式，<code class="fe op oq or nr b">MonotonicallyIncreasingID</code>在计算一个值之前需要显式初始化。对于每个分区，<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/MonotonicallyIncreasingID.scala#L60" rel="noopener ugc nofollow" target="_blank">initializeInternal()</a></code>将<code class="fe op oq or nr b">count</code>重置为零，并将分区索引(<code class="fe op oq or nr b">partitionIndex</code>)移位33位。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3434" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">override protected def </strong>initializeInternal(partitionIndex: <strong class="nr iu">Int</strong>): <strong class="nr iu">Unit </strong>= {  <br/>  count = 0<strong class="nr iu">L</strong><br/>  partitionMask = partitionIndex.toLong &lt;&lt; 33  }   </span></pre><p id="b79e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于该分区中的每条记录，<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/MonotonicallyIncreasingID.scala#L69" rel="noopener ugc nofollow" target="_blank">evalInternal()</a></code>将移位后的分区索引加到记录号上，并将其作为该记录的ID返回:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="615b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">override protected def</strong> evalInternal(input: InternalRow): <strong class="nr iu">Long </strong>= { <br/>  <strong class="nr iu">val</strong> currentCount = count    <br/>  count += 1 <br/>  partitionMask + currentCount  }</span></pre><p id="e9dc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以id通常不是连续的；然而，在每个分区中，id是连续的。例如，考虑之前创建的数据帧<code class="fe op oq or nr b">numberDF</code>。我们将其重新分区为三个分区，然后向其中添加一个ID列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9450" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> numberWithID = repNumberDF.withColumn("id", <br/>  F.monotonically_increasing_id())<br/>numberWithID.show()</span><span id="94a6" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+-----------+ <br/>|number|         id| <br/>+------+-----------+ <br/>|     5|          0| <br/>|     7|          1| <br/>|     8|          2| <br/>|    12|          3| <br/>|     1| 8589934592| <br/>|     3| 8589934593| <br/>|     6| 8589934594| <br/>|     9| 8589934595| <br/>|     2|17179869184| <br/>|     4|17179869185| <br/>|    10|17179869186| <br/>|    11|17179869187| <br/>+------+-----------+</strong></span></pre><p id="714a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">图6显示了如何计算每行的id。例如，<code class="fe op oq or nr b">numberWithID</code>的最后一行属于分区2，所以分区ID是:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ae70" class="ms kz it nr b gy nv nw l nx ny">2L &lt;&lt; 33</span><span id="6a7d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: Long = 17179869184</strong></span></pre><p id="9921" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于这个分区中有四行，它们的id将是:<code class="fe op oq or nr b">17179869184+0</code>，… <code class="fe op oq or nr b">17179869184+3</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/22a3824e813782a9bb0c3680bc09c58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8VecG-YhHpajOQDI8tQRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6</p></figure><p id="0972" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了在所有行中都有连续的id，我们可以对数据帧进行重新分区，使其只有一个分区。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="76b7" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> repOneNumberDF = numberDF.coalesce(1)<br/>repOneNumberDF.withColumn("id",  <br/>  F.monotonically_increasing_id()).show()</span><span id="3c9a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+---+ <br/>|number| id| <br/>+------+---+ <br/>|     1|  0| <br/>|     2|  1| <br/>|     3|  2| <br/>|     4|  3| <br/>|     5|  4| <br/>|     6|  5| <br/>|     7|  6| <br/>|     8|  7| <br/>|     9|  8| <br/>|    10|  9| <br/>|    11| 10| <br/>|    12| 11| <br/>+------+---+</strong></span></pre><p id="a8ed" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">ziptwithindex()</strong></p><p id="be40" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们也可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1388" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1388" rel="noopener ugc nofollow" target="_blank">RDD</a></code>的方法<code class="fe op oq or nr b">zipWithIndex()</code>来生成一个ID列。该方法被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="671b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> zipWithIndex(): RDD[(T, <strong class="nr iu">Long</strong>)]</span></pre><p id="6f6e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它压缩了调用它的RDD及其元素索引。这个RDD拥有类型为<code class="fe op oq or nr b">T</code>的对象，结果是一个拥有类型为<code class="fe op oq or nr b">(T, Long)</code>的对象的RDD。因此，得到的RDD的每条记录都是一个元组，它将原始RDD的记录与类型为<code class="fe op oq or nr b">Long</code>的索引组合在一起。指数从零开始，增加1。</p><p id="c55f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">排序首先基于分区索引，然后基于每个分区内的项目排序。第一个分区中的第一个项目的索引为0，最后一个分区中的最后一个项目的索引最大。还记得我们之前创建的数据帧<code class="fe op oq or nr b">repNumberDF</code>吗？它有三个分区。然后我们添加了一个ID列，结果是<code class="fe op oq or nr b">numberWithID</code>。我们可以使用<code class="fe op oq or nr b">zipWithIndex()</code>向<code class="fe op oq or nr b">repNumberDF</code>添加一个ID列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="be68" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> zippedRDD= repNumberDF.rdd.zipWithIndex()</span><span id="63a6" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: Array[(org.apache.spark.sql.Row, Long)] = Array(([1],0), ([2],1), ([3],2), ([4],3), ([5],4), ([6],5), ([7],6), ([8],7), ([9],8), ([10],9), ([11],10), ([12],11))</strong></span></pre><p id="48fe" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">结果是一个RDD，其中每个记录都是一个元组。这个元组的第一个元素是<code class="fe op oq or nr b">repNumberDF</code>的原始记录，第二个元素是它生成的索引。要从该RDD创建数据帧，我们可以使用以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bccf" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_<br/>import</strong> org.apache.spark.sql.Row<strong class="nr iu"><br/>import</strong> org.apache.spark.sql.types._<br/><strong class="nr iu">val</strong> rows= zippedRDD.map(<br/>  r =&gt; Row.fromSeq(r._1.toSeq ++ <strong class="nr iu">Seq</strong>(r._2))<br/>  )<br/><strong class="nr iu">val</strong> someSchema =  repNumberDF.schema.fields ++ <br/>  <strong class="nr iu">Array</strong>(StructField("id", LongType, <strong class="nr iu">true</strong>))  <br/><strong class="nr iu">val</strong> dfWithID = spark.createDataFrame(rows, StructType(someSchema))<br/>dfWithID.show()</span><span id="70ca" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------+---+ <br/>|number| id| <br/>+------+---+ <br/>|     1|  0| <br/>|     2|  1| <br/>|     3|  2| <br/>|     4|  3| <br/>|     5|  4| <br/>|     6|  5| <br/>|     7|  6| <br/>|     8|  7| <br/>|     9|  8| <br/>|    10|  9| <br/>|    11| 10| <br/>|    12| 11| <br/>+------+---+</strong></span></pre><p id="d853" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了创建一个数据帧，我们首先映射<code class="fe op oq or nr b">zippedRDD</code>的记录，从它们创建<code class="fe op oq or nr b">Row</code>对象。现在每一行都有一个索引。因此应该相应地修改原始数据帧的模式。我们向该模式添加索引，然后使用该模式创建一个新的数据帧。在<code class="fe op oq or nr b">repNumberDF</code>中，包含4、5和6的行属于分区0，因此它们的索引是0、1和2。事实上，<code class="fe op oq or nr b">dfWithID</code>和<code class="fe op oq or nr b">numberWithID</code>中的行的顺序是完全相同的，因为排序首先基于分区索引，然后基于每个分区内的项目排序。</p><p id="f37c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您也可以创建<code class="fe op oq or nr b">dfWithID</code>，使<code class="fe op oq or nr b">id</code>列成为第一列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4879" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types.<strong class="nr iu">_<br/>import</strong> org.apache.spark.sql.Row<strong class="nr iu"><br/>val</strong> rows= zippedRDD.map{<br/>  <strong class="nr iu">case</strong> (row, index) =&gt; Row.fromSeq(index +: row.toSeq)<br/>  }<br/><strong class="nr iu">val</strong> someSchema = StructField("id", LongType, <strong class="nr iu">true</strong>) +:<br/>  repNumberDF.schema.fields   <br/><strong class="nr iu">val</strong> dfWithID = spark.createDataFrame(rows, StructType(someSchema))</span></pre><p id="41d0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">一旦有了<code class="fe op oq or nr b">id</code>列，就可以用它来过滤结果。例如，要从<code class="fe op oq or nr b">dfWithID</code>中选择索引为1到3的行，您可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9ab3" class="ms kz it nr b gy nv nw l nx ny">dfWithID.filter($"id".between(1, 3)).show()</span><span id="566e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+------+ <br/>| id|number| <br/>+---+------+ <br/>|  1|     2| <br/>|  2|     3| <br/>|  3|     4|<br/>+---+------+</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="a1ff" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">保存数据集</strong></h1><h2 id="cfa1" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">写()</h2><p id="bd8e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3367" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3367" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">write()</code>充当一个接口，将非流式数据集的内容保存到外部存储中。它被定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b5b1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> write: DataFrameWriter[T] = {  <br/>  <strong class="nr iu">if</strong> (isStreaming) {  <br/>    logicalPlan.failAnalysis("'write' can not be called on streaming <br/>      Dataset/DataFrame") <br/>  } <br/>  <strong class="nr iu">new</strong> DataFrameWriter[T](this)<br/>}</span></pre><p id="79ec" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它从调用它的数据集创建一个<code class="fe op oq or nr b">DataFrameWriter</code>对象。最后一个类<code class="fe op oq or nr b">DataFrameWriter</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#51" rel="noopener ugc nofollow" target="_blank">包org.apache.spark.sql </a>中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="11ca" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">final class</strong> DataFrameWriter[T] <strong class="nr iu">private</strong>[sql](ds: Dataset[T]) {    <br/>  <strong class="nr iu">private val</strong> df = ds.toDF()<br/>  ...</span></pre><p id="4ef3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将传递给它的数据集(<code class="fe op oq or nr b">ds</code>)转换成数据帧(<code class="fe op oq or nr b">df</code>)，其方法使用该数据帧进行进一步的操作。它的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#84" rel="noopener ugc nofollow" target="_blank">方法</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#84" rel="noopener ugc nofollow" target="_blank">mode()</a></code>指定了当要保存的数据集已经存在时<code class="fe op oq or nr b">DataFrameWriter</code>对象的行为。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a652" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> mode(saveMode: <strong class="nr iu">String</strong>): DataFrameWriter[T] = {    <br/>  saveMode.toLowerCase(Locale.ROOT) <strong class="nr iu">match</strong> { <br/>    <strong class="nr iu">case</strong> "overwrite" =&gt; mode(SaveMode.Overwrite)<br/>    <strong class="nr iu">case</strong> "append" =&gt; mode(SaveMode.Append)      <br/>    <strong class="nr iu">case</strong> "ignore" =&gt; mode(SaveMode.Ignore)     <br/>    <strong class="nr iu">case</strong> "error" | "errorifexists" | "default" =&gt; <br/>      mode(SaveMode.ErrorIfExists)     <br/>    <strong class="nr iu">case</strong> <strong class="nr iu">_</strong> =&gt; <strong class="nr iu">throw new</strong> IllegalArgumentException(s"Unknown save <br/>      mode: $saveMode. Accepted " + "save modes are 'overwrite', <br/>      'append', 'ignore', 'error', 'errorifexists', 'default'.")    <br/>  }   <br/>}</span></pre><p id="4a03" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下是<code class="fe op oq or nr b">saveMode</code>可能采用的值及其效果:</p><ul class=""><li id="ff35" class="ob oc it ls b lt mn lw mo lz od md oe mh of ml og oh oi oj bi translated"><code class="fe op oq or nr b">overwrite</code>:覆盖现有数据</li><li id="b59a" class="ob oc it ls b lt ok lw ol lz om md on mh oo ml og oh oi oj bi translated"><code class="fe op oq or nr b">append</code>:追加数据</li><li id="63eb" class="ob oc it ls b lt ok lw ol lz om md on mh oo ml og oh oi oj bi translated"><code class="fe op oq or nr b">ignore</code>:忽略操作(即不操作)</li><li id="fe5c" class="ob oc it ls b lt ok lw ol lz om md on mh oo ml og oh oi oj bi translated"><code class="fe op oq or nr b">error</code>、<code class="fe op oq or nr b">errorIfExists</code>或<code class="fe op oq or nr b">default</code>:运行时抛出异常(这是默认选项)</li></ul><p id="e2e5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">format()</code>是 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#100" rel="noopener ugc nofollow" target="_blank">DataFrameWriter</a></code>的另一个<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#100" rel="noopener ugc nofollow" target="_blank">方法，它指定了数据集将被保存的格式。内置选项有parquet、JSON、CSV等。</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="71e2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> format(source: <strong class="nr iu">String</strong>): DataFrameWriter[T] = {<br/>  <strong class="nr iu">this</strong>.source = source<br/>  <strong class="nr iu">this</strong><br/>}</span></pre><p id="d7e6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">DataFrameWriter</code>中的方法<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L124" rel="noopener ugc nofollow" target="_blank">option()</a></code>为底层数据源添加了一个输出选项，并声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d86a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> option(key: <strong class="nr iu">String</strong>, value: <strong class="nr iu">String</strong>): DataFrameWriter[T]</span></pre><p id="2ae0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它需要一个键和一个值。<code class="fe op oq or nr b">value</code>表示布尔值，也可以是Boolean、Long或Double，将转换为字符串。您可以在文档中找到所有可在<code class="fe op oq or nr b">option</code>中使用的键的<a class="ae mm" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html" rel="noopener ugc nofollow" target="_blank">列表。例如，对于CSV文件，<code class="fe op oq or nr b">option("header", "true")</code>将列名作为第一行写入。<code class="fe op oq or nr b">DataFrameWriter</code>中的方法<code class="fe op oq or nr b">save()</code>将数据集保存在指定路径:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="fb9b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> save(path: <strong class="nr iu">String</strong>): Unit </span></pre><p id="f965" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，要将<code class="fe op oq or nr b">staffDF</code>保存为CSV格式，我们可以编写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f384" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> path = "/tmp/staff_ds.csv"<br/>staffDF.write.mode("overwrite")<br/>  .option("header", "true").format("csv").save(path)</span></pre><p id="e38f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用方法<code class="fe op oq or nr b">csv()</code>，它将数据集以CSV格式保存在指定的路径:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5b4e" class="ms kz it nr b gy nv nw l nx ny">staffDF.write.mode("overwrite")<br/>  .option("header", "true").csv(path)</span></pre><p id="0311" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">还有其他方法，像<code class="fe op oq or nr b">text()</code>、<code class="fe op oq or nr b">parquet()</code>等。，它可以以其他格式保存数据集。需要注意的是，数据集的每个分区都将保存在不同的文件中。这里的每一行<code class="fe op oq or nr b">staffDF</code>都保存在一个单独的CSV文件中。由于<code class="fe op oq or nr b">staffDF</code>只有5行，其中一个文件没有存储任何行。一个名为<code class="fe op oq or nr b">staff_ds.csv</code>的文件夹被创建，这些CSV文件存储在其中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/300976e6fba4c7e17dc12abc1ca2e109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSD3K95ouNyHqhdaadE2TA.png"/></div></div></figure><p id="d1a3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果需要单个输出文件，可以先用<code class="fe op oq or nr b">repartition()</code>或者<code class="fe op oq or nr b">coalesce()</code>拥有一个只有一个分区的数据集，然后保存。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7826" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> path = "/tmp/staff_df"<br/><strong class="nr iu">val</strong> coalesced = staffDF.coalesce(1)<br/>coalesced.write.mode("overwrite").option("header", <br/>  "true").format("csv").save(path)</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="9457" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">分类</strong></h1><h2 id="0cbc" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">排序()</h2><p id="326f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c087" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> sort(sortCol: <strong class="nr iu">String</strong>, sortCols: <strong class="nr iu">String</strong>*): Dataset[T]</span></pre><p id="5ee8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">in <a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1280" rel="noopener ugc nofollow" target="_blank"> class </a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1280" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>对由指定列调用的数据集进行排序(全部按升序)，并将结果作为新的数据集返回。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b83e" class="ms kz it nr b gy nv nw l nx ny">staffDF.sort("name").show()</span><span id="8214" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>|  Ali| Data engineer|  3200| <br/>|James| Data engineer|  3200| <br/>| John|Data scientist|  4500| <br/>|Laura|Data scientist|  4100| <br/>|Steve|     Developer|  3600| <br/>+-----+--------------+------+</strong></span></pre><h2 id="8d24" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">desc()</h2><p id="b7ee" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果您需要按降序对列进行排序，那么您需要使用它的重载变量，该变量接受<code class="fe op oq or nr b">Column</code>对象:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9bce" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> sort(sortExprs: Column*): Dataset[T]</span></pre><p id="ee25" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在你可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1219" rel="noopener ugc nofollow" target="_blank">类</a>中的<code class="fe op oq or nr b">desc()</code>方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="436d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> desc: Column = withExpr { SortOrder(expr, Descending) }</span></pre><p id="d8d9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它根据调用它的列的降序返回排序表达式。所以我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5f7d" class="ms kz it nr b gy nv nw l nx ny">staffDF.sort($"name".desc).show()</span><span id="a88b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>|Steve|     Developer|  3600| <br/>|Laura|Data scientist|  4100| <br/>| John|Data scientist|  4500| <br/>|James| Data engineer|  3200| <br/>|  Ali| Data engineer|  3200| <br/>+-----+--------------+------+</strong></span></pre><p id="1673" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L182" rel="noopener ugc nofollow" target="_blank">对象</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L182" rel="noopener ugc nofollow" target="_blank">functions</a></code>中的方法<code class="fe op oq or nr b">desc()</code>做同样的事情:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="64c5" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> desc(columnName: <strong class="nr iu">String</strong>): Column = Column(columnName).desc</span></pre><p id="3f92" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所以我们也可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ce29" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.sort(F.desc("name")).show()</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="1513" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">聚合</strong></h1><h2 id="66cb" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">groupBy()</h2><p id="4b7d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1658" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1658" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">groupBy()</code>用于将数据集中特定列的相同数据收集成组，并对分组后的数据进行聚合。该方法定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cbbb" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> groupBy(cols: Column*): RelationalGroupedDataset = {<br/>  RelationalGroupedDataset(toDF(), cols.map(_.expr), <br/>  RelationalGroupedDataset.GroupByType)  }</span></pre><p id="8fa9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">并将<code class="fe op oq or nr b">Column</code>对象的名称作为参数。但是您也可以传递列名:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b90b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> groupBy(col1: <strong class="nr iu">String</strong>, cols: <strong class="nr iu">String*</strong>): RelationalGroupedDataset</span></pre><p id="477c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它返回一个类<code class="fe op oq or nr b">RelationalGroupedDataset</code>的对象。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L49" rel="noopener ugc nofollow" target="_blank">这个类</a>在org.apache.spark.sql中定义，它有一套在由<code class="fe op oq or nr b">groupby()</code>创建的数据帧上聚合的方法。使用<code class="fe op oq or nr b">toDF()</code>将调用<code class="fe op oq or nr b">groupBy()</code>的数据集转换成数据帧(如果它还不是数据帧),并作为<code class="fe op oq or nr b">RelationalGroupedDataset</code>的第一个参数传递。第二个参数是使用<code class="fe op oq or nr b">map()</code>创建的<code class="fe op oq or nr b">groupBy()</code>中指定的列的内部Catalyst表达式列表。最后一个参数表示分组类型。</p><p id="03a5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">特征<code class="fe op oq or nr b">GroupType</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L635" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L635" rel="noopener ugc nofollow" target="_blank">RelationalGroupedDataset</a></code>中定义。扩展该特征的对象表示操作的类型分组类型，包括<code class="fe op oq or nr b">GroupByType</code>、<code class="fe op oq or nr b">CubeType</code>、<code class="fe op oq or nr b">RollupType</code>和<code class="fe op oq or nr b">PivotType</code>。对于一个<code class="fe op oq or nr b">groupBy()</code>操作，我们需要将<code class="fe op oq or nr b">GroupByType</code>作为第三个参数传递。当<code class="fe op oq or nr b">groupBy()</code>返回<code class="fe op oq or nr b">RelationalGroupedDataset</code>对象时，我们可以使用它的方法来运行聚合方法。</p><p id="177b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里有一个例子。在<code class="fe op oq or nr b">staffDF</code>中，我们想知道每个角色雇佣了多少人。为此，我们使用聚合方法<code class="fe op oq or nr b">count()</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="09b2" class="ms kz it nr b gy nv nw l nx ny">staffDF.groupBy("role").count().show()</span><span id="0ea5" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+-----+ <br/>|          role|count| <br/>+--------------+-----+ <br/>| Data engineer|    2| <br/>|     Developer|    1| <br/>|Data scientist|    2| <br/>+--------------+-----+</strong></span></pre><h2 id="3934" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">计数()</h2><p id="5898" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="26b2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> count(): DataFrame</span></pre><p id="cc5a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L272" rel="noopener ugc nofollow" target="_blank">类中</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L272" rel="noopener ugc nofollow" target="_blank">RelationalGroupedDataset</a></code>统计每组的行数。产生的数据帧也将包含分组列。一个分组可以有多个列。例如，我们可以根据<code class="fe op oq or nr b">role</code>和<code class="fe op oq or nr b">salary</code>进行分组:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9fe7" class="ms kz it nr b gy nv nw l nx ny">staffDF.groupBy("role", "salary").count().show()</span><span id="c5f0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+------+-----+ <br/>|          role|salary|count| <br/>+--------------+------+-----+ <br/>|     Developer|  3600|    1| <br/>| Data engineer|  3200|    2| <br/>|Data scientist|  4100|    1| <br/>|Data scientist|  4500|    1| <br/>+--------------+------+-----+</strong></span></pre><p id="93da" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">表4给出了我们可以使用的<code class="fe op oq or nr b">RelationalGroupedDataset</code>中的一些聚合方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/8ecb171c8e0f489a972da3a8f357a328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SqzEWI4hUgKRpT12JyyD4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表4</p></figure><h2 id="54ff" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">agg()</h2><p id="6c8c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们也可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L258" rel="noopener ugc nofollow" target="_blank">方法</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7047" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> agg(expr: Column, exprs: Column*): DataFrame</span></pre><p id="af3b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它通过指定一系列聚合列来计算聚合。每个参数都是一个列表达式。当我们希望一次运行多个聚合函数时，这个函数非常有用。</p><h2 id="af70" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">聚合函数</strong></h2><p id="6a31" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">记住，对象<code class="fe op oq or nr b">org.apache.spark.sql.functions</code>提供了内置的标准函数来处理列。这些功能有不同的类别。聚合函数类别包括为分组数据帧的每组返回值的函数。所以我们可以在<code class="fe op oq or nr b">groupBy()</code>之后使用它们作为我们的聚合函数。您可以参考<a class="ae mm" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html" rel="noopener ugc nofollow" target="_blank"> Spark手册</a>获取这些功能的完整列表。它提供了一些<code class="fe op oq or nr b">RelationalGroupedDataset </code>类中没有的额外方法。</p><p id="110f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">比如在<code class="fe op oq or nr b">staffDF</code>中，我们想看看每个角色雇佣了多少人，每个角色的平均工资是多少。为此，我们使用以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="dd9a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.groupBy("role").agg(F.count("salary"),  <br/>  F.avg("salary")).show()</span><span id="8762" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+--------------+-------------+-----------+ <br/>|          role|count(salary)|avg(salary)| <br/>+--------------+-------------+-----------+ <br/>|Data scientist|            2|     4300.0| <br/>| Data engineer|            2|     3200.0| <br/>|     Developer|            1|     3600.0| <br/>+--------------+-------------+-----------+</strong></span></pre><p id="ae1e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">值得注意的是，该函数也可用于未分组的数据帧或数据集。在这种情况下，该函数应用于整个数据帧或数据集(就像只有一个组的数据帧一样)，并返回一个只有一行的<code class="fe op oq or nr b">Column</code>对象。例如，要计算<code class="fe op oq or nr b">staffDF</code>中所有薪水的总和，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3c26" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>staffDF.agg(F.sum("salary")).show()</span><span id="1a13" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----------+ <br/>|sum(salary)| <br/>+-----------+ <br/>|      18600| <br/>+-----------+</strong></span></pre><p id="fbf3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">此外，你不需要在这里使用<code class="fe op oq or nr b">agg()</code>。可以使用<code class="fe op oq or nr b">select()</code>方法选择结果列。所以前面的代码也可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8a3c" class="ms kz it nr b gy nv nw l nx ny">staffDF.select(F.sum("salary")).show()</span></pre><h2 id="9afa" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">旋转</strong></h2><p id="d29e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">支点()</strong></p><p id="ce42" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2eb3" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> pivot(pivotColumn: <strong class="nr iu">String</strong>): RelationalGroupedDataset</span></pre><p id="d374" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">是<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L352" rel="noopener ugc nofollow" target="_blank">阶层</a>的成员<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L352" rel="noopener ugc nofollow" target="_blank">RelationalGroupedDataset</a></code>。要使用它，首先需要在数据集或数据帧上调用类似于<code class="fe op oq or nr b">groupby()</code>的方法来返回一个<code class="fe op oq or nr b">RelationalGroupedDataset</code>对象，然后在该对象上调用<code class="fe op oq or nr b">pivot()</code>。它在<code class="fe op oq or nr b">RelationalGroupedDataset</code>对象中旋转分组数据帧的一列，这意味着它将<code class="fe op oq or nr b">pivotColumn</code>的唯一行值转换为结果数据帧的列标题。</p><p id="4cb0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将这个结果作为一个新的<code class="fe op oq or nr b">RelationalGroupedDataset</code>对象返回。然后，您可以将聚合方法应用于该结果。<code class="fe op oq or nr b">pivotColumn</code>以字符串形式给出要透视的列的名称。但是，您也可以传递一个<code class="fe op oq or nr b">Column</code>对象。 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L421" rel="noopener ugc nofollow" target="_blank">pivot()</a></code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L421" rel="noopener ugc nofollow" target="_blank">重载版本以<code class="fe op oq or nr b">pivotColumn</code>为<code class="fe op oq or nr b">Column</code>对象:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4530" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> pivot(pivotColumn: Column): RelationalGroupedDataset</span></pre><p id="7b97" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们看一个这个方法的例子。我们首先定义一个名为<code class="fe op oq or nr b">productDF</code>的数据帧。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1f2a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.sqlContext.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> products = <strong class="nr iu">Seq</strong>(<br/>  ("P1", 100, "Vancouver"),<br/>  ("P2", 150, "Vancouver"),<br/>  ("P3", 130, "Vancouver"),<br/>  ("P4", 190, "Vancouver"),<br/>  ("P1", 50, "Toronto"),<br/>  ("P2", 60, "Toronto"),<br/>  ("P3", 70, "Toronto"),<br/>  ("P4", 60, "Toronto"),<br/>  ("P1", 30, "Calgary"),<br/>  ("P2", 140 ,"Calgary"),<br/>  ("P3", 110, "Montreal"))</span><span id="6c5a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> productDF = products.toDF("productID", "quantity", "city")<br/>productDF.show()</span><span id="9ab4" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------+--------+---------+ <br/>|productID|quantity|     city| <br/>+---------+--------+---------+ <br/>|       P1|     100|Vancouver| <br/>|       P2|     150|Vancouver| <br/>|       P3|     130|Vancouver| <br/>|       P4|     190|Vancouver| <br/>|       P1|      50|  Toronto| <br/>|       P2|      60|  Toronto| <br/>|       P3|      70|  Toronto| <br/>|       P4|      60|  Toronto| <br/>|       P1|      30|  Calgary| <br/>|       P2|     140|  Calgary| <br/>|       P3|     110| Montreal| <br/>+---------+--------+---------+</strong></span></pre><p id="da34" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该数据框架给出了每个城市的订购产品数量。现在，我们首先使用<code class="fe op oq or nr b">groupBy()</code>按照列<code class="fe op oq or nr b">productID</code>对这个数据框架进行分组，然后旋转列<code class="fe op oq or nr b">city</code>,得到每个城市中每个订购产品的总数量。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4628" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> pivotDF =productDF.groupBy("productID").pivot("city")  <br/>  .sum("quantity")<br/>pivotDF.show()</span><span id="7ca0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------+-------+--------+-------+---------+ |productID|Calgary|Montreal|Toronto|Vancouver| <br/>+---------+-------+--------+-------+---------+ <br/>|       P2|    140|    null|     60|      150| <br/>|       P3|   null|     110|     70|      130| <br/>|       P4|   null|    null|     60|      190| <br/>|       P1|     30|    null|     50|      100| <br/>+---------+-------+--------+-------+---------+</strong></span></pre><p id="1ddb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">还有另一个 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L381" rel="noopener ugc nofollow" target="_blank">pivot()</a></code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L381" rel="noopener ugc nofollow" target="_blank">重载版本，它要求调用者在<code class="fe op oq or nr b">pivotColumn</code>中指定不同值的列表以进行转换:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c3a4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> pivot(pivotColumn: <strong class="nr iu">String</strong>, values: <strong class="nr iu">Seq</strong>[Any]): <br/>  RelationalGroupedDataset</span></pre><p id="dd95" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这个版本效率更高，因为如果不指定<code class="fe op oq or nr b">values</code>，Spark需要首先在内部计算<code class="fe op oq or nr b">pivotColumn</code>中不同值的列表。因此，前面的例子可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="979b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> cities = <strong class="nr iu">Seq</strong>("<!-- -->Calgary", "Montreal", "Toronto", "Vancouver"<!-- -->)<br/><strong class="nr iu">val</strong> pivotDF =productDF.groupBy("productID").pivot("city", <!-- -->cities<!-- -->)<br/>  .sum("quantity")</span></pre><p id="4684" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您还可以使用该类中存在的<code class="fe op oq or nr b">pivot()</code>的其他重载变量，将<code class="fe op oq or nr b">values</code>作为<code class="fe op oq or nr b">List</code>传递，将<code class="fe op oq or nr b">pivotColumn</code>作为<code class="fe op oq or nr b">Column</code>对象传递。</p><h2 id="e6a4" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">取消投票</strong></h2><p id="acb9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们还可以取消一个表的透视，这意味着我们将多个列标题变成一个列。(每个列标题将是该新列的一行。)我们还创建了第二列来存储原始列的值。要取消数据帧的透视，我们可以使用内置的SQL方法<code class="fe op oq or nr b">stack()</code>。<code class="fe op oq or nr b">stack(n, expr1, …, exprk)</code>分离<code class="fe op oq or nr b">expr1</code>，...，<code class="fe op oq or nr b">exprk</code>成<code class="fe op oq or nr b">n</code>行。下面的代码反透视<code class="fe op oq or nr b">pivotDF</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8e69" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> unPivotDF = pivotDF.select($"productID",<br/>   F.expr("stack(3, 'Vancouver', Vancouver,<br/>                    'Toronto', Toronto,<br/>                    'Calgary', Calgary) as (city, quantity)"))<br/>                    .filter($"quantity".isNotNull)<br/>unPivotDF.show()</span><span id="5e05" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---------+---------+--------+ <br/>|productID|     city|quantity| <br/>+---------+---------+--------+ <br/>|       P2|Vancouver|     150| <br/>|       P2|  Toronto|      60| <br/>|       P2|  Calgary|     140| <br/>|       P3|Vancouver|     130| <br/>|       P3|  Toronto|      70| <br/>|       P4|Vancouver|     190| <br/>|       P4|  Toronto|      60| <br/>|       P1|Vancouver|     100| <br/>|       P1|  Toronto|      50| <br/>|       P1|  Calgary|      30| <br/>+---------+---------+--------+</strong></span></pre><p id="2734" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们看看它是如何工作的。在<code class="fe op oq or nr b">pivotDF</code>中，每行都有列。SQL语句</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="54fa" class="ms kz it nr b gy nv nw l nx ny">stack(3, 'Vancouver', Vancouver, 'Toronto', Toronto,                    <br/>  'Calgary', Calgary) as (city, quantity)</span></pre><p id="d7ba" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">取<code class="fe op oq or nr b">pivotDF</code>的每一行，变成三行。因为我们在<code class="fe op oq or nr b">stack()</code>中有六个表达式，所以这三行有两列。这些列将被命名为<code class="fe op oq or nr b">city</code>和<code class="fe op oq or nr b">quantity</code>。在每一行中，第一列将城市名称作为文字，第二列将代表该城市的列的值作为文字。例如，<code class="fe op oq or nr b">pivotDF</code>的第一行是:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="cff3" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">+-------+--------+-------+---------+ |Calgary|Montreal|Toronto|Vancouver| <br/>+-------+--------+-------+---------+ <br/>|    140|    null|     60|      150| <br/>+-------+--------+-------+---------+</strong></span></pre><p id="f2bd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">stack</code>把它变成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ee93" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">+---------+--------+ <br/>|     city|quantity| <br/>+---------+--------+ <br/>|Vancouver|     150| <br/>| Montreal|    null|<br/>|  Toronto|      60| <br/>|  Calgary|     140|<br/>+---------+--------+</strong></span></pre><p id="f83a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还选择<code class="fe op oq or nr b">productID</code>将其作为一个单独的列。最后<code class="fe op oq or nr b">filter($”quantity”.isNotNull)</code>删除<code class="fe op oq or nr b">quantity</code>为<code class="fe op oq or nr b">null</code>的行。</p><h2 id="dad8" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">转置</strong></h2><p id="580e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们也可以转置一个数据帧，而不使用<code class="fe op oq or nr b">pivot()</code>方法和聚合函数。假设我们有这样一个数据帧:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="164e" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df = <strong class="nr iu">Seq</strong>(("c1", 1), ("c2", 2), ("c3", 3), ("c4", 4))<br/>  .toDF("col1", "col2")<br/>df.show()</span><span id="c786" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+----+----+ <br/>|col1|col2| <br/>+----+----+ <br/>|  c1|   1| <br/>|  c2|   2| <br/>|  c3|   3| <br/>|  c4|   4| <br/>+----+----+</strong></span></pre><p id="4917" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在假设我们想转置这个表。我们使用以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6c38" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.types._<br/><strong class="nr iu">import</strong> org.apache.spark.sql.Row<br/><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}</span><span id="02a5" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> schema = StructType(df.select(F.collect_list("col1"))<br/>  .first().getAs[<strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]](0)<br/>  .map(x =&gt; StructField(x, IntegerType)))</span><span id="c388" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> rows = Seq(Row.fromSeq(df.select(<br/>  F.collect_list("col2")).first().getAs[<strong class="nr iu">Seq</strong>[<strong class="nr iu">Integer</strong>]](0))<br/>  )<br/><strong class="nr iu">val</strong> data = spark.sparkContext.parallelize(rows)</span><span id="01e0" class="ms kz it nr b gy nz nw l nx ny">spark.createDataFrame(values, schema).show()</span><span id="ea5a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+---+---+---+ <br/>| c1| c2| c3| c4| <br/>+---+---+---+---+ <br/>|  1|  2|  3|  4| <br/>+---+---+---+---+</strong></span></pre><p id="8381" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，我们首先为转置的数据帧创建了一个模式。</p><p id="24cd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">收藏_列表()</strong></p><p id="de0f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该方法</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a082" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> collect_list(columnName: <strong class="nr iu">String</strong>): Column</span></pre><p id="ccde" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L321" rel="noopener ugc nofollow" target="_blank">functions</a></code>中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L321" rel="noopener ugc nofollow" target="_blank">将<code class="fe op oq or nr b">columnName</code>列的所有元素收集到一个<code class="fe op oq or nr b">WrappedArray</code>中，并将其放入一个只有一行的<code class="fe op oq or nr b">Column</code>对象中。所以为了得到<code class="fe op oq or nr b">col1</code>的元素，我们写:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b739" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>df.select(F.collect_list("col1")).show()</span><span id="cf92" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+------------------+ <br/>|collect_list(col1)| <br/>+------------------+ <br/>|  [c1, c2, c3, c4]| <br/>+------------------+</strong></span></pre><p id="79bf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然后，我们需要使用<code class="fe op oq or nr b">first()</code>(我们也可以使用<code class="fe op oq or nr b">head()</code>)获得结果数据帧的第一行，并将其转换为一个字符串对象序列。所以我们使用<code class="fe op oq or nr b">getAs()</code>将它转换成一个字符串序列:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0fac" class="ms kz it nr b gy nv nw l nx ny">df.select(F.collect_list("col1")).first().getAs[<strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]](0)</span><span id="68f0" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res1: Seq[String] = WrappedArray(c1, c2, c3, c4)</strong></span></pre><p id="3a9e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后，我们使用<code class="fe op oq or nr b">map(x =&gt; StructField(x, IntegerType))</code>为这个序列的每个元素创建一个<code class="fe op oq or nr b">StructField</code>对象的数组。现在，每个元素都可以是转置数据帧的一列。我们将这个数组放在<code class="fe op oq or nr b">StructType()</code>中，以创建转置数据帧的模式。我们知道转置数据帧的所有列都将获得<code class="fe op oq or nr b">Integer</code>值。所以我们在<code class="fe op oq or nr b">StructField()</code>中使用<code class="fe op oq or nr b">IntegerType</code>。<code class="fe op oq or nr b">df</code>中<code class="fe op oq or nr b">col2</code>的值将构成转置数据帧的唯一一行。我们重复相同的过程来创建它。我们使用<code class="fe op oq or nr b">getAs[Seq[Integer]]</code>将它们转换成一个整数序列，并使用<code class="fe op oq or nr b">fromRow()</code>从这个序列中创建一个行对象。最后，我们使用<code class="fe op oq or nr b">createDataFrame()</code>使用模式和行创建转置的数据帧。</p><p id="6bb4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当您转置数据帧时，您应该注意原始数据帧的每一行都将是转置数据帧的一列，因此每一行中的所有元素都应该具有相同的类型。换句话说，除了将用于创建转置表的列标题的列之外，原始表的所有列都应该具有相同的类型。否则，您必须将其中一些转换为新类型。这是另一个例子。这次原始表格(<code class="fe op oq or nr b">df1</code>)有三列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="729c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df1 = <strong class="nr iu">Seq</strong>(("c1", 1, "a"),<br/>              ("c2", 2, "b"),<br/>              ("c3", 3, "c"),<br/>              ("c4", 4, "d")).toDF("col1", "col2", "col3")<br/>df1.show()</span><span id="5dc4" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+----+----+----+ <br/>|col1|col2|col3| <br/>+----+----+----+ <br/>|  c1|   1|   a| <br/>|  c2|   2|   b| <br/>|  c3|   3|   c| <br/>|  c4|   4|   d| <br/>+----+----+----+</strong></span></pre><p id="5043" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如前所述，<code class="fe op oq or nr b">col1</code>将用于创建转置表的列标题。但是这次<code class="fe op oq or nr b">col2</code>和<code class="fe op oq or nr b">col3</code>的类型不同。所以我们需要将<code class="fe op oq or nr b">col2</code>转换成弦乐来移调<code class="fe op oq or nr b">df1</code>。为此，我们使用<code class="fe op oq or nr b">map(_.toString)</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b0b8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> schema = StructType(df1.select(F.collect_list("col1"))<br/>  .first().getAs[<strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]](0)<br/>  .map(x =&gt; StructField(x, StringType)))<br/><strong class="nr iu">val</strong> rows = <strong class="nr iu">Seq</strong>(<br/>  Row.fromSeq(df1.select(  F.collect_list("col2")).first()<br/>  .getAs[<strong class="nr iu">Seq</strong>[<strong class="nr iu">Integer</strong>]](0).map(<strong class="nr iu">_</strong>.toString)),                                           <br/>  Row.fromSeq(df1.select(F.collect_list("col3")).first()<br/>  .getAs[<strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]](0))<br/>  )</span><span id="8984" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> data = spark.sparkContext.parallelize(rows)<br/>spark.createDataFrame(data, schema).show()</span><span id="612a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+---+---+---+ <br/>| c1| c2| c3| c4| <br/>+---+---+---+---+ <br/>|  1|  2|  3|  4| <br/>|  a|  b|  c|  d| <br/>+---+---+---+---+</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="4505" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">窗口功能</strong></h1><p id="7b8a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">窗口函数对于计算移动平均值或累积和等任务非常有用。<em class="oa">窗口</em>是相对于数据帧的当前行定义的行的集合。窗口中的每一行都与当前行相距指定的行数。例如，对于数据帧的每一行，我们可以定义一个包含该行及其前一行的窗口。因此，对于每一行，对应的窗口遵循相同的模式，但其元素是不同的。一旦定义了窗口，我们就可以对其应用窗口函数，因此对于数据帧的每一行，窗口函数将使用其对应的窗口作为输入参数来返回该行的单个值。</p><p id="7f2f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L43" rel="noopener ugc nofollow" target="_blank">包org . Apache . spark . SQL . expressions</a>中的对象<code class="fe op oq or nr b">Window</code>提供了在DataFrame中定义窗口的实用方法。所有这些方法都返回一个<code class="fe op oq or nr b">WindowSpec</code>对象。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L32" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L32" rel="noopener ugc nofollow" target="_blank">WindowSpec</a></code>也在包org . Apache . spark . SQL . expressions中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ef52" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">class</strong> WindowSpec <strong class="nr iu">private</strong>[sql](partitionSpec: <strong class="nr iu">Seq</strong>[Expression],  <br/>  orderSpec: Seq[SortOrder], frame: WindowFrame) { ... </span></pre><p id="fba5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">并且存储数据帧的窗口的划分(<code class="fe op oq or nr b">partitionSpec</code>)、排序(<code class="fe op oq or nr b">orderSpec</code>)和帧边界(<code class="fe op oq or nr b">frame</code>)。当你定义一个窗口时，你需要使用对象<code class="fe op oq or nr b">Window</code>中的方法来提供这个信息。</p><h2 id="8f69" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">orderBy()</h2><p id="fcaa" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">基于数据帧中有序列的行来定义窗口。<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L77" rel="noopener ugc nofollow" target="_blank">Window</a></code>中的方法<code class="fe op oq or nr b">orderBy()</code>用于此目的。<code class="fe op oq or nr b">orderBy()</code>使用方法<code class="fe op oq or nr b">spec</code>返回一个<code class="fe op oq or nr b">WindowSpec</code>对象。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d7c8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> orderBy(cols: Column*): WindowSpec = { spec.orderBy(cols : <strong class="nr iu">_</strong>*) }</span></pre><p id="9dff" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><code class="fe op oq or nr b">spec</code>是对象 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L217" rel="noopener ugc nofollow" target="_blank">Window</a></code>内部的私有方法<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L217" rel="noopener ugc nofollow" target="_blank">，返回类<code class="fe op oq or nr b">WindowSpec</code>的对象，其中<code class="fe op oq or nr b">partitionSpec</code>和<code class="fe op oq or nr b">orderSpec</code>用空序列初始化，<code class="fe op oq or nr b">frame</code>用<code class="fe op oq or nr b">UnspecifiedFrame</code>初始化:</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0e80" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">private</strong>[sql] <strong class="nr iu">def</strong> spec: WindowSpec = {<br/>  <strong class="nr iu">new</strong> WindowSpec(Seq.empty, Seq.empty, UnspecifiedFrame)  }</span></pre><p id="0de6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当<code class="fe op oq or nr b">orderBy()</code>调用<code class="fe op oq or nr b">spec.orderBy(cols : <strong class="ls iu">_</strong>*)</code>时，它正在调用类<code class="fe op oq or nr b">WindowSpec</code>的<code class="fe op oq or nr b">orderBy()</code>方法。<code class="fe op oq or nr b">WindowSpec</code>中的<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L69" rel="noopener ugc nofollow" target="_blank">orderBy()</a></code>按升序对列的内部Catalyst表达式进行排序(如果它们还没有排序的话),并返回一个带有新<code class="fe op oq or nr b">orderSpec</code>的新<code class="fe op oq or nr b">WindowSpec</code>对象。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f88d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> orderBy(cols: Column*): WindowSpec = { <br/>  <strong class="nr iu">val</strong> sortOrder: <strong class="nr iu">Seq</strong>[SortOrder] = cols.map { col =&gt;<br/>    col.expr <strong class="nr iu">match</strong> { <br/>      <strong class="nr iu">case</strong> expr: SortOrder =&gt; expr   <br/>      <strong class="nr iu">case</strong> expr: Expression =&gt; SortOrder(expr, Ascending) <br/>    }<br/>  }<br/>  <strong class="nr iu">new</strong> WindowSpec(partitionSpec, sortOrder, frame)<br/>}</span></pre><p id="2b45" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">方法定义显示，默认情况下，<code class="fe op oq or nr b">orderBy()</code>按升序对一列中的行进行排序。您也可以使用 <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L60" rel="noopener ugc nofollow" target="_blank">orderBy()</a></code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L60" rel="noopener ugc nofollow" target="_blank">重载变体将列名作为<code class="fe op oq or nr b">String</code>传递。</a></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2ab2" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> orderBy(colName: <strong class="nr iu">String</strong>, colNames: <strong class="nr iu">String</strong>*): WindowSpec = {    <br/>  orderBy((colName +: colNames).map(Column(<strong class="nr iu">_</strong>)): <strong class="nr iu">_*</strong>)  }</span></pre><h2 id="330d" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">partitionBy()</h2><p id="2a1e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们也可以首先将一个数据帧划分成组，然后为每个分区中的所有行创建窗口。我们使用<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L50" rel="noopener ugc nofollow" target="_blank">Window</a></code>中的方法<code class="fe op oq or nr b">partitionBy()</code>来定义分区。它被定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3744" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> partitionBy(cols: Column<strong class="nr iu">*</strong>): WindowSpec = {    <br/>  spec.partitionBy(cols : <strong class="nr iu">_*</strong>)  }<br/><strong class="nr iu">def</strong> partitionBy(colName: <strong class="nr iu">String</strong>, colNames: <strong class="nr iu">String</strong>*): WindowSpec = {    <br/>  spec.orderBy(colName, colNames : <strong class="nr iu">_*</strong>)  }</span></pre><h2 id="f8e6" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">rowsBetween()</h2><p id="2ddf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">定义窗口的边框边界，我们可以用<code class="fe op oq or nr b">Window</code> : <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L161" rel="noopener ugc nofollow" target="_blank">rowsBetween()</a></code>和<code class="fe op oq or nr b">rangeBetween()</code>另外两种方法。该方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5615" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> rowsBetween(start: <strong class="nr iu">Long</strong>, end: <strong class="nr iu">Long</strong>): WindowSpec</span></pre><p id="f2ca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">返回一个定义了框架边界的<code class="fe op oq or nr b">WindowSpec</code>对象，从<code class="fe op oq or nr b">start</code>(含)到<code class="fe op oq or nr b">end</code>(含)。它们都是相对于当前行定义的。例如，<code class="fe op oq or nr b">start=0</code>表示窗口的框架边界从当前行开始，<code class="fe op oq or nr b">start=-1</code>表示从当前行的前一行开始，<code class="fe op oq or nr b">start=2</code>表示从当前行的后两行开始。建议将<code class="fe op oq or nr b">start</code>或<code class="fe op oq or nr b">end</code>设置为<code class="fe op oq or nr b">Window.currentRow</code>，而不是使用<code class="fe op oq or nr b">0</code>的值来描述当前行。方法<code class="fe op oq or nr b">currentRow()</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L114" rel="noopener ugc nofollow" target="_blank">类</a>T11中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b409" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> currentRow: <strong class="nr iu">Long</strong> = 0</span></pre><p id="4b9f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果<code class="fe op oq or nr b">start</code>设置为<code class="fe op oq or nr b">Window.unboundedPreceding</code>或<code class="fe op oq or nr b">end</code>设置为<code class="fe op oq or nr b">Window.unboundedFollowing</code>，则框架为无界。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L91" rel="noopener ugc nofollow" target="_blank">方法</a>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0f65" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> unboundedPreceding: <strong class="nr iu">Long</strong> = <strong class="nr iu">Long</strong>.MinValue</span></pre><p id="1a5c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">返回一个<code class="fe op oq or nr b">Long</code>类型的最小值，代表一个分区中的第一行，而<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala#L103" rel="noopener ugc nofollow" target="_blank">方法</a>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1daa" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> unboundedFollowing: <strong class="nr iu">Long</strong> = <strong class="nr iu">Long</strong>.MaxValue</span></pre><p id="005a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">返回<code class="fe op oq or nr b">Long</code>类型的最大值，并表示分区中的最后一行。同样，建议使用它们而不是值来描述分区的第一行或最后一行。</p><p id="21ca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们来看一些使用windows的例子。考虑之前定义的<code class="fe op oq or nr b">temperatureDF</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="770a" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> temp = <strong class="nr iu">Seq</strong>(("2020-01-01 07:30:00", 17.0), <br/>  ("2020-01-02 07:30:00", 25.5),  <br/>  ("2020-01-03 07:30:00", 19.5),  <br/>  ("2020-01-04 07:30:00", 21.2),  <br/>  ("2020-01-05 07:30:00", 18.0), <br/>  ("2020-01-06 07:30:00", 20.5)<br/>  ).toDF("time", "temperature")</span><span id="d733" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> temperatureDF = temp.withColumn("time", <br/>  F.col("time").cast("timestamp"))<br/>temperatureDF.show()</span><span id="fd82" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+<br/>|               time|temperature|<br/>+-------------------+-----------+<br/>|2020-01-01 07:30:00|       17.0|<br/>|2020-01-02 07:30:00|       25.5|<br/>|2020-01-03 07:30:00|       19.5|<br/>|2020-01-04 07:30:00|       21.2|<br/>|2020-01-05 07:30:00|       18.0|<br/>|2020-01-06 07:30:00|       20.5|<br/>+-------------------+-----------+</strong></span></pre><p id="11f0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在假设我们想要计算列<code class="fe op oq or nr b">temperature</code>的最后三行的移动平均值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="dd35" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.expressions.Window<br/><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_<br/>import</strong> org.apache.spark.sql.{functions=&gt;F}</span><span id="5a7e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> w = Window.orderBy("time").rowsBetween(-2, Window.currentRow)  <br/>temperatureDF.withColumn("rolling_average",<br/>  F.round(F.avg("temperature").over(w), 2)).show()</span><span id="e05a" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+---------------+ <br/>|               time|temperature|rolling_average| <br/>+-------------------+-----------+---------------+ <br/>|2020-01-01 07:30:00|       17.0|           17.0| <br/>|2020-01-02 07:30:00|       25.5|          21.25| <br/>|2020-01-03 07:30:00|       19.5|          20.67| <br/>|2020-01-04 07:30:00|       21.2|          22.07| <br/>|2020-01-05 07:30:00|       18.0|          19.57| <br/>|2020-01-06 07:30:00|       20.5|           19.9| <br/>+-------------------+-----------+---------------+</strong></span></pre><p id="7098" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，我们首先通过对列<code class="fe op oq or nr b">time</code>进行排序来定义窗口，并定义窗口框架边界或包括当前行和之前的两行。我们将聚合函数<code class="fe op oq or nr b">avg()</code>应用于列<code class="fe op oq or nr b">temperature</code>，然后将方法<code class="fe op oq or nr b">over()</code>应用于它，以计算该窗口中每一行<code class="fe op oq or nr b">temperatureDF</code>的平均值。</p><h2 id="07e4" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">结束()</h2><p id="dc60" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe op oq or nr b">over()</code>在<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1363" rel="noopener ugc nofollow" target="_blank">类</a>中定义<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1363" rel="noopener ugc nofollow" target="_blank">Column</a></code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d859" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> over(window: expressions.WindowSpec): Column = <br/>  window.withAggregate(<strong class="nr iu">this</strong>)</span></pre><p id="4bbe" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里的<code class="fe op oq or nr b">expressions.WindowSpec</code>是指包org . Apache . spark . SQL . expressions中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L32" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L32" rel="noopener ugc nofollow" target="_blank">WindowSpec</a></code>，在<code class="fe op oq or nr b">Column</code>对象上调用<code class="fe op oq or nr b">over()</code>时，会以一个窗口为参数，调用其方法<code class="fe op oq or nr b">withAggregate()</code>。调用<code class="fe op oq or nr b">over()</code>的列被传递给这个方法。<code class="fe op oq or nr b">withAggregate()</code>在org . Apache . spark . SQL . expressions包中的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L215" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L215" rel="noopener ugc nofollow" target="_blank">WindowSpec</a></code>中定义:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="46e9" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">private</strong>[sql] <strong class="nr iu">def</strong> withAggregate(aggregate: Column): Column = { <br/>  <strong class="nr iu">val</strong> spec = WindowSpecDefinition(partitionSpec, orderSpec, frame)  <br/>  <strong class="nr iu">new</strong> Column(WindowExpression(aggregate.expr, spec))<br/>}</span></pre><p id="2db0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">案例类<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/windowExpressions.scala#L41" rel="noopener ugc nofollow" target="_blank">WindowSpecDefinition()</a></code>和<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/windowExpressions.scala#L279" rel="noopener ugc nofollow" target="_blank">WindowExpression()</a></code>在包org . Apache . Spark . SQL . catalyst . expressions中定义。<code class="fe op oq or nr b">withAggregate()</code>将窗口规范信息从Spark SQL中的类<code class="fe op oq or nr b">WindowSpec</code>转移到Catalyst中的案例类<code class="fe op oq or nr b">WindowSpecDefinition()</code>。一个<code class="fe op oq or nr b">WindowSpecDefinition()</code>的实例加上当前列的内部Catalyst表达式(<code class="fe op oq or nr b">aggregate</code>)被用来创建一个新的<code class="fe op oq or nr b">Column</code>对象，它是调用<code class="fe op oq or nr b">over()</code>的最终结果。如前所述，<code class="fe op oq or nr b">orderBy()</code>默认情况下对一列中的行进行升序排序。要按降序对它们进行排序，请使用类<code class="fe op oq or nr b">Column</code>中的<code class="fe op oq or nr b">desc()</code>方法。</p><p id="52b8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">请记住，您只能将这个<code class="fe op oq or nr b">desc()</code>应用于一个<code class="fe op oq or nr b">Column </code>对象，因此将列名作为<code class="fe op oq or nr b">String</code>的<code class="fe op oq or nr b">orderBy()</code>的重载变量不能用于此目的。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3cd0" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> w = Window.orderBy($"time".desc).rowsBetween(-2, <br/>  Window.currentRow)<br/>temperatureDF.withColumn("rolling_average",<br/>  F.round(F.avg("temperature").over(w), 2)).show()</span><span id="b452" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+---------------+<br/>|               time|temperature|rolling_average|<br/>+-------------------+-----------+---------------+ <br/>|2020-01-06 07:30:00|       20.5|           20.5| <br/>|2020-01-05 07:30:00|       18.0|          19.25| <br/>|2020-01-04 07:30:00|       21.2|           19.9| <br/>|2020-01-03 07:30:00|       19.5|          19.57| <br/>|2020-01-02 07:30:00|       25.5|          22.07| <br/>|2020-01-01 07:30:00|       17.0|          20.67| <br/>+-------------------+-----------+---------------+</strong></span></pre><p id="2ae3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在前面的例子中，我们还使用了<code class="fe op oq or nr b">functions</code>中的方法<code class="fe op oq or nr b">round()</code>来舍入结果。我们也可以在窗口上使用其他功能方法，如<code class="fe op oq or nr b">count()</code>、<code class="fe op oq or nr b">min()</code>、<code class="fe op oq or nr b">max()</code>和<code class="fe op oq or nr b">sum()</code>。</p><p id="ed03" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果使用<code class="fe op oq or nr b">orderBy()</code>创建一个窗口，但没有给出窗口框架规格，则框架边界从<code class="fe op oq or nr b">start=Window.unboundedPreceding</code>(含)到<code class="fe op oq or nr b">end=Window.currentRow</code>(含)定义。例如，下一个示例计算<code class="fe op oq or nr b">temperatureDF</code>中<code class="fe op oq or nr b">temperature</code>列的累积和。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="011f" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> w = Window.orderBy("time")<br/>temperatureDF.withColumn("cum_sum", F.sum("temperature").over(w))<br/>  .show()</span><span id="4b89" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+-------+ <br/>|               time|temperature|cum_sum| <br/>+-------------------+-----------+-------+ <br/>|2020-01-01 07:30:00|       17.0|   17.0| <br/>|2020-01-02 07:30:00|       25.5|   42.5| <br/>|2020-01-03 07:30:00|       19.5|   62.0| <br/>|2020-01-04 07:30:00|       21.2|   83.2| <br/>|2020-01-05 07:30:00|       18.0|  101.2| <br/>|2020-01-06 07:30:00|       20.5|  121.7| <br/>+-------------------+-----------+-------+</strong></span></pre><h2 id="b5a6" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">滞后()</h2><p id="e856" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们还可以使用<code class="fe op oq or nr b">functions</code>中的窗口函数给列添加滞后。<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L914" rel="noopener ugc nofollow" target="_blank">功能</a>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7abd" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> lag(columnName: <strong class="nr iu">String</strong>, offset: <strong class="nr iu">Int</strong>): Column</span></pre><p id="bafa" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">将<code class="fe op oq or nr b">columnName</code>移位<code class="fe op oq or nr b">offset</code>，并将结果作为新的<code class="fe op oq or nr b">Column</code>对象返回。对于<code class="fe op oq or nr b">columnName</code>的每一行，它返回当前行之前<code class="fe op oq or nr b">offset</code>行的值，如果当前行之前少于<code class="fe op oq or nr b">offset</code>行，则返回<code class="fe op oq or nr b">null</code>行的值。将其应用于<code class="fe op oq or nr b">columnName</code>的所有行的结果是一个新的<code class="fe op oq or nr b">Column</code>对象，该对象由<code class="fe op oq or nr b">lag()</code>返回。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d700" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> w = Window.orderBy("time")  <br/>temperatureDF.withColumn("lag_col", <br/>  F.lag("temperature", 1).over(w)).show()</span><span id="6052" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+-------+ <br/>|               time|temperature|lag_col| <br/>+-------------------+-----------+-------+ <br/>|2020-01-01 07:30:00|       17.0|   null| <br/>|2020-01-02 07:30:00|       25.5|   17.0| <br/>|2020-01-03 07:30:00|       19.5|   25.5| <br/>|2020-01-04 07:30:00|       21.2|   19.5| <br/>|2020-01-05 07:30:00|       18.0|   21.2| <br/>|2020-01-06 07:30:00|       20.5|   18.0| <br/>+-------------------+-----------+-------+</strong></span></pre><p id="cb1a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它还接受一个<code class="fe op oq or nr b">Column</code>对象:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="89e3" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> lag(e: Column, offset: Int): Column</span></pre><p id="e257" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果希望用默认值填充结果空值，可以使用:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bfda" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> lag(columnName: String, offset: Int, defaultValue: Any): Column</span></pre><p id="e9a8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这个重载变量返回的值是当前行之前的<code class="fe op oq or nr b">offset</code>行，如果当前行之前的行数少于<code class="fe op oq or nr b">offset</code>行，则返回<code class="fe op oq or nr b">defaultValue</code>行。如果<code class="fe op oq or nr b">offset</code>为负值，则<code class="fe op oq or nr b">lag()</code>返回当前行之后<code class="fe op oq or nr b">offset</code>行的值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b1db" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/>temperatureDF.withColumn("lead_col", <br/>  F.lag("temperature", -1, 0).over(w)).show()</span><span id="66d5" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-------------------+-----------+--------+ <br/>|               time|temperature|lead_col| <br/>+-------------------+-----------+--------+ <br/>|2020-01-01 07:30:00|       17.0|    25.5| <br/>|2020-01-02 07:30:00|       25.5|    19.5| <br/>|2020-01-03 07:30:00|       19.5|    21.2| <br/>|2020-01-04 07:30:00|       21.2|    18.0| <br/>|2020-01-05 07:30:00|       18.0|    20.5| <br/>|2020-01-06 07:30:00|       20.5|     0.0| <br/>+-------------------+-----------+--------+</strong></span></pre><p id="6783" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">下一个例子显示了<code class="fe op oq or nr b">partitionBy()</code>的用法。在<code class="fe op oq or nr b">staffDF</code>中，我们想给每个有特定角色的员工一个<code class="fe op oq or nr b">id</code>，从最低工资开始:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="455b" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> windowSpec  = Window.partitionBy("role").orderBy("salary")<br/>staffDF.withColumn("row_number", F.row_number.over(windowSpec))<br/>  .show()</span><span id="7267" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+----------+ <br/>| name|          role|salary|row_number| <br/>+-----+--------------+------+----------+ <br/>|James| Data engineer|  3200|         1| <br/>|  Ali| Data engineer|  3200|         2| <br/>|Steve|     Developer|  3600|         1| <br/>|Laura|Data scientist|  4100|         1| <br/>| John|Data scientist|  4500|         2| <br/>+-----+--------------+------+----------+</strong></span></pre><p id="2d45" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们先用<code class="fe op oq or nr b">role</code>划分窗口，然后用<code class="fe op oq or nr b">salary</code>排序。<code class="fe op oq or nr b">functions</code>中的方法<code class="fe op oq or nr b">row_number()</code>返回一个窗口分区内从1开始的序列号。</p><h2 id="464e" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">范围介于()</h2><p id="fc9a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">类<code class="fe op oq or nr b">Window</code>有另一个指定框架边界的方法，叫做<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala#L193" rel="noopener ugc nofollow" target="_blank">rangeBetween()</a></code>。它被声明为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2db3" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> rangeBetween(start: <strong class="nr iu">Long</strong>, end: <strong class="nr iu">Long</strong>): WindowSpec</span></pre><p id="ab76" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">与<code class="fe op oq or nr b">rowsBetween()</code>类似，该方法创建一个定义了帧边界的<code class="fe op oq or nr b">WindowSpec</code>，从<code class="fe op oq or nr b">start</code>(含)到<code class="fe op oq or nr b">end</code>(含)。但是，也有一些不同之处。<code class="fe op oq or nr b">rangeBetween()</code>的工作基于<code class="fe op oq or nr b">orderBy()</code>返回的<code class="fe op oq or nr b">Column</code>对象的实际值，而不是行在分区中的位置。我用一个例子来解释一下。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8a97" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> df = Seq((1, 1),(2, 2), (2, 3), (2, 4), (4, 5), (6, 6),<br/> (7, 7)).toDF("id", "num")<br/><strong class="nr iu">val</strong> w = Window.orderBy("id").rowsBetween(Window.currentRow, 1)  <br/>df.withColumn("rolling_sum", F.sum("num").over(w)).show()</span><span id="6600" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+---+-----------+ <br/>| id|num|rolling_sum| <br/>+---+---+-----------+ <br/>|  1|  1|          3| <br/>|  2|  2|          5| <br/>|  2|  3|          7| <br/>|  2|  4|          9| <br/>|  4|  5|         11| <br/>|  6|  6|         13| <br/>|  7|  7|          7| <br/>+---+---+-----------+</strong></span></pre><p id="16da" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们创建一个数据帧，并定义一个包含当前行和下一行的窗口。然后，我们使用该窗口计算列<code class="fe op oq or nr b">num</code>的滚动和。现在让我们用<code class="fe op oq or nr b">rangeBetween()</code>代替<code class="fe op oq or nr b">rowsBetween()</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="155c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> w1 = Window.orderBy("id").rangeBetween(Window.currentRow, 1)  <br/>df.withColumn("rolling_sum", F.sum("num").over(w1)).show()</span><span id="19a1" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+---+-----------+ <br/>| id|num|rolling_sum| <br/>+---+---+-----------+ <br/>|  1|  1|         10| <br/>|  2|  2|          9| <br/>|  2|  3|          9| <br/>|  2|  4|          9| <br/>|  4|  5|          5| <br/>|  6|  6|         13| <br/>|  7|  7|          7| <br/>+---+---+-----------+</strong></span></pre><p id="41d7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如你所见，这两种方法给出了不同的结果。在<code class="fe op oq or nr b">rowsBetween()</code>中，当前行的位置用于计算其窗口框架边界。例如，使用<code class="fe op oq or nr b">rowsBetween()</code>时，<code class="fe op oq or nr b">staffDF</code> ( <code class="fe op oq or nr b">id</code> =1，<code class="fe op oq or nr b">num</code> =1)中第一行的窗口包括这一行和下一行(<code class="fe op oq or nr b">id</code> =2，<code class="fe op oq or nr b">num</code> =2)。所以这一行的<code class="fe op oq or nr b">rolling_sum</code>是1+2=3。</p><p id="be01" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">另一方面，<code class="fe op oq or nr b">rangeBetween()</code>不使用当前行的位置来计算其框架边界。相反，它使用<code class="fe op oq or nr b">orderBy("id")</code>的实际值来计算当前行的帧边界。在这里，列<code class="fe op oq or nr b">id</code>首先被排序，因此它具有与<code class="fe op oq or nr b">orderBy("id")</code>相同的值。因此，<code class="fe op oq or nr b">orderBy("id")</code>(或<code class="fe op oq or nr b">id</code>)的值用于指定当前行的帧边界。</p><p id="da9e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">假设第一行(<code class="fe op oq or nr b">id</code> =1，<code class="fe op oq or nr b">num</code> =1)是当前行。将<code class="fe op oq or nr b">rowsBetween()</code>中的<code class="fe op oq or nr b">start</code>和<code class="fe op oq or nr b">end</code>参数的值加到<code class="fe op oq or nr b">id</code>中，计算帧边界。所以窗口中的下一行应该有一个1+1=2的<code class="fe op oq or nr b">id</code>。但是我们有三行，<code class="fe op oq or nr b">id</code> =2。所有这些行都包含在窗口中，并且该窗口包含<code class="fe op oq or nr b">num</code> =1、2、3和4的行。所以第一行的<code class="fe op oq or nr b">rolling_sum</code>是1+2+3+4=10。</p><p id="5c9d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于第二行，我们有<code class="fe op oq or nr b">id</code> =2和<code class="fe op oq or nr b">num</code> =2，所以窗口中的下一行应该有<code class="fe op oq or nr b">id</code> =2+1=3。但是，带有此<code class="fe op oq or nr b">id</code>的行不存在，所以窗口只包含当前行，当前行包括所有带有<code class="fe op oq or nr b">id</code> =2的行。因此<code class="fe op oq or nr b">rolling_sum</code>是2+3+4=9。类似地，<code class="fe op oq or nr b">num</code> =3和<code class="fe op oq or nr b">num</code> =4的行获得相同的值<code class="fe op oq or nr b">rolling_sum</code>。现在让我们尝试以下方法:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ba97" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<strong class="nr iu"><br/>val</strong> w2 = Window.orderBy($"id" * 2)<br/>  .rangeBetween(Window.currentRow, 1)  <br/>df.withColumn("rolling_sum", F.sum("num").over(w1))<br/>  .withColumn("id*2", $"id"*2).show()</span><span id="28cb" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+---+---+-----------+----+ <br/>| id|num|rolling_sum|id*2| <br/>+---+---+-----------+----+ <br/>|  1|  1|          1|   2| <br/>|  2|  2|          9|   4| <br/>|  2|  3|          9|   4| <br/>|  2|  4|          9|   4| <br/>|  4|  5|          5|   8| <br/>|  6|  6|          6|  12| <br/>|  7|  7|          7|  14| <br/>+---+---+-----------+----+</strong></span></pre><p id="017c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里我们使用<code class="fe op oq or nr b">orderBy($”id” * 2)</code>来创建窗口。<code class="fe op oq or nr b">orderBy($”id” * 2)</code>的值不再连续，因此所有行的窗口只包含当前行。</p><p id="2431" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">方法<code class="fe op oq or nr b">rangeBetween()</code>给<code class="fe op oq or nr b">orderBy()</code>方法增加了一些限制。使用<code class="fe op oq or nr b">rangeBetween()</code>时，<code class="fe op oq or nr b">orderBy()</code>只能带一个参数(一个列表达式)。由于该列表达式的值被添加到<code class="fe op oq or nr b">rangeBetween()</code>中的<code class="fe op oq or nr b">start</code>和<code class="fe op oq or nr b">end</code>参数中以计算框架边界，因此它应该具有数字数据类型。但是，如果<code class="fe op oq or nr b">rangeBetween()</code>中的<code class="fe op oq or nr b">start</code>和<code class="fe op oq or nr b">end</code>的值仅限于<code class="fe op oq or nr b">Window.unboundedPreceding</code>、<code class="fe op oq or nr b">Window.currentRow</code>和<code class="fe op oq or nr b">Window.unboundedFollowing</code>，则可以在<code class="fe op oq or nr b">orderBy()</code>中使用非数字列表达式。在这种情况下，Spark不需要<code class="fe op oq or nr b">orderBy()</code>中列表达式的值来计算帧边界。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="9550" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">归附</strong></h1><h2 id="51cb" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">加入()</h2><p id="8a30" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1067" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1067" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">join()</code>可用于连接数据集和数据帧。它被定义为:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0039" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> join(right: Dataset[<strong class="nr iu">_</strong>], joinExprs: Column, joinType: <strong class="nr iu">String</strong>): <br/>DataFrame =<br/>  <strong class="nr iu">val</strong> plan = withPlan(Join(logicalPlan, right.logicalPlan, <br/>    JoinType(joinType), <strong class="nr iu">Some</strong>(joinExprs.expr), JoinHint.NONE))       <br/>    .queryExecution.analyzed.asInstanceOf[Join]  <br/>  <br/>  ...</span></pre><p id="e6c7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它使用给定的连接表达式(<code class="fe op oq or nr b">joinExprs</code>)将调用它的数据集与另一个数据集(<code class="fe op oq or nr b">right</code>)连接，并将结果作为新的DataFrame返回。它使用了文件<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala#L321" rel="noopener ugc nofollow" target="_blank">basicLogicalOperators.scala</a></code>中的案例类<code class="fe op oq or nr b">Join()</code>。<code class="fe op oq or nr b">joinExprs</code>是一个布尔列表达式，文件<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/joinTypes.scala#24" rel="noopener ugc nofollow" target="_blank">joinTypes.scala</a></code>中的对象<code class="fe op oq or nr b">joinType</code>决定了连接的类型:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="d796" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">object</strong> JoinType {<br/>  <strong class="nr iu">def</strong> apply(typ: <strong class="nr iu">String</strong>): JoinType = <br/>    typ.toLowerCase(Locale.ROOT).replace("<strong class="nr iu">_</strong>", "") <strong class="nr iu">match</strong> {<br/>      <strong class="nr iu">case</strong> "inner" =&gt; Inner<br/>      <strong class="nr iu">case</strong> "outer" | "full" | "fullouter" =&gt; FullOuter<br/>      <strong class="nr iu">case</strong> "leftouter" | "left" =&gt; LeftOuter<br/>      <strong class="nr iu">case</strong> "rightouter" | "right" =&gt; RightOuter <br/>      <strong class="nr iu">case</strong> "leftsemi" | "semi" =&gt; LeftSemi  <br/>      <strong class="nr iu">case</strong> "leftanti" | "anti" =&gt; LeftAnti  <br/>      <strong class="nr iu">case</strong> "cross" =&gt; Cross   </span></pre><p id="ab5f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如该对象所示，我们有七种不同类型的连接:内连接、全外连接、左外连接、右外连接、左半连接、左反连接和交叉连接。现在我们将逐一解释。正如您在<code class="fe op oq or nr b">apply()</code>方法的定义中看到的，连接类型的名称(<code class="fe op oq or nr b">typ</code>)是不区分大小写的。此外，您可以在任何地方使用下划线(<code class="fe op oq or nr b">_</code>)，因为它将被<code class="fe op oq or nr b">""</code>替换。因此<code class="fe op oq or nr b">join()</code>认为<code class="fe op oq or nr b">full_outer</code>和<code class="fe op oq or nr b">FULLOUT_er</code>是等价的！</p><h2 id="a5b6" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">内部连接</strong></h2><p id="7043" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">内部连接给出了两个数据集或数据帧的交集。对于第一个数据帧的每一行(在其上调用<code class="fe op oq or nr b">join()</code>)，如果在第二个数据帧(<code class="fe op oq or nr b">right</code>)中发现一行<code class="fe op oq or nr b">joinExprs</code>为真，则它的列被添加到第一个数据帧中该行的列，以创建一个新行。这里有一个例子。我们使用之前提到的<code class="fe op oq or nr b">staffDF</code>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5b07" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> spark.implicits.<strong class="nr iu">_</strong><br/><strong class="nr iu">val</strong> staffDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 4500),<br/>  ("James", "Data engineer", 3200),<br/>  ("Laura", "Data scientist", 4100),<br/>  ("Ali", "Data engineer", 3200),<br/>  ("Steve", "Developer", 3600)<br/>  ).toDF("name", "role", "salary")</span><span id="c98d" class="ms kz it nr b gy nz nw l nx ny">staffDF.show()</span></pre><p id="1490" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">创建一个名为<code class="fe op oq or nr b">ageDF</code>的新数据框架，给出员工的年龄:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4bce" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> ageDF = <strong class="nr iu">Seq</strong>(<br/>  ("John", 45),<br/>  ("James", 25),<br/>  ("Laura", 30),<br/>  ("Will", 28)<br/>  ).toDF("name", "age")<br/>ageDF.show()</span></pre><p id="1883" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下代码给出了这些数据帧的内部连接:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9449" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF,<br/>  staffDF("name") === ageDF("name"), "inner")<br/>joinedDF.show()</span><span id="b69e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+-----+---+ <br/>| name|          role|salary| name|age| <br/>+-----+--------------+------+-----+---+ <br/>| John|Data scientist|  4500| John| 45| <br/>|James| Data engineer|  3200|James| 25| <br/>|Laura|Data scientist|  4100|Laura| 30| <br/>+-----+--------------+------+-----+---+</strong></span></pre><p id="69b1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于内部连接，<code class="fe op oq or nr b">joinType</code>应该等于<code class="fe op oq or nr b">"inner"</code>。这里选择了两个数据帧中具有相同<code class="fe op oq or nr b">name</code>的行。请注意，我们在结果数据帧中有一个重复的<code class="fe op oq or nr b">name</code>列。为了避免内部连接中的列重复，可以使用<code class="fe op oq or nr b">join()</code>的重载变体:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="03ad" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> join(right: Dataset[<strong class="nr iu">_</strong>], usingColumns: <strong class="nr iu">Seq</strong>[<strong class="nr iu">String],<br/> </strong> joinType: <strong class="nr iu">String</strong>): DataFrame</span></pre><p id="3bc3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因此，您只需给出应该在两个数据集中匹配的列的名称。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a312" class="ms kz it nr b gy nv nw l nx ny">staffDF.join(ageDF, <strong class="nr iu">Seq</strong>("name"), "inner").show()</span><span id="7c91" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+---+ <br/>| name|          role|salary|age| <br/>+-----+--------------+------+---+ <br/>| John|Data scientist|  4500| 45| <br/>|James| Data engineer|  3200| 25| <br/>|Laura|Data scientist|  4100| 30| <br/>+-----+--------------+------+---+</strong></span></pre><p id="a5a3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果没有通过<code class="fe op oq or nr b">joinType</code>，<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1041" rel="noopener ugc nofollow" target="_blank">的话，默认连接是一个内部连接</a>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b7fd" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> join(right: Dataset[<strong class="nr iu">_</strong>], joinExprs: Column): DataFrame = <br/>  join(right, joinExprs, "inner")</span><span id="cf30" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">def</strong> join(right: Dataset[<strong class="nr iu">_</strong>], usingColumns: <strong class="nr iu">Seq</strong>[<strong class="nr iu">String</strong>]): DataFrame = {<br/>  join(right, usingColumns, "inner") <br/>}</span></pre><p id="715f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在前面的例子中，我们也可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="0970" class="ms kz it nr b gy nv nw l nx ny">staffDF.join(ageDF, <strong class="nr iu">Seq</strong>("name"), "inner").show() </span></pre><p id="6b93" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果我们想只对一列进行内部连接并避免重复，那么我们可以使用<code class="fe op oq or nr b">join()</code>的<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L961" rel="noopener ugc nofollow" target="_blank">重载版本</a>:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9c10" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> join(right: Dataset[<strong class="nr iu">_</strong>], usingColumn: <strong class="nr iu">String</strong>): DataFrame = {    <br/>  join(right, Seq(usingColumn))  }</span></pre><p id="36bb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">前面的例子也可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2478" class="ms kz it nr b gy nv nw l nx ny">staffDF.join(ageDF, "name").show()</span></pre><p id="80f0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因为<code class="fe op oq or nr b">joinExprs</code>是一个列表达式，所以我们应该使用表2中的方法。我们也可以使用<code class="fe op oq or nr b">$</code>操作符来给出列名；但是，当一个列名同时存在于两个数据帧中时，我们需要将数据帧的名称添加到列名中，以避免歧义。例如，前面的连接也可以写成:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4bf1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.as("df1").join(ageDF.as("df2"),<br/>  $"df1.name" === $"df2.name", "inner")</span></pre><p id="209c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里，我们首先为DataFrame创建一个别名，然后使用点将它的名称添加到列名中。请注意，我们不能使用DataFrame的原始名称，因此下面的代码不会编译:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="60d1" class="ms kz it nr b gy nv nw l nx ny">// This won't compile<strong class="nr iu"><br/>val</strong> joinedDF = staffDF.join(ageDF,  <br/>  $"staffDF.name" === $"ageDF.name", "inner")</span></pre><p id="cd83" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当我们想要将一个数据帧与其自身连接(自连接)时，这是很有用的。例如，要将<code class="fe op oq or nr b">staffDF</code>与其自身连接，我们可以写:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8631" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.as("df1").join(staffDF.as("df2"), <br/>  $"df1.salary" &lt; $"df2.salary", "inner")<br/>joinedDF.show()</span></pre><p id="a15c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">当数据帧中的列名不相同时，我们不需要提及数据帧的名称。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3b6d" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> ageDF1 = <strong class="nr iu">Seq</strong>(<br/>  ("John", "Data scientist", 45),<br/>  ("James", "Data engineer", 25),<br/>  ("Laura", "Data scientist", 30),<br/>  ("Will", "Data engineer", 28)<br/>  ).toDF("employee_name", "employee_role", "age")<br/>ageDF1.show()</span><span id="153b" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF1, <br/>  $"name" === $"employee_name" &amp;&amp; $"role" === $"employee_role",  <br/>  "inner")<br/>joinedDF.show()</span><span id="8680" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+-------------+--------------+---+<br/>| name|          role|salary|employee_name| employee_role|age|<br/>+-----+--------------+------+-------------+--------------+---+<br/>| John|Data scientist|  4500|         John|Data scientist| 45| |James| Data engineer|  3200|        James| Data engineer| 25| |Laura|Data scientist|  4100|        Laura|Data scientist| 30| <br/>+-----+--------------+------+-------------+--------------+---+</strong></span></pre><p id="89fb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于内部连接，我们也可以使用<code class="fe op oq or nr b">where()</code>或<code class="fe op oq or nr b">filter()</code>方法单独指定连接表达式。我们可以使用<code class="fe op oq or nr b">join()</code>的这个<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L936" rel="noopener ugc nofollow" target="_blank">重载版本</a>，其中我们只传递第二个数据帧:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6f48" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> join(right: Dataset[<strong class="nr iu">_</strong>]): DataFrame = withPlan {    <br/>  Join(logicalPlan, right.logicalPlan, joinType = Inner, <strong class="nr iu">None</strong>, <br/>  JoinHint.NONE)  }</span></pre><p id="c764" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下代码给出了一个示例:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bd22" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF)<br/>  .where(staffDF("name") === ageDF("name"))<br/>joinedDF.show()</span><span id="b1bc" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+-----+---+ <br/>| name|          role|salary| name|age| <br/>+-----+--------------+------+-----+---+ <br/>| John|Data scientist|  4500| John| 45| <br/>|James| Data engineer|  3200|James| 25| <br/>|Laura|Data scientist|  4100|Laura| 30| <br/>+-----+--------------+------+-----+---+</strong></span></pre><h2 id="020f" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">左外和右外连接</strong></h2><p id="7906" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要进行左连接，我们可以使用<code class="fe op oq or nr b">join()</code>和<code class="fe op oq or nr b">joinType</code>等于<code class="fe op oq or nr b">"left"</code>或<code class="fe op oq or nr b">"leftouter"</code>或<code class="fe op oq or nr b">"left_outer"</code>。在左连接中，结果中返回第一个数据帧的所有行(调用了<code class="fe op oq or nr b">join()</code>)。</p><p id="ae62" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于第一个数据帧的每一行(在其上调用了<code class="fe op oq or nr b">join()</code>)，如果在第二个数据帧(<code class="fe op oq or nr b">right</code>)中找到一行<code class="fe op oq or nr b">joinExprs</code>为真，则它的列被添加到第一个数据帧中该行的列中，以创建一个新行。如果第二个数据帧的所有行都不满足<code class="fe op oq or nr b">joinExprs</code>，那么第二个数据帧的列将被添加到第一个数据帧中具有空值的行的列中。因此，第一个DataFrame的所有原始行都返回第二个data frame的匹配列，或者只返回null值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a423" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF,<br/>  staffDF("name") === ageDF("name"), "left")<br/>joinedDF.show()</span><span id="c727" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+-----+----+ <br/>| name|          role|salary| name| age| <br/>+-----+--------------+------+-----+----+ <br/>| John|Data scientist|  4500| John|  45| <br/>|James| Data engineer|  3200|James|  25| <br/>|Laura|Data scientist|  4100|Laura|  30| <br/>|  Ali| Data engineer|  3200| null|null| <br/>|Steve|     Developer|  3600| null|null|<br/>+-----+--------------+------+-----+----+</strong></span></pre><p id="d21c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">同样，我们可以通过在<code class="fe op oq or nr b">join()</code>中使用一系列列名来避免重复的列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="beb8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF, <strong class="nr iu">Seq</strong>("name"), "left")</span></pre><p id="2c88" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">但是，除了内部联接之外，我们不能对任何类型的联接只使用一个列名。所以这不会编译:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2428" class="ms kz it nr b gy nv nw l nx ny">//This won't compile<strong class="nr iu"><br/>val</strong> joinedDF = staffDF.join(ageDF, "name", "left")</span></pre><p id="fd5e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在右连接中，返回第二个数据帧的所有原始行，或者是第一个数据帧的匹配列，或者只是它们的空值。为了进行右连接，我们可以使用<code class="fe op oq or nr b">join()</code>和<code class="fe op oq or nr b">joinType</code>等于<code class="fe op oq or nr b">"right"</code>或<code class="fe op oq or nr b">"rightouter"</code>或<code class="fe op oq or nr b">"right_outer"</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1643" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF, <strong class="nr iu">Seq</strong>("name"), "right")<br/>joinedDF.show()</span><span id="0e51" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+---+ <br/>| name|          role|salary|age| <br/>+-----+--------------+------+---+ <br/>|James| Data engineer|  3200| 25| <br/>| John|Data scientist|  4500| 45| <br/>| Will|          null|  null| 28| <br/>|Laura|Data scientist|  4100| 30| <br/>+-----+--------------+------+---+</strong></span></pre><h2 id="cd57" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">全外连接</strong></h2><p id="7e3b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">要进行完全连接，我们可以使用<code class="fe op oq or nr b">join()</code>，其中<code class="fe op oq or nr b">joinType</code>等于<code class="fe op oq or nr b">"full"</code>、<code class="fe op oq or nr b">"outer"</code>、<code class="fe op oq or nr b">"fullouter"</code>或<code class="fe op oq or nr b">"full_outer"</code>。完全连接合并了左连接和右连接的结果。返回左联接产生的所有行，此外，添加右联接产生的所有行，左联接中已存在的行除外。因此不会返回重复的行。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f53e" class="ms kz it nr b gy nv nw l nx ny">staffDF.join(ageDF, <strong class="nr iu">Seq</strong>("name"), "full").show()</span><span id="e7d7" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:</strong><br/><strong class="nr iu">+-----+--------------+------+----+ <br/>| name|          role|salary| age| <br/>+-----+--------------+------+----+ <br/>|Steve|     Developer|  3600|null| <br/>|James| Data engineer|  3200|  25| <br/>| John|Data scientist|  4500|  45| <br/>| Will|          null|  null|  28| <br/>|Laura|Data scientist|  4100|  30| <br/>|  Ali| Data engineer|  3200|null| <br/>+-----+--------------+------+----+</strong></span></pre><p id="8563" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果将这个输出与左外连接和右外连接的输出进行比较，您会看到它包含了这两个连接的所有行。</p><h2 id="4ef5" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">左半连接</strong></h2><p id="ad06" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">它类似于左连接，但有两个不同之处。首先，对于第一个数据帧的每一行，如果在第二个数据帧中找到一行<code class="fe op oq or nr b">joinExprs</code>为真，则只返回第一行的列，而不将第二个数据帧的行的列添加到该数据帧中。此外，对于第一个数据帧的每一行，如果第二个数据帧的所有行都不满足<code class="fe op oq or nr b">joinExprs</code>，则不返回该行。</p><p id="d8ae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它就像一个左连接，其中第二个数据帧的列被删除，这些列的行<code class="fe op oq or nr b">null</code>也被删除。为了有一个半左连接，我们可以使用join with <code class="fe op oq or nr b">joinType</code>等于<code class="fe op oq or nr b">"leftsemi"</code>，或者<code class="fe op oq or nr b">"left_semi"</code>，或者<code class="fe op oq or nr b">"semi"</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="91f4" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF,<br/>  staffDF("name") === ageDF("name"), "leftsemi")<br/>joinedDF.show()</span><span id="eb5e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+ <br/>| name|          role|salary| <br/>+-----+--------------+------+ <br/>| John|Data scientist|  4500| <br/>|James| Data engineer|  3200| <br/>|Laura|Data scientist|  4100| <br/>+-----+--------------+------+</strong></span></pre><h2 id="c9e3" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">左反连接</strong></h2><p id="8941" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">顾名思义，它与左半连接完全相反。对于第一个数据帧的每一行，如果第二个数据帧中没有<code class="fe op oq or nr b">joinExprs</code>为真的行，则返回该行。要进行半左连接，我们可以使用<code class="fe op oq or nr b">join()</code>和<code class="fe op oq or nr b">joinType</code>等于<code class="fe op oq or nr b">"leftanti"</code>或<code class="fe op oq or nr b">"left_anti"</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="37bc" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF,<br/>  staffDF("name") === ageDF("name"), "leftanti")<br/>joinedDF.show()</span><span id="96b6" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+-------------+------+ <br/>| name|         role|salary| <br/>+-----+-------------+------+ <br/>|  Ali|Data engineer|  3200| <br/>|Steve|    Developer|  3600| <br/>+-----+-------------+------+</strong></span></pre><h2 id="9b77" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">交叉连接</strong></h2><p id="0100" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了进行交叉连接，我们可以使用<code class="fe op oq or nr b">join()</code>和<code class="fe op oq or nr b">joinType</code>相等的<code class="fe op oq or nr b">"cross"</code>。交叉连接创建第一个数据帧和第二个数据帧的<em class="oa">笛卡尔乘积</em>。两个数据帧的笛卡尔积是一组有序对，其中第一个元素是第一个数据帧的一行，第二个元素是第二个数据帧的一行。因此它给出了第一个数据帧的行与第二个数据帧的行的每种可能的组合。对于每一对，如果<code class="fe op oq or nr b">joinExprs</code>为真，则该对的元素被组合以创建一个新行。第二数据帧的行的列被添加到第一数据帧的行的列。事实上，交叉连接和内部连接给出了相同的结果。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ffc5" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.join(ageDF,<br/>  staffDF("name") === ageDF("name"), "cross")<br/>joinedDF.show()</span><span id="77ac" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+-----+---+ <br/>| name|          role|salary| name|age| <br/>+-----+--------------+------+-----+---+ <br/>| John|Data scientist|  4500| John| 45| <br/>|James| Data engineer|  3200|James| 25| <br/>|Laura|Data scientist|  4100|Laura| 30| <br/>+-----+--------------+------+-----+---+</strong></span></pre><p id="2ce5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">交叉连接()</strong></p><p id="d8fa" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用<a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1130" rel="noopener ugc nofollow" target="_blank">类</a> <code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1130" rel="noopener ugc nofollow" target="_blank">Dataset</a></code>中的方法<code class="fe op oq or nr b">crossJoin()</code>来创建两个数据帧的笛卡尔积，而无需任何附加条件:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1d95" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> crossJoin(right: Dataset[<strong class="nr iu">_</strong>]): DataFrame = withPlan {    <br/>  Join(logicalPlan, right.logicalPlan, joinType = Cross,<br/>    None, JoinHint.NONE)<br/>}</span></pre><p id="a8c3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如您所见，它简单地使用了相同的案例类<code class="fe op oq or nr b">Join()</code>和<code class="fe op oq or nr b">joinType = Cross</code>，并且没有<code class="fe op oq or nr b">joinExprs</code>的值。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="09b5" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> joinedDF = staffDF.crossJoin(ageDF)<br/>joinedDF.show()</span><span id="8e04" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+-----+--------------+------+-----+---+ <br/>| name|          role|salary| name|age| <br/>+-----+--------------+------+-----+---+ <br/>| John|Data scientist|  4500| John| 45| <br/>| John|Data scientist|  4500|James| 25| <br/>| John|Data scientist|  4500|Laura| 30| <br/>| John|Data scientist|  4500| Will| 28| <br/>|James| Data engineer|  3200| John| 45| <br/>|James| Data engineer|  3200|James| 25| <br/>|James| Data engineer|  3200|Laura| 30| <br/>|James| Data engineer|  3200| Will| 28| <br/>|Laura|Data scientist|  4100| John| 45| <br/>|Laura|Data scientist|  4100|James| 25| <br/>|Laura|Data scientist|  4100|Laura| 30| <br/>|Laura|Data scientist|  4100| Will| 28| <br/>|  Ali| Data engineer|  3200| John| 45| <br/>|  Ali| Data engineer|  3200|James| 25| <br/>|  Ali| Data engineer|  3200|Laura| 30| <br/>|  Ali| Data engineer|  3200| Will| 28| <br/>|Steve|     Developer|  3600| John| 45| <br/>|Steve|     Developer|  3600|James| 25| <br/>|Steve|     Developer|  3600|Laura| 30| <br/>|Steve|     Developer|  3600| Will| 28| <br/>+-----+--------------+------+-----+---+</strong></span></pre><p id="e885" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于大型数据帧，这是一个非常昂贵的操作。具有<em class="oa"> m </em>行的数据帧与具有<em class="oa"> n </em>行的数据帧的交叉连接产生了具有<em class="oa"> m × n </em>行的数据帧。对于大型数据帧，很容易导致内存不足的异常。</p><h2 id="ca06" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><strong class="ak">保留类型的连接</strong></h2><p id="0806" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu"> joinWith() </strong></p><p id="5c7f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您可能已经注意到join方法返回一个DataFrame。我们也可以连接两个数据集并返回一个数据集。方法<code class="fe op oq or nr b">joinWith()</code>可用于此目的:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="11d1" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> joinWith[U](other: Dataset[U], condition: Column, joinType: <br/>  <strong class="nr iu">String</strong>): Dataset[(T, U)]</span></pre><p id="9de7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">与<code class="fe op oq or nr b">join()</code>类似，它采用连接表达式(<code class="fe op oq or nr b">condition</code>)和<code class="fe op oq or nr b">joinType</code>；但是，它返回元组的数据集。这个数据集的每一行都有两列(<code class="fe op oq or nr b">Dataset[(T, U)]</code>)。第一列是第一个数据集的一行，第二个元素是第二个数据集的相应行。这两行基于在<code class="fe op oq or nr b">join()</code>中用于特定<code class="fe op oq or nr b">joinType</code>的相同规则进行匹配。这里有一个例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="fd27" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">case class</strong> someRow1(name: <strong class="nr iu">String</strong>, role: <strong class="nr iu">String</strong>)<br/><strong class="nr iu">case class</strong> someRow2(name: <strong class="nr iu">String</strong>, id: Integer)<br/><strong class="nr iu">val</strong> ds1 = Seq(someRow1("John", "Data scientist"),<br/>             someRow1("James", "Data engineer")).toDS()<br/><strong class="nr iu">val</strong> ds2 = Seq(someRow2("John", 127),<br/>              someRow2("Steve", 192)).toDS()</span><span id="9e7e" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">val</strong> joinedDS = ds1.joinWith(ds2, ds1("name") === ds2("name"), <br/>  "left")<br/>joinedDS.show(false)</span><span id="47a3" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>+----------------------+-----------+ <br/>|_1                    |_2         | <br/>+----------------------+-----------+ <br/>|[John, Data scientist]|[John, 127]| <br/>|[James, Data engineer]|null       | <br/>+----------------------+-----------+</strong></span></pre><p id="a3e8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如您所见，这类似于左外连接；但是，第二个数据集的列不会添加到第一个数据集中来创建新行。相反，每个数据集的行都有自己的列。因此第一列的类型为<code class="fe op oq or nr b">someRow1</code>，第二列的类型为<code class="fe op oq or nr b">someRow2</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8f9a" class="ms kz it nr b gy nv nw l nx ny">joinedDS.head()</span><span id="521f" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output:<br/>res22: (someRow1, someRow2) = (someRow1(John,Data scientist),someRow2(John,127))</strong></span></pre><p id="9396" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于内部连接，我们可以省略<code class="fe op oq or nr b"><a class="ae mm" href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1237" rel="noopener ugc nofollow" target="_blank">joinWith()</a></code>中的<code class="fe op oq or nr b">joinType</code>。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="35f8" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {  <br/>  joinWith(other, condition, "inner")<br/>}</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="3ee0" class="ky kz it bd la lb nl ld le lf nm lh li jz nn ka lk kc no kd lm kf np kg lo lp bi translated"><strong class="ak">连接数据集和数据帧</strong></h1><h2 id="c059" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated">联合()</h2><p id="bb31" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">类Dataset中的方法<code class="fe op oq or nr b">union()</code>可用于追加两个数据集。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c5fa" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">def</strong> union(other: Dataset[T]): Dataset[T]</span></pre><p id="11de" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">它将<code class="fe op oq or nr b">other</code>的行附加到调用它的数据集的末尾，并返回一个新的数据集。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c350" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df1 = <strong class="nr iu">Seq</strong>((1, 2, "a")).toDF("col0", "col1", "col2")<br/><strong class="nr iu">val</strong> df2 = <strong class="nr iu">Seq</strong>((4, 5, "b")).toDF("col0", "col1", "col2")<br/>df1.union(df2).show</span><span id="f71d" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output<br/>+----+----+----+ <br/>|col0|col1|col2| <br/>+----+----+----+ <br/>|   1|   2|   3| <br/>|   4|   5|   6| <br/>+----+----+----+</strong></span></pre><p id="bb91" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由<code class="fe op oq or nr b">union()</code>追加的数据帧或数据集应该具有相同的列数。请注意，在追加第二个数据帧的行时，<code class="fe op oq or nr b">union()</code>与列名不匹配，这些行是按照它们原来的列顺序追加的。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="665c" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df1 = <strong class="nr iu">Seq</strong>((1, 2, "c")).toDF("col0", "col1", "col2")<br/><strong class="nr iu">val</strong> df2 = <strong class="nr iu">Seq</strong>(("d", 4, 5)).toDF("col2", "col0", "col1")<br/>df1.union(df2).show</span><span id="4725" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output<br/>+----+----+----+ <br/>|col0|col1|col2| <br/>+----+----+----+ <br/>|   1|   2|   c| <br/>|   d|   4|   5| <br/>+----+----+----+</strong></span></pre><p id="cdbc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以使用方法<code class="fe op oq or nr b">unionByName()</code>通过类型化对象中的字段名来解析列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9d83" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">val</strong> df1 = <strong class="nr iu">Seq</strong>((1, 2, "c")).toDF("col0", "col1", "col2")<br/><strong class="nr iu">val</strong> df2 = <strong class="nr iu">Seq</strong>(("d", 4, 5)).toDF("col2", "col0", "col1")<br/>df1.unionByName(df2).show</span><span id="e876" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output<br/>+----+----+----+ <br/>|col0|col1|col2| <br/>+----+----+----+ <br/>|   1|   2|   c| <br/>|   4|   5|   d| <br/>+----+----+----+</strong></span></pre><p id="8dfc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">请注意，如果您想对两个数据集使用<code class="fe op oq or nr b">union()</code>，那么列应该匹配，因为数据集是强类型集合。例如，考虑以下代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8987" class="ms kz it nr b gy nv nw l nx ny">//won't compile<strong class="nr iu"><br/>case class</strong> someRow1(col1: <strong class="nr iu">String</strong>, col2: <strong class="nr iu">Integer</strong>, col3: <strong class="nr iu">Integer</strong>)<br/><strong class="nr iu">case class</strong> someRow2(col2: <strong class="nr iu">Integer</strong>, col3: <strong class="nr iu">Integer</strong>, col1: <strong class="nr iu">String</strong>)<br/><strong class="nr iu">val</strong> ds1 = <strong class="nr iu">Seq</strong>(someRow1("a", 2, 3)).toDS()<br/><strong class="nr iu">val</strong> ds2 = <strong class="nr iu">Seq</strong>(someRow2(5, 6, "b")).toDS()<br/>ds1.union(ds2).show<br/>ds1.<!-- -->unionByName(ds2).show()</span></pre><p id="b80b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这里同时调用<code class="fe op oq or nr b">union()</code>和<code class="fe op oq or nr b">unionByName()</code>方法会导致编译错误。</p><p id="108e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还可以使用内部连接将一个数据帧的列追加到另一个数据帧中。这里，我们首先使用<code class="fe op oq or nr b">monotonically_increasing_id()</code>在两个数据帧中创建一个虚拟列，然后在此基础上内部连接这些数据帧。最后，我们从结果数据帧中删除虚拟列。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1562" class="ms kz it nr b gy nv nw l nx ny"><strong class="nr iu">import</strong> org.apache.spark.sql.{functions=&gt;F}<br/><strong class="nr iu">val</strong> df1 = <strong class="nr iu">Seq</strong>((1, 2), (3, 4)).toDF("col1", "col2")<br/><strong class="nr iu">val</strong> df2 = <strong class="nr iu">Seq</strong>(("a", "b"), ("c", "d")).toDF("col3", "col4")</span><span id="e19f" class="ms kz it nr b gy nz nw l nx ny">df1.withColumn("row_id", F.monotonically_increasing_id())<br/>  .join(df2.withColumn("row_id", F.monotonically_increasing_id()),  <br/>  "row_id").drop("row_id")<br/> .show()</span><span id="53a2" class="ms kz it nr b gy nz nw l nx ny"><strong class="nr iu">//Output<br/>+----+----+----+----+ <br/>|col1|col2|col3|col4| <br/>+----+----+----+----+ <br/>|   1|   2|   a|   b| <br/>|   3|   4|   c|   d| <br/>+----+----+----+----+</strong></span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="6294" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我希望你喜欢阅读这篇文章。如果您有任何问题或建议，请告诉我。本文中的所有代码清单都可以作为Scala文件从GitHub下载，网址:<a class="ae mm" href="https://github.com/reza-bagheri/spark-datasets-dataframes" rel="noopener ugc nofollow" target="_blank">https://github.com/reza-bagheri/spark-datasets-dataframes</a>。您可以轻松地将此文件作为笔记本导入到DataBricks工作区中。您可以参考<a class="ae mm" href="https://docs.databricks.com/notebooks/notebooks-manage.html" rel="noopener ugc nofollow" target="_blank"> Databricks文档</a>来了解如何将Scala文件导入为Databricks笔记本。</p></div></div>    
</body>
</html>