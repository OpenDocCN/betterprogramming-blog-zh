<html>
<head>
<title>Mastering Web Scraping in Python: Scaling to Distributed Crawling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">掌握Python中的Web抓取:扩展到分布式抓取</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/mastering-web-scraping-in-python-scaling-to-distributed-crawling-34017a1322c7?source=collection_archive---------1-----------------------#2021-08-27">https://betterprogramming.pub/mastering-web-scraping-in-python-scaling-to-distributed-crawling-34017a1322c7?source=collection_archive---------1-----------------------#2021-08-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fa61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在不到300行代码中发现页面并存储您需要的确切内容</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4c30001b285ba94307aa82316f33dc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8t7GEwl50zIZHkOCZREkrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">本·艾伦在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="09eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想知道如何大规模构建网站爬虫和解析器吗？实施一个项目，以分布式和容错的方式对内容进行爬网、抓取、提取和大规模存储。我们将从以前的帖子中获取所有知识，并将其结合起来。</p><p id="7523" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们学习了<a class="ae ky" href="https://medium.com/codex/mastering-web-scraping-in-python-from-zero-to-hero-51e27705b51b" rel="noopener"> pro技术来抓取内容</a>，尽管我们今天只使用CSS选择器。然后是<a class="ae ky" href="https://uxdesign.cc/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja-8cb76db119ae" rel="noopener" target="_blank">避免阻塞的技巧</a>，我们将从中添加代理、头和无头浏览器。最后，我们<a class="ae ky" rel="noopener ugc nofollow" target="_blank" href="/mastering-web-scraping-in-python-crawling-from-scratch-cb510bcb9fb6">构建了一个并行爬虫</a>，这篇博客文章从这段代码开始。</p><p id="14a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不明白某些部分或片段，它可能在较早的职位。振作起来；冗长的片段来了。</p><h1 id="7ee8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">先决条件</h1><p id="398d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了让代码工作，你需要安装<a class="ae ky" href="https://redis.io/" rel="noopener ugc nofollow" target="_blank"> Redis </a>和<a class="ae ky" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank">python 3</a>。有些系统已经预装了它。之后，通过运行<code class="fe ms mt mu mv b">pip install</code>安装所有必要的库。</p><pre class="kj kk kl km gt mw mv mx my aw mz bi"><span id="e022" class="na lw it mv b gy nb nc l nd ne">pip install install requests beautifulsoup4 playwright "celery[redis]"<br/>npx playwright install</span></pre><h1 id="1c5f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">芹菜和雷迪斯介绍</h1><p id="45d2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><a class="ae ky" href="https://docs.celeryproject.org/en/stable/getting-started/introduction.html" rel="noopener ugc nofollow" target="_blank">芹菜</a>“是一个开源的异步任务队列。”在上一篇博文中，我们创建了一个简单的并行版本。Celery更进一步，提供了一个实际的分布式队列实现。我们将使用它在工作人员和服务器之间分配我们的负载。</p><p id="1164" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://redis.io/" rel="noopener ugc nofollow" target="_blank"> Redis </a>“是一个开源的内存数据结构存储，用作数据库、缓存和消息代理。”我们将使用Redis作为数据库，而不是使用数组和集合来存储所有内容(在内存中)。而且，芹菜可以使用Redis作为经纪人，所以我们不会需要其他软件来运行它。</p><h1 id="969d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">简单的芹菜任务</h1><p id="b71f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们的第一步是在Celery中创建一个任务，打印parameter接收的值。将代码片段保存在名为<code class="fe ms mt mu mv b">tasks.py</code>的文件中，然后运行它。如果将它作为常规Python文件运行，将只打印一个字符串。如果您用<code class="fe ms mt mu mv b">celery -A tasks worker</code>运行控制台，它将打印两行不同的内容。</p><p id="7f3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同之处在于<code class="fe ms mt mu mv b">demo</code>函数的调用。直接调用意味着“执行任务”，而<code class="fe ms mt mu mv b">delay</code>意味着“将任务排队等待工人处理。”查看文档了解更多关于<a class="ae ky" href="https://docs.celeryproject.org/en/stable/userguide/calling.html#basics" rel="noopener ugc nofollow" target="_blank">调用任务</a>的信息。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="b978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个<code class="fe ms mt mu mv b">celery</code>命令不会结束；我们需要通过退出控制台来停止它(即<code class="fe ms mt mu mv b">ctrl + C</code>)。我们将需要它几次，因为芹菜不会在代码更改后重新加载。</p><h1 id="7486" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">从任务中搜索</h1><p id="cc1b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">下一步是将芹菜任务与爬行过程连接起来。这一次，我们将使用上一篇文章中看到的<a class="ae ky" href="http://www.localhost.com:8000/blog/mastering-web-scraping-in-python-crawling-from-scratch#final-code" rel="noopener ugc nofollow" target="_blank">助手函数</a>的一个略有改动的版本。<code class="fe ms mt mu mv b">extract_links</code>将获得页面上除<code class="fe ms mt mu mv b">nofollow</code>之外的所有链接。我们稍后将添加过滤选项。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="1b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以对检索到的链接进行循环，并对它们进行排队，但这样会导致重复地搜索相同的页面。我们已经看到了执行任务的基础，现在我们将开始分解成文件并跟踪Redis上的页面。</p><h1 id="17f6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">用于跟踪URL的Redis</h1><p id="95aa" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们已经说过，依赖内存变量不再是一种选择。我们需要保存所有的数据:访问过的页面，当前正在抓取的页面，保存一个“要访问”的列表，并在以后存储一些内容。尽管如此，我们将使用Redis来避免重新抓取和重复，而不是直接对芹菜进行排队。并将URL仅排入队列一次。</p><p id="1d93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不会深入Redis的细节，但是我们将使用<a class="ae ky" href="https://redis.io/commands#list" rel="noopener ugc nofollow" target="_blank">列表</a>、<a class="ae ky" href="https://redis.io/commands#set" rel="noopener ugc nofollow" target="_blank">集合</a>和<a class="ae ky" href="https://redis.io/commands#hash" rel="noopener ugc nofollow" target="_blank">散列</a>。</p><p id="6516" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">取最后一个代码片段，删除最后两行调用任务的代码。用以下内容创建一个新文件<code class="fe ms mt mu mv b">main.py</code>。我们将创建一个名为<code class="fe ms mt mu mv b">crawling:to_visit</code>的列表，并推送起始URL。然后，我们将进入一个循环，在列表中查询条目，并阻塞一分钟，直到一个条目准备好。当检索到一个项目时，我们调用<code class="fe ms mt mu mv b">crawl</code>函数，将其执行排队。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="0e17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它几乎和以前一样，但允许我们向列表中添加项目，它们将被自动处理。我们可以通过循环<code class="fe ms mt mu mv b">links</code>并全部推送来轻松实现，但如果没有重复数据删除和最大页数，这并不是一个好主意。我们将跟踪所有使用集合的<code class="fe ms mt mu mv b">queued</code>和<code class="fe ms mt mu mv b">visited</code>，一旦它们的总和超过允许的最大值，我们将退出。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="cea5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行后，所有内容都在Redis中，所以再次运行它不会像预期的那样工作。我们需要手动清理。我们可以通过使用<code class="fe ms mt mu mv b">redis-cli</code>或类似<a class="ae ky" href="https://github.com/joeferner/redis-commander#readme" rel="noopener ugc nofollow" target="_blank"> redis-commander </a>的GUI来实现。有删除键(即<code class="fe ms mt mu mv b">DEL crawling:to_visit</code>)或<a class="ae ky" href="https://redis.io/commands/flushdb" rel="noopener ugc nofollow" target="_blank">刷新数据库</a>的命令(小心这个)。</p><h1 id="6ac7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分别负责</h1><p id="4c26" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将在项目成长之前开始分离概念。我们已经有两个文件:<code class="fe ms mt mu mv b">tasks.py</code>和<code class="fe ms mt mu mv b">main.py</code>。我们将创建另外两个来托管与爬虫相关的功能(<code class="fe ms mt mu mv b">crawler.py</code>)和数据库访问(<code class="fe ms mt mu mv b">repo.py</code>)。请看下面的回购文件片段，它并不完整，但你得到的想法。有一个<a class="ae ky" href="https://github.com/ZenRows/scaling-to-distributed-crawling" rel="noopener ugc nofollow" target="_blank"> GitHub存储库</a>，里面有最终的内容，如果你想查看的话。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="7f84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且<code class="fe ms mt mu mv b">crawler</code>文件将具有爬行、提取链接等功能。</p><h1 id="8c97" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">允许分析器定制</h1><p id="9f23" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如上所述，我们需要某种方法来提取和存储内容，并只将特定的链接子集添加到队列中。我们需要一个新的概念:默认解析器(<code class="fe ms mt mu mv b">parsers/defaults.py</code>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="7ebb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且在<code class="fe ms mt mu mv b">repo.py</code>文件中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="71a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里没有什么新的东西，但是它将允许我们抽象链接和内容提取。它将是一组作为参数传递的函数，而不是硬编码在爬虫中。现在我们可以用导入来代替对这些函数的调用(目前)。</p><p id="b55b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了将其完全抽象，我们需要一个生成器或工厂。我们将创建一个新文件来托管它— <code class="fe ms mt mu mv b">parserlist.py</code>。为了简单起见，我们允许每个域有一个定制的解析器。演示包括两个测试域:<a class="ae ky" href="https://scrapeme.live/shop/page/1/" rel="noopener ugc nofollow" target="_blank"> scrapeme.live </a>和<a class="ae ky" href="http://quotes.toscrape.com/page/1/" rel="noopener ugc nofollow" target="_blank">quotes.toscrape.com</a>。</p><p id="cc26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还没有为每个域做任何事情，所以我们将为它们使用默认的解析器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="c91f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以用新的每个域的解析器来修改这个任务。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h1 id="94ae" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">自定义分析器</h1><p id="2639" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们先以<code class="fe ms mt mu mv b">scrapeme</code>为例。检查最终版本的<a class="ae ky" href="https://github.com/ZenRows/scaling-to-distributed-crawling/blob/main/parsers/scrapemelive.py" rel="noopener ugc nofollow" target="_blank"> repo </a>和其他自定义解析器。</p><p id="0de2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这部分需要页面及其HTML的知识。如果你想感受一下，就看看吧。总而言之，我们将获得产品列表中每一项的产品id、名称和价格。然后使用id作为键将它存储在一个集合中。至于允许的链接，只有分页的链接会经过过滤。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/fc3961ef1773fad4fb3f109f43297a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*X1BxphhG2YzdcJ4qu7hvRw.png"/></div></figure><p id="5fc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe ms mt mu mv b">quotes</code>站点，我们需要以不同的方式处理它，因为每个报价没有ID。我们将提取列表中每个条目的作者和引用。然后，在<code class="fe ms mt mu mv b">store_content</code>函数中，我们将为每个作者创建一个列表并添加引用。Redis在必要时处理列表的创建。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/2339b6a1343bad9710b51f67266705f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-pVE61H8PmEgED5QDwVAMw.png"/></div></div></figure><p id="2717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最近的几个变化中，我们引入了易于扩展的定制解析器。当添加一个新站点时，我们必须为每个新域名创建一个文件，并在<code class="fe ms mt mu mv b">parserlist.py</code>中创建一行引用它。我们可以更进一步，“自动发现”它们，但没有必要让它变得更加复杂。</p><h1 id="2881" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">获取HTML:无头浏览器</h1><p id="d65d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">到目前为止，访问的每个页面都是使用<code class="fe ms mt mu mv b">requests.get</code>完成的，这在某些情况下是不够的。假设我们想使用不同的库或无头浏览器，但只是针对某些情况或领域。加载浏览器很耗内存，速度很慢，所以如果不是强制的，我们应该避免。解决办法？甚至更多的定制。新概念:收藏家。</p><p id="9813" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将创建一个名为<code class="fe ms mt mu mv b">collectors/basic.py</code>的文件，并粘贴已知的<code class="fe ms mt mu mv b">get_html</code>函数。然后通过导入来更改默认值以使用它。接下来，创建一个新文件<code class="fe ms mt mu mv b">collectors/headless_firefox.py</code>，用于获取目标HTML的新方法。和上一篇文章一样，我们将使用剧作家<a class="ae ky" href="https://playwright.dev/python/docs/intro/" rel="noopener ugc nofollow" target="_blank">。我们还将参数化头和代理，以防我们想要使用它们。<em class="nj">剧透:我们会</em>。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="1aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想在某个领域使用无头Firefox，只需修改该解析器的<code class="fe ms mt mu mv b">get_html</code>(即<code class="fe ms mt mu mv b">parsers/scrapemelive.py</code>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="4299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你在<a class="ae ky" href="https://github.com/ZenRows/scaling-to-distributed-crawling/blob/main/collectors/fake.py" rel="noopener ugc nofollow" target="_blank">最终回购</a>中看到的，我们还有一个<code class="fe ms mt mu mv b">fake.py</code>收集器用于<code class="fe ms mt mu mv b">scrapemelive.py</code>。因为我们用那个网站进行了紧张的测试，所以我们在第一时间下载了所有的产品页面，并将它们存储在一个<code class="fe ms mt mu mv b">data</code>文件夹中。我们可以用一个无头浏览器来定制，但是我们可以用一个文件阅读器来做同样的事情，因此有了“假”的名字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h1 id="3496" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">避免使用标头和代理进行检测</h1><p id="1c03" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你猜对了:我们想要添加自定义头并使用代理。我们将从创建文件<code class="fe ms mt mu mv b">headers.py</code>的头文件开始。我们不会在这里粘贴全部内容，Linux机器有三组不同的头文件，而且会很长。查看<a class="ae ky" href="https://github.com/ZenRows/scaling-to-distributed-crawling/blob/main/headers.py" rel="noopener ugc nofollow" target="_blank">回购</a>了解详情。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="ce04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以导入一组具体的头或者调用<code class="fe ms mt mu mv b">random_headers</code>来获得一个可用的选项。我们稍后将看到一个使用示例。</p><p id="728c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这同样适用于代理:创建一个新文件，<code class="fe ms mt mu mv b">proxies.py</code>。它将包含一个由提供者分组的列表。在我们的例子中，我们将只包括<a class="ae ky" href="https://free-proxy-list.net/" rel="noopener ugc nofollow" target="_blank">免费代理</a>。在<code class="fe ms mt mu mv b">proxies</code>字典中添加您的付费类型，并将默认类型更改为您喜欢的类型。如果我们要把事情复杂化，我们可以添加一个不同的提供者重试，以防失败。</p><p id="0100" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:这些<a class="ae ky" href="https://free-proxy-list.net/" rel="noopener ugc nofollow" target="_blank">免费代理</a>可能不适合你；它们是短命的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="6a95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以及在解析器中的用法:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h1 id="24a7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">将这一切结合在一起</h1><p id="3b43" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是一次漫长而多事的旅行。是时候通过完成拼图来结束它了。我们希望你了解整个过程和大规模抓取和爬行的所有挑战。</p><p id="b962" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不能在这里展示最终的代码，所以看一下<a class="ae ky" href="https://github.com/ZenRows/scaling-to-distributed-crawling" rel="noopener ugc nofollow" target="_blank">库</a>，如果有任何疑问，请不要犹豫发表评论或联系我们。</p><p id="aa65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个入口点是芹菜的<code class="fe ms mt mu mv b">tasks.py</code>和开始对URL进行排队的<code class="fe ms mt mu mv b">main.py</code>。从那里，我们开始在Redis中存储URL以保持跟踪，并开始抓取第一个URL。自定义或默认解析器将获取HTML，提取和过滤链接，并生成和存储适当的内容。我们将这些链接添加到一个列表中，然后重新开始这个过程。多亏了Celery，一旦队列中有多个链接，并行/分布式流程就开始了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3925b33c9ff9454d14d9428621561816.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*R5KCIBu1SR-YPOMDwTZe8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终文件结构</p></figure><h1 id="ae6b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">仍然缺少点</h1><p id="ea0b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们已经谈了很多，但总会有更多的进展。这里有一些我们没有包括的功能。另外，请注意，为了简洁起见，大多数代码不包含错误处理或重试。</p><h1 id="1bea" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分布的</h1><p id="c74a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们没有包括它，但芹菜提供开箱即用。对于本地测试，我们可以启动两个不同的工人<code class="fe ms mt mu mv b">celery -A tasks worker --concurrency=20 -n worker1</code>和<code class="fe ms mt mu mv b">... -n worker2</code>。方法是在其他机器上做同样的事情，只要它们可以连接到代理(在我们的例子中是Redis)。我们甚至可以动态地添加或删除工作人员和服务器，而无需重启其余部分。芹菜处理工人和分配负载。</p><p id="212e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，工人的姓名是必不可少的，尤其是在同一台机器上启动几个工人时。如果我们在不改变工人名字的情况下执行上面的命令两次，芹菜就不会正确地识别它们。因此，推出第二个作为<code class="fe ms mt mu mv b">-n worker2</code>。</p><h1 id="769e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">速率极限</h1><p id="2d78" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Celery不允许对每个任务和参数(在我们的例子中是域)进行速率限制。这意味着我们可以限制工作人员或队列，但不能限制到我们想要的细粒度细节。有几个<a class="ae ky" href="https://github.com/celery/celery/issues/5732" rel="noopener ugc nofollow" target="_blank">问题</a>未解决，还有<a class="ae ky" href="https://stackoverflow.com/questions/29854102/celery-rate-limit-on-tasks-with-the-same-parameters" rel="noopener ugc nofollow" target="_blank">解决方法</a>。通过阅读其中的几篇，我们得到的结论是，如果我们自己不跟踪请求，我们就无法做到这一点。</p><p id="fe77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用提供的参数<code class="fe ms mt mu mv b">@app.task(rate_limit="30/m")</code>，我们可以轻松地将每个任务的请求速率限制为每分钟30个。但是请记住，这将影响任务，而不是被爬网的域。</p><h1 id="843d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Robots.txt</h1><p id="5948" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">除了<code class="fe ms mt mu mv b">allow_url_filter</code>部分，我们还应该添加一个robots.txt检查器。为此，<a class="ae ky" href="https://docs.python.org/3/library/urllib.robotparser.html" rel="noopener ugc nofollow" target="_blank"> robotparser库</a>可以获取一个URL，并告诉我们是否允许抓取它。我们可以把它添加到默认或者作为一个独立的功能，然后每个刮刀决定是否使用它。我们认为它足够复杂，所以没有实现这个功能。</p><p id="2c1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您要这样做，请考虑最后一次使用<code class="fe ms mt mu mv b">mtime()</code>访问文件的时间，并不时地重新读取它。此外，缓存它以避免为每个URL请求它。</p><h1 id="92a8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="e125" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">大规模构建定制的爬虫/解析器既不是一件容易的事情，也不是一件简单的任务。我们提供了一些指导和提示，希望对你们的日常工作有所帮助。</p><p id="45f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开发如此大规模的项目之前，请考虑一些重要的要点:</p><ol class=""><li id="28cf" class="nl nm it lb b lc ld lf lg li nn lm no lq np lu nq nr ns nt bi translated">分开责任。</li><li id="50b4" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">必要时使用抽象，但不要过度工程化。</li><li id="5a2a" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">不要害怕使用专门的软件，而不是构建一切。</li><li id="d94b" class="nl nm it lb b lc nu lf nv li nw lm nx lq ny lu nq nr ns nt bi translated">即使现在不需要，也要考虑缩放；记住就好。</li></ol><p id="c07d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要忘记看看本系列的其他文章。<br/> + <a class="ae ky" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-crawling-from-scratch?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=distributed_crawling" rel="noopener ugc nofollow" target="_blank">从零开始爬行</a> (3/4) <br/> + <a class="ae ky" href="https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=distributed_crawling" rel="noopener ugc nofollow" target="_blank">像忍者一样躲避阻挡</a> (2/4) <br/> + <a class="ae ky" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-from-zero-to-hero?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=distributed_crawling" rel="noopener ugc nofollow" target="_blank">掌握抽取</a> (1/4)</p><p id="ff1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谢谢你一直陪我们到最后。这是一个有趣的系列，我们希望它对你也有吸引力。如果你喜欢，你可能会对Javascript Web抓取指南感兴趣。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><p id="b905" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nj">原载于</em><a class="ae ky" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-scaling-to-distributed-crawling?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=distributed_crawling" rel="noopener ugc nofollow" target="_blank"><em class="nj">https://www.zenrows.com</em></a></p></div></div>    
</body>
</html>