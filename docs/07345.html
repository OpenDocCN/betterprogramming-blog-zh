<html>
<head>
<title>How to Summarize Text With Google’s T5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用谷歌的T5概括文本</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6?source=collection_archive---------4-----------------------#2021-01-06">https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6?source=collection_archive---------4-----------------------#2021-01-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4f6e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在文本摘要中使用前沿的自然语言处理技术</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c778e28541bcffaf0b170b82d70002c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VLds44FJMfhocuXs"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@mitchel3uo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米切尔·罗</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7d25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自动文本摘要允许我们将长的文本片段缩短成易于阅读的短片段，这些片段仍然传达了原始文本中最重要和最相关的信息。</p><p id="6307" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将使用Google的T5构建一个简单但功能强大的文本摘要器。我们将使用PyTorch和拥抱脸的变形金刚框架。</p><p id="4e02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这分为三个部分:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6773" class="ma mb it lw b gy mc md l me mf">1. <a class="ae ky" href="#2d88" rel="noopener ugc nofollow">Import and Initialization</a><br/>2. <a class="ae ky" href="#6976" rel="noopener ugc nofollow">Data and Tokenization</a><br/>3. <a class="ae ky" href="#47b0" rel="noopener ugc nofollow">Summary Generation</a></span></pre><p id="b2d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点击这里查看本文的视频版本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mg mh l"/></div></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="2d88" class="mp mb it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">导入和初始化</h1><p id="49d2" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们需要从Transformers库中导入PyTorch和<code class="fe nl nm nn lw b">AutoTokenizer</code>和<code class="fe nl nm nn lw b">AutoModelWIthLMHead</code>对象:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9109" class="ma mb it lw b gy mc md l me mf">import torch<br/>from transformers import AutoTokenizer, AutoModelWithLMHead</span></pre><p id="1b25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PyTorch可以按照这里的说明安装，变压器可以使用<code class="fe nl nm nn lw b">pip install transformers</code>安装。如果你需要在Python中设置ML环境的帮助，请查看本文。</p><p id="fdc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦导入了所有内容，我们就可以初始化记号赋予器和模型:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7190" class="ma mb it lw b gy mc md l me mf">tokenizer = AutoTokenizer.from_pretrained('t5-base')<br/>model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)</span></pre><p id="852b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始处理一些文本数据！</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="6976" class="mp mb it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">数据和令牌化</h1><p id="11f6" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">这里使用的数据是温斯顿·丘吉尔的维基百科页面上的一段文字。当然，你可以用任何你喜欢的东西！但是，如果您想了解相同的数据，可以从这里复制:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no mh l"/></div></figure><p id="48d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们有了数据，我们需要使用我们的<code class="fe nl nm nn lw b">tokenizer</code>来标记它。这将获取每个单词或标点符号，并将其转换为数字id，T5模型将读取这些数字id并将其映射到预训练的单词嵌入。</p><p id="9ec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标记化非常容易。我们只是在我们的输入数据上调用<code class="fe nl nm nn lw b">tokenizer.encode</code>:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0ce7" class="ma mb it lw b gy mc md l me mf">inputs = tokenizer.encode("summarize: " + text,<br/>                          return_tensors='pt',<br/>                          max_length=512,<br/>                          truncation=True)</span></pre></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="47b0" class="mp mb it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">汇总生成</h1><p id="35f5" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们通过调用<code class="fe nl nm nn lw b">model.generate</code>来总结使用T5的标记化数据，如下所示:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8ff2" class="ma mb it lw b gy mc md l me mf">summary_ids = model.generate(inputs, max_length=150, min_length=80, length_penalty=5., num_beams=2)</span></pre><ul class=""><li id="b3bc" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated"><code class="fe nl nm nn lw b">max_length</code>定义我们希望在摘要中包含的最大令牌数</li><li id="0c0d" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><code class="fe nl nm nn lw b">min_length</code>定义我们想要的最小令牌数</li><li id="890b" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><code class="fe nl nm nn lw b">length_penalty</code>允许我们对生成低于/高于我们定义的最小/最大阈值的汇总的模型进行或多或少的惩罚</li><li id="53a7" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><code class="fe nl nm nn lw b">num_beams</code>设置探索最有希望预测的潜在表征的波束数量[1]</li></ul><p id="8348" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们有了摘要标记，我们就可以使用<code class="fe nl nm nn lw b">tokenizer.decode</code>将它们解码成人类可读的语言:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d3ed" class="ma mb it lw b gy mc md l me mf">summary = tokenizer.decode(summary_ids[0])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/43d4330dc984693b9ea59066c742b099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtDxtpWwGdl56HiGJRnvGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们T5生成的维基百科文本摘要</p></figure><p id="0594" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这样，我们用Google的T5构建了一个文本摘要器！</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="4f7f" class="mp mb it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">结论</h1><p id="79b6" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">这真的是所有的事情。总共七行代码，开始使用谷歌最先进的机器学习算法之一处理复杂的自然语言问题。</p><p id="3f6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现把这样的东西放在一起是如此容易，这令人吃惊，我希望这篇简短的教程已经证明了NLP可以(有时)多么容易理解——也许还解决了一些问题。</p><p id="6b74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章。感谢阅读！</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="cdb1" class="mp mb it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">参考</h1><p id="a94e" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">[1] <a class="ae ky" href="https://en.wikipedia.org/wiki/Beam_search" rel="noopener ugc nofollow" target="_blank">光束搜索</a>，维基百科</p><p id="2404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖NLP与变形金刚课程</a></p></div></div>    
</body>
</html>