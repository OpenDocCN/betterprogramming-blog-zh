<html>
<head>
<title>Kubernetes Lessons in Alerting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Kubernetes的警示课程</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/kubernetes-lessons-in-alerting-a0b7a455e89d?source=collection_archive---------6-----------------------#2020-07-09">https://betterprogramming.pub/kubernetes-lessons-in-alerting-a0b7a455e89d?source=collection_archive---------6-----------------------#2020-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db3c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实时问题是一个学习和提高的好机会。这就是我们的遭遇</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49a876617334ba9209357805f19d3d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S9XUAr4K3rkbxz0G"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@yer_a_wizard?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗勒</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><p id="5d92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将探讨一个案例，当我们的一个服务扩展到其最大值时，我们如何改变我们的警报以防止这在未来成为一个问题。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c526" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">我们的基础设施</h1><p id="472e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们在本文中作为示例使用的服务部署在启用了自动缩放的<a class="ae ky" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a> (K8s)上。我们基于每秒请求数进行扩展，K8s被配置为将每秒请求数(RPS)保持在50。在扩展服务之前会有一点延迟，因为RPS是一分钟内的平均值。有关K8s缩放的更多信息，请查看<a class="ae ky" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" rel="noopener ugc nofollow" target="_blank">的文档</a>。</p><p id="57a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了保持高可用性，我们运行两个K8s集群。下图将这些集群显示为<code class="fe mz na nb nc b">region-1</code>和<code class="fe mz na nb nc b">region-2</code>。当涉及自动扩展时，这会产生额外的复杂性，因为集群是完全独立的，并且不共享指标。我们的网站以主动-主动方式运行，并且在两个区域之间实现了负载平衡。</p><p id="64a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<a class="ae ky" href="https://prometheus.io/" rel="noopener ugc nofollow" target="_blank"> Prometheus </a>和<a class="ae ky" href="https://grafana.com/" rel="noopener ugc nofollow" target="_blank"> Grafana </a>来监控我们的应用程序。下图来自Grafana，显示了来自Prometheus的各种指标。Alert Manager用于提醒Prometheus metrics，允许我们在晚上唤醒待命的工程师，并在白天向工程师发送通知。我们试图在白天发送先发制人的通知，并且只有在出现实际上影响客户的问题时才叫醒待命工程师。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c519" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">该事件</h1><p id="ec39" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">由于一个单独的应用程序出现问题，我们的基础架构工程师故障切换到一个数据中心/区域。这导致所有请求都发送到一个区域，如下图所示。绿线显示了服务正在处理的请求总数，另外两条线是针对每个地区的。16:20刚过，故障转移发生在橙线与绿线汇合的地方，蓝线归零确认了流量变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/d6cab20b7d8b44d8e272723233400d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpOflfAbEoL2_YYbTj8fkg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图表显示每个地区的请求。所有请求都发往一个地区。</p></figure><p id="931d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在事故发生前几个小时，团队得到了警报，因为一个区域已经达到了80%的容量(最多10个吊舱中有8个正在使用)。最大吊舱可以在下图中看到，由10处的直线显示。该图还示出了一个区域放大到最大值，而另一个区域由于交通状况的变化而缩小。我们将缩放比例配置为最少为2，这就是蓝线不为零的原因。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/c1d5a3476c7378feda1c73ff067d636c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TxprAWXdP3anrbpXbznMg.png"/></div></div></figure><p id="58ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向上扩展到允许的最大pod本身实际上不是问题。当服务的请求数量超过pod可以处理的最大数量时，就会出现问题。你可能会争辩说，由于达到最大pod而叫醒待命工程师是错误的，因为客户可能不会受到影响。相反，您应该只在响应时间或错误增加时叫醒人们。这确实取决于在几小时内有先发制人的警报，并对你的警报有信心。</p><p id="e840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的两张图显示了每个单独的pod是如何受到影响的。第一个图表显示了pods是如何比正常情况下每秒接收更多请求的。这导致CPU增加并达到最大值，如第二个图表所示。这证实了我们没有足够的容量来满足在故障转移期间发送给服务的RPS。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/2c45fba6521f43b2c580058422478631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4yQdQ98ZPGKaXetAek5eA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图表显示每个pod每秒的请求数。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/22f7c5630714eb0484262874f31d4700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WH0aMRrCfOvKFJvF3VDInA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">显示每个pod的CPU使用率的图表。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c176" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">我们学到了什么</h1><p id="de0e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们知道，由于白天触发了80%利用率警报，我们已经达到了可以调配的pod数量的极限。我们很幸运，那天晚上发生了故障转移。我们计划第二天提高限制，但如果不是因为这个问题，我们可能不会解决真正的问题。由于应用程序在两个地区运行，我们需要在警报中考虑这一点。我们应该针对40%发出警报，而不是针对80%的pod利用率发出警报，这是一个区域处于故障切换状态时的80%。</p><p id="6e1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事故发生15分钟后，向我们的待命工程师发出了警报。这被设置为15分钟的持续时间，因为如果我们爆发到最大吊舱和规模缩小，这不是世界末日。理论上，客户不会受到影响。这应该被改变以考虑请求的数量而不是正在使用的pod的数量。当请求数量超过最大pod处理能力时，将触发警报。</p><p id="c49d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的服务高度依赖于下游响应时间，因为它是一种代理。下游服务没有明确定义的预期响应时间，因此我们很难对诸如高响应时间和错误之类的症状发出警报。理想情况下，我们会对这些发出警报，而不是最大化pod或对pod的太多请求。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="012d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="851e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">感谢阅读。我希望这能帮助其他工程师改进他们的应用程序的扩展和警报方式。我确信有更多的经验可以学习和分享。</p></div></div>    
</body>
</html>