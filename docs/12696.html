<html>
<head>
<title>HaGRID — HAnd Gesture Recognition Image Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">海格——手势识别图像数据集</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/hagrid-hand-gesture-recognition-image-dataset-a70cc291e539?source=collection_archive---------4-----------------------#2022-06-23">https://betterprogramming.pub/hagrid-hand-gesture-recognition-image-dataset-a70cc291e539?source=collection_archive---------4-----------------------#2022-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e45f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">构建您自己的数据集</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/85ac76cbe66f056d1f27cd1ad9caa1fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dhZ8FB7tDpwteA3_.jpg"/></div></div></figure><p id="cff1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">手势在人类交流中的使用起着重要的作用:手势可以在情感上加强陈述，也可以完全取代陈述。此外，手势识别(HGR)可以成为人机交互的一部分。</p><p id="6abe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种系统可以用于视频会议服务(Zoom、Skype、Discord、Jazz等)。)、家庭自动化系统、汽车行业、为有语言和听力障碍的人提供的服务等。此外，该系统可以成为积极的手语使用者——听力和语言障碍者——的虚拟助理或服务的一部分。</p><p id="f707" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些领域要求系统在线工作，并对背景、场景、主题和照明条件具有鲁棒性。这些问题以及其他一些问题启发我们创建了一个新的HGR数据集。</p><h1 id="9546" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">资料组</h1><p id="c6fd" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated"><strong class="kt ir">海格</strong>(<strong class="kt ir">HA</strong>nd<strong class="kt ir">G</strong>esture<strong class="kt ir">R</strong>ecognition<strong class="kt ir">I</strong>mage<strong class="kt ir">D</strong>ataset)是HGR系统最大的数据集之一。该数据集包含552，992张全高清RGB图像，分为<strong class="kt ir"> 18 </strong>类手势。我们特别关注与设备的交互来管理它们。这就是为什么所有18个选择的手势都是功能性的，为大多数人所熟悉，并且可能是采取一些行动的激励。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/ecbc97a264b2c95ca94c33051de75f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qdHkFap6kzTBeCHo.jpg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">“inv。”是“倒置”的缩写</p></figure><p id="f0ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们使用众包平台来收集数据集，并考虑各种参数以确保数据的多样性。数据集包含<strong class="kt ir">34730个独特的场景</strong>。它主要是在室内采集的，光线有相当大的<strong class="kt ir">变化，包括人造光和自然光。此外，数据集包括在极端条件下拍摄的图像，例如<strong class="kt ir">面对和背对窗户</strong>。此外，受试者必须在距离摄像机0.5至4米</strong>的<strong class="kt ir">处展示手势。</strong></p><p id="147a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">海格可用于两项HGR任务:手部检测和手势分类；以及额外的任务——引导手搜索。注释由COCO格式的手的边界框<code class="fe mp mq mr ms b">[top left X position, top left Y position, width, height]</code>和手势标签组成。同样，注释具有标记<code class="fe mp mq mr ms b">leading hands</code>(手势为<code class="fe mp mq mr ms b">left</code>或<code class="fe mp mq mr ms b">right</code>)和<code class="fe mp mq mr ms b">leading_conf</code>作为<code class="fe mp mq mr ms b">leading_hand</code>注释的置信度。我们提供了<code class="fe mp mq mr ms b">user_id</code>字段，允许您自己分割训练/测试数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="da9f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请记住，建议的数据集包含两个位置的一些手势:手的前部和后部。这允许使用两个静态手势来解释动态手势。例如，通过手势<code class="fe mp mq mr ms b">stop </code>和<code class="fe mp mq mr ms b">stop inverted</code>，您可以设计动态手势<code class="fe mp mq mr ms b">swipe up</code> ( <code class="fe mp mq mr ms b">stop thumbs down</code>，即<code class="fe mp mq mr ms b">stop</code>旋转180度，作为行的开始，<code class="fe mp mq mr ms b">stop inverted</code>作为结束)，以及<code class="fe mp mq mr ms b">swipe down</code> ( <code class="fe mp mq mr ms b">stop</code>作为行的开始，<code class="fe mp mq mr ms b">stop inverted thumbs down</code>，即<code class="fe mp mq mr ms b">stop inverted</code>旋转180度，作为结束)。此外，您还可以获得2个动态手势，<code class="fe mp mq mr ms b">swipe right</code>和<code class="fe mp mq mr ms b">swipe left</code>，并增加90度旋转。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/7c62dc8c41ac2c12e47cf8773ce24894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-dtyjPOLE6u3AmPv8830w.jpeg"/></div></div></figure><p id="e3c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在注释中增加了<code class="fe mp mq mr ms b">Leading_hand</code>，用静态手势解释动态手势:<code class="fe mp mq mr ms b">swipe up</code>和<code class="fe mp mq mr ms b">swipe down</code>可以用一只手显示，而手势<code class="fe mp mq mr ms b">swipe right</code>和<code class="fe mp mq mr ms b">swipe left</code>不使用第二只手很难显示。如果左手显示水平静态手势<code class="fe mp mq mr ms b">stop</code>和<code class="fe mp mq mr ms b">stop inverted</code>，则为动态手势<code class="fe mp mq mr ms b">swipe right</code>，否则为<code class="fe mp mq mr ms b">swipe left</code>。引导手标签可以用来从一个手势设计两个手势。比如右<code class="fe mp mq mr ms b">three</code>和左<code class="fe mp mq mr ms b">three</code>可以是两种不同的手势<code class="fe mp mq mr ms b">right three</code>和<code class="fe mp mq mr ms b">left three</code>。</p><p id="2d81" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下载海格的链接在<a class="ae mw" href="https://github.com/hukenovs/hagrid" rel="noopener ugc nofollow" target="_blank">资源库</a>中公开。</p><h1 id="3787" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">数据挖掘</h1><p id="d315" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">数据集分4个阶段收集:(1)手势图像收集阶段，称为<strong class="kt ir">挖掘</strong> , (2) <strong class="kt ir">验证</strong>阶段，其中检查挖掘规则和一些条件，(3) <strong class="kt ir">过滤</strong>不合适的图像，(4) <strong class="kt ir">标记边界框和引导手的注释</strong>阶段。通过为每个手势类别划分池，分类阶段被构建到挖掘和验证管道中。</p><ol class=""><li id="3338" class="mx my iq kt b ku kv kx ky la mz le na li nb lm nc nd ne nf bi translated"><strong class="kt ir">采矿。</strong>人群工作人员的任务是用任务描述中指定的特定姿势给自己拍照。我们定义了以下标准:(1)注释者必须在距离摄像机0.5 - 4米的<strong class="kt ir">处，(2)有手势的手(即引导手)必须完全在</strong>帧中<strong class="kt ir">。有时，受试者会接受一项任务，在弱光条件下或对着明亮的光源拍照，以使神经网络对极端条件具有弹性。还使用图像散列比较来检查所有接收到的图像的重复。</strong></li><li id="1a88" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated"><strong class="kt ir">验证。</strong>由于用户试图在挖掘阶段欺骗系统，我们实施了验证阶段以获得高置信度图像。验证阶段的目标是在挖掘阶段选择正确执行的图像。</li><li id="257e" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated"><strong class="kt ir">过滤。</strong>由于伦理原因，儿童、不穿衣服的人和带有铭文的图像在此阶段被从海格中移除。</li><li id="8e32" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated"><strong class="kt ir">注解。</strong>在此阶段，群组工作人员应在每张图像上的手势周围画一个红色边框，如果手完全在帧中，则在没有手势的手周围画一个绿色边框。不同的颜色需要进一步翻译成标签。</li></ol><p id="5ed6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于数据集和挖掘的详细信息在我们的<a class="ae mw" href="https://arxiv.org/pdf/2206.08219.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中提供。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="60c7" class="ln lo iq bd lp lq ns ls lt lu nt lw lx jw nu jx lz jz nv ka mb kc nw kd md me bi translated">手势识别系统</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/51fffdb5a1cdbbcafd07ee7dfce469a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*4C38CQlsk3Q7K2MCs7QsaA.gif"/></div></figure><p id="19f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本节中，我们将提供一个教程来展示如何使用海格建立一个HGR系统，它可以检测你的手势并对其进行分类。另外，我们要说明如何做一个有两个头的模型，其中第二个是预测主导手。</p><p id="a803" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们从导入所有必需的库开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="6cf8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">海格分为18类手势和一个<code class="fe mp mq mr ms b">no gesture</code>类。由于本教程选择的数据集的子样本，我们不使用很多历元，并且模型可以快速学习。在模型训练之前，它移动到选定的设备。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="636f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们实现了一个继承自<code class="fe mp mq mr ms b">Dataset</code>类型的类<code class="fe mp mq mr ms b">GestureDataset</code>，并定义了数据读取和数据预处理函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="c15a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，我们为<code class="fe mp mq mr ms b">get_transform()</code>函数实现了自己的类<code class="fe mp mq mr ms b">ToTensor</code>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="23b7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们指定两个不同的数据集，一个用于训练模型(训练集)，另一个用于测试模型(测试集)。两个命令只有一个参数<code class="fe mp mq mr ms b">is_train</code>不同，用户使用<code class="fe mp mq mr ms b">user_id</code>散列将整个数据集分成两部分。如果有更多数据，可以将定型集分为定型集和验证集。</p><p id="3efc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">数据集/ </strong>的结构如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="7f64" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">已实施<code class="fe mp mq mr ms b">GestureDataset</code>的输出如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="aa3e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们创建简短的类名，以便于图像和混淆矩阵的可视化:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="48c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们尝试可视化多个图像，以确保数据得到正确处理:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/39414de29cb18b461ef39a729341d682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UXTIVr1ghJ30DOya.png"/></div></div></figure><p id="1592" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，我们可以将我们的图片放入一个<code class="fe mp mq mr ms b">DataLoader</code>中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="d0a2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们实现了一个类<code class="fe mp mq mr ms b">ResNet18</code>,将第二个头添加到预先训练好的火炬视觉ResNet18模型中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="7a09" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用动量为0.9、重量衰减为0.0005的SGD作为优化器来训练我们的模型。学习率从0.005开始。选择交叉熵损失函数作为判据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="13f4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，我们在<code class="fe mp mq mr ms b">train_dataloader</code>中迭代一批图像。所有管道都是标准的，除了计算两个任务的损耗并求和。代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="d401" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们在我们的<code class="fe mp mq mr ms b">test_dataloader</code>上评估训练过的模型。与模型训练代码类似，以下代码在计算两个指标时不同于标准代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="d0a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于每个任务，模型评估返回的F1分数为<strong class="kt ir"> 93.8 % </strong>。给定下面的混淆矩阵，我们可以得出结论，所有的类都很好地彼此分离。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4ecfde9211cb5e84105a047f7cea7a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/0*5aO68wPT1G1HckNg.png"/></div></figure><h1 id="d899" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">资源</h1><p id="c8ae" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">整个数据集、每类100幅图像的试用版、预训练模型和演示在<a class="ae mw" href="https://github.com/hukenovs/hagrid" rel="noopener ugc nofollow" target="_blank">资源库</a>中公开提供。</p><p id="285f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其他链接:<a class="ae mw" href="https://arxiv.org/pdf/2206.08219.pdf" rel="noopener ugc nofollow" target="_blank"> arXiv </a>，<a class="ae mw" href="https://www.kaggle.com/datasets/kapitanov/hagrid" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>，<a class="ae mw" href="https://habr.com/ru/company/sberdevices/blog/671614/" rel="noopener ugc nofollow" target="_blank"> Habr </a></p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><pre class="kg kh ki kj gt oa ms ob oc aw od bi"><span id="5c8b" class="oe lo iq ms b gy of og l oh oi">Co-authored by <a class="ae mw" href="https://www.linkedin.com/in/hukenovs/" rel="noopener ugc nofollow" target="_blank">Alexander Kapitanov</a>, <a class="ae mw" href="https://www.linkedin.com/in/makhliarchuk" rel="noopener ugc nofollow" target="_blank">Andrey Makhlyarchuk</a></span></pre></div></div>    
</body>
</html>