<html>
<head>
<title>How To Run Meta’s New AI System ‘Galactica’ in a Python Environment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Python环境中运行Meta的新人工智能系统‘Galactica’</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-run-metas-new-ai-system-galactica-in-a-python-environment-57e6b96420b1?source=collection_archive---------0-----------------------#2022-12-02">https://betterprogramming.pub/how-to-run-metas-new-ai-system-galactica-in-a-python-environment-57e6b96420b1?source=collection_archive---------0-----------------------#2022-12-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9f13" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">测试有争议的人工智能仅两天后就被关闭</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/91c1f7e2dc7a707b3cff0bd81ff95221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KFfBbWPTASUoG4jn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@steve_j?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯蒂夫·约翰森</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="fee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">11月15日，卡拉狄加正式呈现在世人面前。它是Meta AI为科学开发的大型语言模型，应该能够自动<em class="lv">组织</em>科学知识。</p><p id="f69b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在实践中，这意味着能够<a class="ae ky" href="https://twitter.com/paperswithcode/status/1592546933679476736?s=20&amp;t=DJ6184tl4871w-Guwfhstg" rel="noopener ugc nofollow" target="_blank"><em class="lv">总结学术文献，解决数学问题，生成维基文章，编写科学代码，注释分子和蛋白质，以及更多的</em> </a> <em class="lv">。</em></p><p id="c81f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Meta AI由深度学习巨头Yann LeCun、Mark Zuckerberg和Rob Fergus创立。预印本<a class="ae ky" href="https://galactica.org/static/paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>不缺少最高级，仅举几个例子:训练了4800万篇论文、教科书、课堂笔记等等。首先引用伽利略·伽利雷的一句话来描述数据集。</p><p id="3f14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在某些任务上，以68.2%对49.0%的分数显著击败GPT-3。当然，一些能激发很多兴趣的东西。对于此次发布，网页galactica.org允许测试人工智能。然而，当第一批结果发布在社交媒体上时，很明显有些不对劲。</p><p id="c96d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，产生的许多结果没有意义。有人称之为<a class="ae ky" href="https://www.aiweirdness.com/galactica/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">编东西的AI知识库</em></a><em class="lv">；还有人把它描述成一个什么<a class="ae ky" href="https://towardsdatascience.com/galactica-what-dangerous-ai-looks-like-f31366438ca6" rel="noopener" target="_blank"> <em class="lv">的例子，危险的AI长得像</em> </a>。而且上市才两天，就<a class="ae ky" href="https://www.cnet.com/science/meta-trained-an-ai-on-48-million-science-papers-it-was-shut-down-after-two-days/" rel="noopener ugc nofollow" target="_blank">被关停</a>。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="3b81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果你，尽管公共反斜杠变得好奇，仍然有一种方法在你自己的系统上运行卡拉狄加，尽管公共演示被关闭。这个故事会引导你度过难关。</p><p id="0982" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将概括一个演示提示，尝试我们自己的提示，并查看GPU和CPU上的结果和计算时间。</p><h1 id="f1bd" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">要求</h1><p id="8d49" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">训练卡拉狄加所需的硬件非同寻常。</p><p id="97fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有几个不同大小的型号，从<code class="fe mv mw mx my b">Mini</code> (125 M参数)到<code class="fe mv mw mx my b">Huge</code> (120 B参数)。</p><p id="3ad5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mv mw mx my b">Huge</code>模型是在128(！)NVIDIA A100 80 GB显卡。虽然推断应该在一台设备上运行，但每张卡的价格都超过10，000美元。</p><p id="d1f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在消费级电脑上运行完整的模型在短期内是不可能的。然而，我们将能够运行一些较小的模型。</p><h1 id="3148" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">可用型号</strong></h1><p id="fedc" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">让我们先来看看不同的小型模型应该如何表现:我们可以通过比较不同模型的验证损失(本文图6)来估计它们与<code class="fe mv mw mx my b">Huge</code>模型的比较情况:</p><pre class="kj kk kl km gt mz my na bn nb nc bi"><span id="ef25" class="nd lz it my b be ne nf l ng nh">   Model     Parameters   Loss   <br/> ---------- ------------ ------- <br/>  Huge       120B         ~1.8   <br/>  Large      30B          ~1.81  <br/>  Standard   6.7B         ~2     <br/>  Base       1.3B         ~2.25  <br/>  Mini       125M         ~2.8</span></pre><p id="2b2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较这些型号，<code class="fe mv mw mx my b">Mini</code>型号的损失比<code class="fe mv mw mx my b">Huge</code>型号高50%以上。虽然这为我们提供了损失的定量估计，但很难看出这对结果的质量意味着什么。稍后，我们将在不同的模型尺寸上测试相同的提示。</p><p id="3829" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了下载模型的权重，您需要足够的磁盘空间。对于标准模型，您将需要大约25 GB的空间，并且所需空间与参数的数量成比例。</p><h2 id="bf69" class="ni lz it bd ma nj nk dn me nl nm dp mi li nn no mk lm np nq mm lq nr ns mo nt bi translated">装置</h2><p id="ce66" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">卡拉狄加运行在Python上，所以你需要一个工作的Python环境。</p><p id="a418" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此处给出的说明适用于包管理系统，如<a class="ae ky" href="https://docs.conda.io/en/latest/miniconda.html" rel="noopener ugc nofollow" target="_blank"> miniconda </a>或<a class="ae ky" href="https://www.anaconda.com" rel="noopener ugc nofollow" target="_blank"> anaconda </a>。<a class="ae ky" href="https://github.com/paperswithcode/galai" rel="noopener ugc nofollow" target="_blank"> Galactica GitHub </a>提供了通过pip和<code class="fe mv mw mx my b">pip install galai</code>安装Galactica的快速入门指南。</p><p id="304e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对我来说，这不工作，并显示CUDA的问题，虽然我已经在一个非CUDA系统上测试过。</p><p id="2d6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，模型重量也可以在<a class="ae ky" href="https://huggingface.co/models?other=galactica" rel="noopener ugc nofollow" target="_blank">拥抱面部中枢</a>中获得，并且它们可以在变形金刚库中开箱使用。</p><p id="fea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这对我来说完美无缺，这是我们将在这里描述的路线。顺便说一下，虽然该模型也在CPU上运行，但强烈建议使用GPU加速，但安装可能会很棘手。我在装有GeForce 2080 Ti的Windows系统和装有苹果M1 Max的Macintosh系统上进行了测试。</p><p id="0744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，确保您已经正确设置了GPU环境。对于Nvidia GPUs，您可以遵循其中一个CUDA指南，例如<a class="ae ky" href="https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html" rel="noopener ugc nofollow" target="_blank">这一个</a>用于Windows。通过在命令行执行<code class="fe mv mw mx my b">nvidia-smi</code>可以快速检查是否安装了CUDA。您应该看到驱动程序和安装的CUDA工具包版本。对于M1 Mac系统，您将需要Xcode命令行工具。一个好的指南可以在这里找到<a class="ae ky" href="https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c" rel="noopener" target="_blank">。</a></p><ul class=""><li id="6b80" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">从在conda中创建一个新环境开始:<code class="fe mv mw mx my b">conda create -n torch-gal python=3.8</code></li><li id="997f" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">然后激活它:<code class="fe mv mw mx my b">conda activate torch-gal</code></li></ul><p id="065e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卡拉狄加使用PyTorch，所以按照PyTorch网站上的安装说明<a class="ae ky" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank">进行系统配置</a>。对于Windows和CUDA显卡，这意味着您需要确保选择正确的CUDA工具包。对于M1 Mac系统，如果您想要GPU加速，您将需要选择预览(夜间)-版本。</p><ul class=""><li id="2faf" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">CUDA 11.7的例子:<code class="fe mv mw mx my b">conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia</code></li><li id="8f7b" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">M1的例子:<code class="fe mv mw mx my b">conda install pytorch -c pytorch-nightly</code>。</li><li id="4857" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">安装一些实用程序包:<code class="fe mv mw mx my b">conda install numpy tokenizers</code></li><li id="c260" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">接下来，我们将安装变形金刚库:<code class="fe mv mw mx my b">pip install transformers accelerate</code></li><li id="4eed" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">另外，如果你喜欢用Jupyter笔记本做编码:<code class="fe mv mw mx my b">conda install -c conda-forge jupyter jupyterlab</code></li><li id="cf3a" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">您可以使用以下函数测试Python中的torch是否可以访问GPU。Mac: <code class="fe mv mw mx my b">torch.backends.mps.is_available()</code>，CUDA: <code class="fe mv mw mx my b">torch.cuda.is_available()</code></li></ul><h1 id="9149" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">提示</h1><p id="ce1d" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">一旦一切都设置好了，我们可以概括一下<a class="ae ky" href="https://galactica.org/explore/" rel="noopener ugc nofollow" target="_blank">卡拉狄加演示</a>中的提示。我们将使用以下提示作为示例:</p><ul class=""><li id="64e5" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated"><code class="fe mv mw mx my b">The Transformer architecture [START_REF]</code></li></ul><p id="f402" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中包含参考关键字[START_REF]并应该给我们一个参考著名的<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="lv">注意是你所需要的</em> </a>论文。</p><h2 id="2432" class="ni lz it bd ma nj nk dn me nl nm dp mi li nn no mk lm np nq mm lq nr ns mo nt bi translated">中央处理器</h2><p id="3b4e" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">我们可以通过变形金刚的高级API在CPU上运行卡拉狄加。用相应的型号替换型号串(例如，<code class="fe mv mw mx my b">facebook/galactica-6.7b</code>是<strong class="lb iu">标准</strong>型号，<code class="fe mv mw mx my b">facebook/galactica-125m</code>是<strong class="lb iu">迷你</strong>型号)。)注意，在第一次执行时，库会将权重下载到缓存中。这可能是您的主驱动器，而不一定是保存代码的地方，所以请确保有足够的磁盘空间。如上所述，模型可能非常大。</p><pre class="kj kk kl km gt mz my na bn nb nc bi"><span id="a244" class="nd lz it my b be ne nf l oi nh">from transformers import pipeline<br/>model = pipeline("text-generation", model="facebook/galactica-6.7b")<br/>input_text = "The Transformer architecture [START_REF]"<br/>model(input_text)</span></pre><p id="f5ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将为我们提供以下结果，这是对Transformers论文的预期参考。</p><blockquote class="oj ok ol"><p id="eff8" class="kz la lv lb b lc ld ju le lf lg jx lh om lj lk ll on ln lo lp oo lr ls lt lu im bi translated">[{ ' generated _ text ':' The Transformer architecture[START _ REF]Attention is All you Need，Vaswani is a sequence-to-'}]</p></blockquote><h2 id="0d79" class="ni lz it bd ma nj nk dn me nl nm dp mi li nn no mk lm np nq mm lq nr ns mo nt bi translated">GPU: CUDA和MPS</h2><p id="5978" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">为了在GPU上运行相同的提示，我们将CUDA设备作为参数传递给模型初始化，例如，<code class="fe mv mw mx my b">pipeline(..., device=0),</code>在第一个GPU上运行。目前，使用<code class="fe mv mw mx my b">pipeline</code>时不支持苹果的MPS。</p><p id="99c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了克服这一点，我们可以使用一个更低级的API，让我们能够更好地控制代码的执行位置:<code class="fe mv mw mx my b">OPTFForCausalLM,</code>，它也适用于MPS。我发现它工作更稳定，消耗更少的内存。这是这样的:</p><pre class="kj kk kl km gt mz my na bn nb nc bi"><span id="830a" class="nd lz it my b be ne nf l oi nh">from transformers import AutoTokenizer, OPTForCausalLM<br/><br/>tokenizer = AutoTokenizer.from_pretrained("facebook/galactica-6.7b")<br/>model = OPTForCausalLM.from_pretrained("facebook/galactica-6.7b", device_map="auto")<br/>input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")<br/><br/>outputs = model.generate(input_ids)<br/>r = tokenizer.decode(outputs[0])<br/>print(r)</span></pre><p id="7179" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出:</p><blockquote class="oj ok ol"><p id="67c2" class="kz la lv lb b lc ld ju le lf lg jx lh om lj lk ll on ln lo lp oo lr ls lt lu im bi translated">变压器架构[START_REF]注意是你所需要的，Vaswani[END_REF]是一个序列到-</p></blockquote><p id="3951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于Mac，您应该指定MPS而不是cuda:。<code class="fe mv mw mx my b">input_ids.to("mps")</code>。你也可以使用CPU。<code class="fe mv mw mx my b">input_ids</code>。</p><h2 id="d1eb" class="ni lz it bd ma nj nk dn me nl nm dp mi li nn no mk lm np nq mm lq nr ns mo nt bi translated">计时</h2><p id="e452" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">在标准架构上运行该提示符大约需要一分钟:</p><pre class="kj kk kl km gt mz my na bn nb nc bi"><span id="4426" class="nd lz it my b be ne nf l ng nh">          Device           Execution Time  <br/> ------------------------ ---------------- <br/>  i9-9920X                 55s             <br/>  NVIDIA GeForce 2080 Ti   68s             <br/>  Apple M1 MAX (CPU)       105s            <br/>  Apple M1 MAX (GPU)       29s             </span></pre><h1 id="9f03" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">测试卡拉狄加</h1><h2 id="dade" class="ni lz it bd ma nj nk dn me nl nm dp mi li nn no mk lm np nq mm lq nr ns mo nt bi translated">问题</h2><p id="a08e" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">查找参考资料似乎可以。可以说这里有一点好处；用传统的搜索引擎找到相关文献可能要快得多。那么一些更有挑战性的问题怎么样？我们来试试一个基本的科学问题:<em class="lv">一个细胞的细胞核有多大？</em> Google返回233.000.000个结果，第一个答案告诉我们大概的大小。6微米和对维基百科文章的引用。这个需要0.6s。</p><p id="ee0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卡拉狄加建议以<code class="fe mv mw mx my b">Question: [xxx] Answer:</code>的形式公式化问题，让我们看看“<em class="lv">问题:细胞核有多大？”</em></p><p id="3236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回答:</p><pre class="kj kk kl km gt mz my na bn nb nc bi"><span id="f442" class="nd lz it my b be ne nf l ng nh">(Trained on Apple M1 w. GPU)<br/><br/>   Model     Time   Question: How large is the nucleus of a cell? Answer:  <br/> ---------- ------ ------------------------------------------------------- <br/>  Mini        7.5   100,0                                                  <br/>  Base       10.6   10000                                                  <br/>  Standard   29.2   The nucleus of a cell is</span></pre><p id="3b14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，结果没有太多意义。另一个带有修改提示的测试(没有问题):<strong class="lb iu"> <em class="lv">细胞核有多大？</em> </strong></p><blockquote class="oj ok ol"><p id="6989" class="kz la lv lb b lc ld ju le lf lg jx lh om lj lk ll on ln lo lp oo lr ls lt lu im bi translated">细胞核是遗传物质</p></blockquote><p id="859f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也没什么意义。</p><p id="1ca0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为对比，我在基于GPT-3的人工智能写作工具<a class="ae ky" href="https://lex.page/" rel="noopener ugc nofollow" target="_blank"> Lex </a>上运行了同样的问题。这大约需要。大约5s:</p><blockquote class="oj ok ol"><p id="6d5d" class="kz la lv lb b lc ld ju le lf lg jx lh om lj lk ll on ln lo lp oo lr ls lt lu im bi translated">细胞核的大小因细胞的具体类型而有很大差异。通常，大多数细胞核的大小在约1至10微米(微米)的范围内。有些原子核可以大到100 μm。</p></blockquote><p id="b1c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这显然是一个更好的结果。</p><h2 id="48ee" class="ni lz it bd ma nj nk dn me nl nm dp mi li nn no mk lm np nq mm lq nr ns mo nt bi translated">关键词</h2><p id="76a6" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">Galactica提供了不同的关键字，可以添加到文本提示中以定制输出。我们已经有了用于引用的<code class="fe mv mw mx my b">[START_REF]</code>，用于提问的<code class="fe mv mw mx my b">Question</code>，但是还有几个<a class="ae ky" href="https://github.com/paperswithcode/galai" rel="noopener ugc nofollow" target="_blank">更多的</a>，比如用于总结文本的<code class="fe mv mw mx my b">TLDR,</code>。</p><h1 id="1d91" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">摘要</h1><p id="9d56" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">尽管有一些限制，人们可以执行卡拉狄加——甚至在消费级计算机上。</p><p id="dd44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以重现一个演示提示，虽然引用查找对于文本生成来说似乎可以，但结果确实值得怀疑。</p><p id="49d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人工智能写作工具似乎给出了更合理的结果。我们无法测试据称最好的巨型模型——因为它超出了消费级硬件的能力范围。我认为，这样的硬件可能连一些研究实验室都负担不起。虽然我们没有测试卡拉狄加的全部可能性以及论文中所声称的，但我认为这一瞥给了我们一些可以期待的视角。</p><p id="349f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从根本上说，大型语言模型是否适合这类任务是有争议的。一种思路认为，大模型会变得太大，我们最终得到的是<a class="ae ky" href="https://dl.acm.org/doi/10.1145/3442188.3445922" rel="noopener ugc nofollow" target="_blank"> <em class="lv">随机鹦鹉</em> </a>而不是有所收获的模型。这里展示的结果肯定是有道理的。</p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="c5fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">我目前正在探索这种形式，以分享更多的数据科学思想，并从算法的角度讨论话题。你有什么改进的建议吗，或者你有一个我应该调查的话题吗？让我知道！</em></p></div></div>    
</body>
</html>