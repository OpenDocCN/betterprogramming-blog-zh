<html>
<head>
<title>Sound Classification on iOS Using Core ML 3 and Create ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Core ML 3和Create ML在iOS上进行声音分类</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/sound-classification-using-core-ml-3-and-create-ml-fc73ca20aff5?source=collection_archive---------7-----------------------#2019-11-12">https://betterprogramming.pub/sound-classification-using-core-ml-3-and-create-ml-fc73ca20aff5?source=collection_archive---------7-----------------------#2019-11-12</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="6eea" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">使用SoundAnalysis框架和使用Create ML构建的核心ML模型，通过现场音频流中的声音进行识别</h2></div><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/06989ecbc7fb6f642d4ad29d3046ebaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oPLA12jzwZEcDahh"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@kanereinholdtsen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯恩·莱因霍尔德森</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="0a51" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在WWDC 2019期间，Create ML <a class="ae kw" href="https://developer.apple.com/videos/play/wwdc2019/430/" rel="noopener ugc nofollow" target="_blank">获得了自己的独立身份</a>。这就像某种起源故事，让位于一系列全新的功能。Create ML现在是Xcode 11附带的一个单独的应用程序。</p><p id="0444" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">苹果再一次通过Create ML展示了他们对手机上的机器学习有多认真。他们最近在Create ML框架方面的进步对移动机器学习开发人员来说是一大福音，因为这些改进允许他们在没有太多机器学习专业知识的情况下轻松创建自己的模型。</p><p id="77ed" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">目前，Create ML支持以下类型的模型:</p><ul class=""><li id="9a35" class="lt lu ir kz b la lb ld le lg lv lk lw lo lx ls ly lz ma mb bi translated">图像分类器</li><li id="11fb" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">目标检测</li><li id="5627" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated"><strong class="kz is">声音分类</strong></li><li id="dbd7" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">文本分类和单词标签</li><li id="f904" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">活动分类器——用于预测行走、跳跃、摇摆等身体动作。来自运动传感器输入。</li><li id="449c" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">推荐系统</li></ul><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj mh"><img src="../Images/e499080fdb72efb69a132aadb5b21ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*rJjMM745X0UIZXlAvYc6Pw.png"/></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">快速浏览新的创建ML模板选择器向导</p></figure><p id="a33f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">Create ML具有易于使用的可视化界面，使得非数据科学家也可以快速构建自己的机器学习模型。</p><h2 id="e50d" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">行动（或活动、袭击）计划</h2><ul class=""><li id="3467" class="lt lu ir kz b la nb ld nc lg nd lk ne lo nf ls ly lz ma mb bi translated">收集音频文件的标记数据集</li><li id="137a" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">训练创建ML模型</li><li id="87b9" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">通过使用<code class="fe ng nh ni nj b">SoundAnalysis</code>和<code class="fe ng nh ni nj b">AVKit</code>框架使用核心ML 3模型的声音分类</li></ul><blockquote class="nk nl nm"><p id="60b0" class="kx ky nn kz b la lb js lc ld le jv lf no lh li lj np ll lm ln nq lp lq lr ls ik bi translated">要使用Create ML构建声音分类模型，需要Mac Catalina和Xcode 11。要部署生成的Core ML 3模型，需要iOS 13或以上的操作系统。</p></blockquote><h1 id="8724" class="nr mj ir bd mk ns nt nu mn nv nw nx mq jx ny jy mt ka nz kb mw kd oa ke mz ob bi translated">收集我们的数据集</h1><p id="e085" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">对于男女声音分类，我们从<a class="ae kw" href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/" rel="noopener ugc nofollow" target="_blank">vox celes</a>数据集中提取了一些音频(<code class="fe ng nh ni nj b">.wav</code>)文件，并将其与来自<a class="ae kw" href="http://wavsource.com/" rel="noopener ugc nofollow" target="_blank"> Wav源</a>的一些录音混合在一起。</p><p id="3c7a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">声音分类器模型在16 kHz音频下运行。因此，鼓励使用相同范围内的数据集，以确保模型的最佳性能。</p><p id="6835" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">由于Create ML的<code class="fe ng nh ni nj b">MLSoundClassifier</code>模型结构只使用第一个通道，而丢弃了其他通道，因此传递单通道音频文件用于训练是一个好主意。</p><p id="673a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">出于演示的目的，在本文中我们将使用一个小型数据集，而不太强调准确性。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj of"><img src="../Images/e9b6b810c3e4162c7e521955f2f7cf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*NfNi4qKK_KjInrvpHOopwg.png"/></div></figure><h1 id="9576" class="nr mj ir bd mk ns nt nu mn nv nw nx mq jx ny jy mt ka nz kb mw kd oa ke mz ob bi translated">用Create ML构建我们的模型</h1><p id="8950" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">接下来，我们将创建一个新的Create ML sound classifier项目，并在输入中添加training文件夹。您可以选择添加验证数据或将其设置为自动。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj og"><img src="../Images/36bef50f8532b01852398dcd845da176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRqdDqjz5JoNSLq3Vkalug.png"/></div></div></figure><p id="694e" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">Create ML自动识别培训文件夹中的课程和音频文件。对于分类器，至少需要两个类。</p><p id="46ee" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">现在只需按下<strong class="kz is"> <em class="nn"> Train </em> </strong>按钮，让Create ML进行特征提取、预处理、训练和验证。完成后，您可以拖放或者选择testing data文件夹来评估模型。测试数据集应该是模型以前没有见过的东西。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj oh"><img src="../Images/c43368bf9479ab6489906edfe9215997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LHq1aY_38e5y6rdHNfZdxw.gif"/></div></div></figure><p id="cafd" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">从收集数据集到构建核心ML 3模型，我花了大约30分钟，所以在这段时间内，上面的精度是不错的。</p><p id="749b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">我们的模型现在可以部署了。Create ML允许您使用Mac的内建麦克风来运行预测。</p><p id="1d61" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在下一节中，我们将在一个iOS应用程序中部署上面的核心ML 3模型，该应用程序可以从现场音频流中识别男性和女性的声音。</p></div><div class="ab cl oi oj hv ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ik il im in io"><h1 id="acb6" class="nr mj ir bd mk ns op nu mn nv oq nx mq jx or jy mt ka os kb mw kd ot ke mz ob bi translated">在iOS应用程序中部署我们的核心ML 3模型</h1><p id="531b" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">现在是时候在Xcode 11的iOS/macOS应用程序中使用之前构建的Core ML 3模型来执行基于语音的音频分类了。</p><h2 id="e3ba" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">方法</h2><ul class=""><li id="a600" class="lt lu ir kz b la nb ld nc lg nd lk ne lo nf ls ly lz ma mb bi translated">使用AudioEngine捕获实时音频流</li><li id="10eb" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">使用SoundAnalysis框架观察音频缓冲结果</li><li id="2be9" class="lt lu ir kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">运行<code class="fe ng nh ni nj b"><a class="ae kw" href="https://developer.apple.com/documentation/soundanalysis/snclassifysoundrequest" rel="noopener ugc nofollow" target="_blank">SNClassifySoundRequest</a></code>对观察到的音频流进行分类</li></ul></div><div class="ab cl oi oj hv ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ik il im in io"><h2 id="178c" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">入门指南</h2><p id="258d" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">首先，让我们将核心的ML 3模型拖放到一个新的Xcode 11项目中:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj ou"><img src="../Images/d036f149c825fd7fb4658de23e7a615f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tg5ivpH0TZsdzH0AajA1Vw.png"/></div></div></figure><h2 id="bc4d" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">隐私使用说明</h2><p id="4f8b" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">我们需要在<code class="fe ng nh ni nj b">info.plist</code>文件中添加麦克风的隐私使用描述，以便收听音频流。下图展示了如何做到这一点:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj ov"><img src="../Images/a43bcd37e30b0662c86cb9146338c261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmvTcQH9etJbp_a-HfLuHQ.png"/></div></div></figure></div><div class="ab cl oi oj hv ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ik il im in io"><h2 id="fee4" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">初始化</h2><p id="a290" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">我们需要导入SoundAnalysis和AVKit框架，并初始化模型和某些属性，如下所示:</p><pre class="kh ki kj kk gu ow nj ox oy aw oz bi"><span id="cf0d" class="mi mj ir nj b gz pa pb l pc pd">import AVKit<br/>import SoundAnalysis</span><span id="d858" class="mi mj ir nj b gz pe pb l pc pd">private let audioEngine = AVAudioEngine()<br/>private var soundClassifier = GenderSoundClassification()<br/>var inputFormat: AVAudioFormat!<br/>var analyzer: SNAudioStreamAnalyzer!<br/>var resultsObserver = ResultsObserver()<br/>let analysisQueue = DispatchQueue(label: "com.apple.AnalysisQueue")</span></pre><p id="5d9a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">如果上面代码中的一些初始化没有意义，不要担心——我们将在下面的小节中讨论它们。</p></div><div class="ab cl oi oj hv ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ik il im in io"><h2 id="24e1" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">开始音频采集</h2><p id="b0cb" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">我们需要使用<code class="fe ng nh ni nj b">AudioEngine</code>框架，它是<code class="fe ng nh ni nj b">AVFoundation</code>的一部分，开始从麦克风捕获音频流。</p><p id="b90f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">通过从<code class="fe ng nh ni nj b">audioEngine</code>访问<code class="fe ng nh ni nj b">inputNode</code>，我们可以在总线上安装一个音频抽头来观察节点的音频流输出。</p><pre class="kh ki kj kk gu ow nj ox oy aw oz bi"><span id="c045" class="mi mj ir nj b gz pa pb l pc pd">private func startAudioEngine() {</span><span id="173a" class="mi mj ir nj b gz pe pb l pc pd">//create stream analyzer request with the Sound Classifier</span><span id="d014" class="mi mj ir nj b gz pe pb l pc pd">do{<br/>try audioEngine.start()<br/>}<br/>catch( _){<br/>print("error in starting the Audio Engine")<br/>}</span><span id="3183" class="mi mj ir nj b gz pe pb l pc pd">}</span></pre><p id="ab5d" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在上面的代码中，我们将在输入节点总线上添加声音分类请求和声音分析器。</p><h2 id="36ee" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">创建声音流分析器</h2><p id="0c67" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">接下来，我们需要使用SoundAnalysis框架创建一个音频流分析器。它以本机格式捕获音频引擎的流，如下所示:</p><pre class="kh ki kj kk gu ow nj ox oy aw oz bi"><span id="1f9f" class="mi mj ir nj b gz pa pb l pc pd">inputFormat = audioEngine.inputNode.inputFormat(forBus: 0)<br/>analyzer = SNAudioStreamAnalyzer(format: inputFormat)</span></pre><h2 id="d863" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">创建声音分类器请求</h2><p id="bb4f" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">现在，我们需要通过在<code class="fe ng nh ni nj b">SNClassifySoundRequest</code>中传递核心ML <code class="fe ng nh ni nj b">model</code>实例来创建一个声音分类器请求。</p><p id="e854" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在您的<code class="fe ng nh ni nj b">startAudioEngine</code>函数的开头添加以下代码:</p><pre class="kh ki kj kk gu ow nj ox oy aw oz bi"><span id="b028" class="mi mj ir nj b gz pa pb l pc pd">do {<br/>let request = try SNClassifySoundRequest(mlModel: soundClassifier.model)<br/>try analyzer.add(request, withObserver: resultsObserver)<br/>} catch {<br/>print("Unable to prepare request: \(error.localizedDescription)")<br/>return<br/>}<br/>}</span></pre><p id="32f8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在上面的代码中，我们已经向<code class="fe ng nh ni nj b">SNAudioStreamAnalyzer</code>添加了声音分类请求实例。分类器最终将结果返回给观察者对象<code class="fe ng nh ni nj b">resultsObserver</code>。</p><p id="9c32" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">该类的实例实现了<code class="fe ng nh ni nj b">SNResultsObserving</code>协议方法，该方法为每个声音分类结果触发，如下所示:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="pf pg l"/></div></figure><p id="b662" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe ng nh ni nj b">GenderClassifierDelegate</code>是一个自定义协议，用于在UILabel中显示最终预测:</p><pre class="kh ki kj kk gu ow nj ox oy aw oz bi"><span id="191d" class="mi mj ir nj b gz pa pb l pc pd">protocol GenderClassifierDelegate {<br/>    func displayPredictionResult(identifier: String, confidence: Double)<br/>}</span><span id="f58f" class="mi mj ir nj b gz pe pb l pc pd">extension ViewController: GenderClassifierDelegate {<br/>    func displayPredictionResult(identifier: String, confidence: Double) {<br/>        DispatchQueue.main.async {<br/>            self.transcribedText.text = ("Recognition: \(identifier)\nConfidence \(confidence)")<br/>        }<br/>    }<br/>}</span></pre><h2 id="5a2d" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">分析音频流</h2><p id="a6ba" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">最后，我们可以通过设置一个单独的串行调度队列来分析音频流，该队列分析来自<code class="fe ng nh ni nj b">inputNode</code>的音频缓冲区，如下面的代码所示:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="pf pg l"/></div></figure><h2 id="8749" class="mi mj ir bd mk ml mm dn mn mo mp dp mq lg mr ms mt lk mu mv mw lo mx my mz na bi translated">运行音频引擎并进行预测</h2><p id="a151" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">最后，我们可以运行我们的音频捕获方法，并对音频缓冲流进行分析和分类，以确定性别类别标签。</p><p id="0226" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">只需在您的<code class="fe ng nh ni nj b">viewDidLoad</code>和<code class="fe ng nh ni nj b">viewDidAppear</code>方法中添加以下代码片段:</p><pre class="kh ki kj kk gu ow nj ox oy aw oz bi"><span id="ed62" class="mi mj ir nj b gz pa pb l pc pd">override func viewDidLoad() {<br/>        super.viewDidLoad()<br/>        <br/>        resultsObserver.delegate = self<br/>        inputFormat = audioEngine.inputNode.inputFormat(forBus: 0)<br/>        analyzer = SNAudioStreamAnalyzer(format: inputFormat)<br/>        buildUI()<br/>}<br/>override func viewDidAppear(_ animated: Bool) {<br/>        startAudioEngine()<br/>}</span></pre><p id="647f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">为了测试这个应用程序，我使用了两个视频，一个是WWDC的(男性声音)，另一个是youtube的(识别女性声音)。这是我们得到的结果:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ph pg l"/></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">我们从用于声音分类的小数据集得到的结果</p></figure><p id="3ae0" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">基于我们模型中100多个文件的小数据集，上述结果是相当准确的。默认情况下，我们的声音分类模型为各种声音返回“男性”。您可以尝试为不同类别(环境、机器人、鸟类)添加更多声音类别标签，并构建自己的数据集。</p><p id="c14e" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">Xcode项目的完整源代码以及包含几个<code class="fe ng nh ni nj b">wav</code>文件的数据集可以在这个<a class="ae kw" href="https://github.com/anupamchugh/iowncode/tree/master/CoreML3SoundClassifier" rel="noopener ugc nofollow" target="_blank"> GitHub资源库</a>中找到。</p><h1 id="9fb4" class="nr mj ir bd mk ns nt nu mn nv nw nx mq jx ny jy mt ka nz kb mw kd oa ke mz ob bi translated">结论</h1><p id="cb88" class="pw-post-body-paragraph kx ky ir kz b la nb js lc ld nc jv lf lg oc li lj lk od lm ln lo oe lq lr ls ik bi translated">通过使用新的<code class="fe ng nh ni nj b">MLSoundClassifier</code>模型和<code class="fe ng nh ni nj b">SoundAnalysis</code>框架，我们很快就能够从现场音频流中对语音进行分类，即使只有很小的数据集。</p><p id="0842" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">Create ML允许移动机器学习开发者快速训练和测试他们自己的模型，而无需过于深入地挖掘机器学习。</p><p id="1683" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这就结束了。我希望你喜欢阅读。</p></div></div>    
</body>
</html>