<html>
<head>
<title>How Machines Can Learn Using TensorFlow or PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器如何使用TensorFlow或PyTorch进行学习</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-machines-can-learn-using-tensorflow-or-pytorch-8f85cd04979d?source=collection_archive---------10-----------------------#2022-05-25">https://betterprogramming.pub/how-machines-can-learn-using-tensorflow-or-pytorch-8f85cd04979d?source=collection_archive---------10-----------------------#2022-05-25</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="a66a" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">深入了解他们的思想，他们的进化</h2></div><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/eeb14dedf677b40a549a89d0bb793471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7v6s2Nni8KOjgzeK_gzsA.jpeg"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">来源:https://unsplash.com/</p></figure><p id="169a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">人工智能和机器学习是最近非常热门的话题。自动驾驶汽车、实时自动翻译、语音识别等。这些只是没有机器学习就无法存在的应用中的一部分。但是机器如何学习呢？在本文中，我将向您展示这种魔力是如何发挥作用的，但我不会谈论神经网络！我将向你展示机器学习的最深处是什么。</p><p id="ad84" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">关于机器学习最好的演讲之一是费李非的TED演讲。</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="lt lu l"/></div></figure><p id="23db" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">正如李在她的演讲中所说，在早期，程序员试图通过算法来解决计算机视觉任务。这些算法寻找形状，并试图通过预编程的规则来识别事物。但是他们不能用这种方法解决。计算机视觉是一个非常复杂的问题，我们无法用算法来解决。所以，机器学习就出现了。</p><p id="b7f4" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><em class="lv">如果我们不能写程序，让我们写一个代替我们写程序的程序。</em></p><p id="4a9e" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">机器学习算法就是这么做的。如果我们有足够的数据，他们可以为我们编写从输入计算预期输出的算法。在图像识别的情况下，输入是图像，输出是图像内容的标签或描述。</p><p id="d024" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这就是为什么李和她的团队非常努力地建立ImageNet，这是世界上最大的标记图像数据库，拥有1500万张图像和22，000个类别。</p><p id="ed54" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">多亏了ImageNet，我们有了足够的数据，但是程序员是怎么用它来解决图像识别的问题的呢？这就是我应该像李一样谈论神经网络的地方，但我不会。神经网络确实受到了生物大脑的启发，但今天的变形金刚与生物模型相去甚远。</p><p id="06ee" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这就是为什么我认为“神经网络”这个名字会产生误导。但是什么名字会更好呢？</p><p id="facb" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">我最喜欢的是Karpathy的(AI @ Tesla) <a class="ae kw" href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="noopener">软件2.0 </a>。</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="lw lu l"/></div></figure><p id="f7f4" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">正如Karpathy在这次演讲中所说，软件1.0是经典的软件，程序员编写代码，但在软件2.0的情况下，另一个软件基于大数据找到它。但是一个软件怎么能写出另一个软件呢？让我引用卡帕西的话:</p><blockquote class="lx"><p id="43f8" class="ly lz ir bd ma mb mc md me mf mg ls dk translated">“梯度下降能比你写的代码更好。对不起。”</p></blockquote><p id="7347" class="pw-post-body-paragraph kx ky ir kz b la mh js lc ld mi jv lf lg mj li lj lk mk lm ln lo ml lq lr ls ik bi translated"><a class="ae kw" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>是大多数机器学习系统使用的魔法名称。为了理解它，让我们想象一个机器学习系统。这是一个有输入和输出的黑盒子。如果系统的目的是图像识别，那么输入是像素阵列，输出是概率分布向量。</p><p id="33d2" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">如果系统能识别猫和狗，输入是一只猫的图像，输出会像10%狗，90%猫。所以，输入是数字，输出也是数字，黑盒的内容是一个巨大的数学表达式。黑盒从像素数据计算输出向量。</p><p id="0ff7" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">我答应讲程序写程序，现在讲数学表达。但实际上，程序是数学表达式。CPU是由逻辑门构成的。他们使用数字的二进制表示和逻辑运算符。</p><p id="62b5" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">任何现有的算法都可以通过这些逻辑表达式来实现，所以一个长的逻辑表达式可以表示任何可以在经典计算机上运行的程序。如你所见，经典程序也只不过是从二进制输入到二进制输出的二进制计算。</p><p id="98df" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">机器学习系统使用实数和运算符，而不是二进制数和运算符，但基本上，这些数学表达式也是“程序”。</p><blockquote class="mm mn mo"><p id="12f7" class="kx ky lv kz b la lb js lc ld le jv lf mp lh li lj mq ll lm ln mr lp lq lr ls ik bi translated">艾伦·图灵在1948年写了一篇关于“B型无组织机器”的论文。这些机器由互连的逻辑与非门构成，可以通过启用/禁用节点之间的连线来训练。在二进制代数中，NAND是一个通用运算符，因为其他所有运算符都可以用它来表示。这些B型无组织机器是通用计算机，因为每种算法都可以在其上实现。</p><p id="bf2a" class="kx ky lv kz b la lb js lc ld le jv lf mp lh li lj mq ll lm ln mr lp lq lr ls ik bi translated">这些B型机器类似于现在的神经网络，但它们是在逻辑门上实现的，就像今天的CPU一样，所以由这些B型机器实现的算法更像我们今天的算法。</p><p id="ee82" class="kx ky lv kz b la lb js lc ld le jv lf mp lh li lj mq ll lm ln mr lp lq lr ls ik bi translated">不幸的是，图灵从未发表过这篇论文，这就是为什么我们不知道他是早期神经网络的发明者。这些B型网络的问题是它们不能被有效地训练。</p></blockquote><p id="05ac" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">如果我们有一组输入和输出以及一个参数化的表达式，我们如何找到正确的参数，以最小的误差从输入计算输出？它有点像一个有许多电位计的黑盒子。电位计位置的每个组合都是一个程序，我们正在寻找正确的位置。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/c9d056357609a0c255793a123678868d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGWOooPm0v9ZT_Wr6u4Sqw.jpeg"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">来源:https://unsplash.com/<a class="ae kw" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="9112" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">为了解决这个问题，我们来想象一下误差函数。它看起来像一幅有山丘和山谷的风景。表达式的每一个参数都是景观的一个维度，当前点的高度就是给定参数的误差。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj ms"><img src="../Images/5adbdf1c129a06730b14c94b1eed0b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H2kAUwqp2VV_WsNl.png"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">来源:https://en.wikipedia.org/wiki/Gradient_descent<a class="ae kw" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="e41d" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">当我们用随机数初始化这个表达式时，我们就处在了一个随机的位置。为了最小化误差，我们必须降低到最低点(代表最低误差)。问题是我们完全看不见。我们怎样才能从山上下来？</p><p id="78fe" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">我们可以四处摸索，找到最陡的斜坡，然后去那里。但是我们如何确定一个函数的斜率呢？图中渐变来了。梯度表示函数在给定点的斜率有多陡。这就是为什么我们称这种方法为“梯度下降”</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj mt"><img src="../Images/f202dd918503bdaea61d01faf7ed21bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*1X2naEd5RM3kZj0I.png"/></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">来源:<a class="ae kw" href="https://www.youtube.com/watch?v=i1gGsE66b5s&amp;t=3138s" rel="noopener ugc nofollow" target="_blank"> <em class="mu">安德烈·卡帕西斯坦福课程讲座3 </em> </a></p></figure><p id="dfed" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">给定点的梯度可以通过偏导数来计算。一个函数必须满足某些条件才能求导。如果我们想使用梯度下降进行优化，我们必须使用这些类型的函数。如果函数是可导的，那么函数链也将是可导的，有一种算法方法可以计算梯度。</p><p id="84ac" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">现在我们有了所有的知识来理解<a class="ae kw" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和<a class="ae kw" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>(两个最流行的机器学习框架)如何为我们的表达式找到正确的参数。</p><p id="e431" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">首先，我们来看TensorFlow。</p><p id="4417" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在Tensorflow中，有一个梯度注册表，通过<code class="fe mv mw mx my b"><a class="ae kw" href="https://www.tensorflow.org/api_docs/python/tf/RegisterGradient" rel="noopener ugc nofollow" target="_blank">RegisterGradient</a></code>方法为操作符注册梯度函数。在学习阶段，每一步都要启动一个<code class="fe mv mw mx my b"><a class="ae kw" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank">GradientTape</a></code>。<code class="fe mv mw mx my b">GradientTape</code>是一种类似录像机的东西。TensorFlow做任何操作的时候，梯度带都会记录。</p><p id="d452" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在正向阶段结束后(当从输入生成输出时)，我们通过使用误差和使用注册的梯度函数计算梯度来停止在测井曲线上反向的梯度带。我们可以通过使用梯度来修改参数并重复该过程，直到我们达到最小误差。</p><p id="33e7" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">让我们看看Python中的代码:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="mz lu l"/></div></figure><p id="e99c" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这段代码展示了我们如何在TensorFlow中使用梯度下降来解决最简单的问题(线性回归)。线性回归的目的是找到最接近每个点的线的参数，其中最接近意味着距离的平方和最小。</p><p id="691a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">张量流使用张量运算进行计算。张量是矩阵的推广。从程序员的角度来看，张量是简单的数组。零维张量是标量，一维张量是向量，二维张量是矩阵，从第三维度看，张量就是单纯的张量。张量运算可以并行进行，以便高效运行，尤其是在GPU或TPU上。</p><p id="03d8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在代码的开始，我们定义了我们的模型，这是一个线性表达式。它有两个标量参数，<code class="fe mv mw mx my b">W</code>和<code class="fe mv mw mx my b">b</code>，表达式为<code class="fe mv mw mx my b">y=W*x+b</code>。<code class="fe mv mw mx my b">W</code>的默认值为<code class="fe mv mw mx my b">16</code>，<code class="fe mv mw mx my b"> b</code>为<code class="fe mv mw mx my b">10</code>。这是在我们的黑盒中，我们的代码将改变<code class="fe mv mw mx my b">W</code>和<code class="fe mv mw mx my b">b</code>以最小化错误。真实世界的模型有数百万或数十亿个参数，但这两个参数足以理解该方法。</p><p id="9461" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">从第21行到第23行，我们定义了随机点集。<code class="fe mv mw mx my b">tf.random.normal</code>方法用1000个正态分布的随机数生成一个向量，我们用它来生成直线附近的点。</p><p id="aaf0" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">第34行定义了损失函数。<code class="fe mv mw mx my b">y</code>和<code class="fe mv mw mx my b">y_pred</code>参数是矢量。<code class="fe mv mw mx my b">y_pred</code>是我们模型的实际产量，<code class="fe mv mw mx my b">y</code>是预期产量。square函数计算每个矢量元素的平方，输出也是一个带有平方的矢量。<code class="fe mv mw mx my b">reduce_mean</code>函数计算元素的平均值，其结果是一个标量。这是我们想要最小化的误差本身。</p><p id="4bed" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">梯度下降是从线36到线42。这是学习发生的代码的本质。第37行的<code class="fe mv mw mx my b">with</code>是一个Python表达式。它在块的开头调用参数对象的<code class="fe mv mw mx my b">__enter__</code>方法，在块的结尾调用<code class="fe mv mw mx my b">__exit__</code>方法。</p><p id="b5d5" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在<code class="fe mv mw mx my b">GradientTape</code>的情况下，<code class="fe mv mw mx my b"> __enter__</code>方法开始记录，<code class="fe mv mw mx my b">__exit__ </code>停止记录。在程序块(第38行)中，我们通过<code class="fe mv mw mx my b">model(X)</code>计算模型输出和误差。在第40行，<code class="fe mv mw mx my b">GradientTape</code>计算参数的梯度(<code class="fe mv mw mx my b">dW</code>和<code class="fe mv mw mx my b">db</code>)，在第41和42行，修改参数。</p><p id="e62b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">有不同的优化策略。我们使用最简单的方法，其中梯度乘以一个固定的学习速率(<code class="fe mv mw mx my b">lr</code>)。</p><p id="cc30" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">简单来说，梯度下降和TensorFlow的<code class="fe mv mw mx my b">GradientTape</code>就是这样工作的。在TensorFlow的网页上可以找到很多<a class="ae kw" href="https://www.tensorflow.org/tutorials" rel="noopener ugc nofollow" target="_blank">教程。用于图像识别、强化学习等的神经网络。但是记住，总有张量运算和梯度带。</a></p><p id="e2c0" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">现在，让我们看看梯度下降在另一个大框架PyTorch中是如何工作的。</p><p id="c386" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">PyTorch使用嵌入火炬张量的<a class="ae kw" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" rel="noopener ugc nofollow" target="_blank">自动签名</a>系统进行梯度计算。如果张量是算子的结果，那么它包含一个指向算子和源张量的反向指针。源张量还包含反向指针等。，全运营商链条可追溯。</p><p id="e50d" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">每个运算符都可以计算自己的梯度。当您在最后一个张量上调用backward方法时，它会回到链上并计算张量的梯度。</p><p id="6cb5" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">让我们看看PyTorch中前面的线性回归代码:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="mz lu l"/></div></figure><p id="11a7" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">模型和损耗部分与TensorFlow非常相似。您会发现第37行和第46行的<code class="fe mv mw mx my b">train</code>方法的不同之处。在计算完<code class="fe mv mw mx my b">current_loss</code>张量后，我们对它调用倒推法。它递归地回到链上，计算<code class="fe mv mw mx my b">W</code>和<code class="fe mv mw mx my b">b</code>张量的梯度。</p><p id="54c3" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">从第41行到第43行，我们修改了<code class="fe mv mw mx my b">W</code>和<code class="fe mv mw mx my b">b</code>张量。重要的是，该计算在<code class="fe mv mw mx my b">torch.no_grad()</code>块中进行。<code class="fe mv mw mx my b">no_grad()</code>方法暂时禁用运算符的梯度计算，当我们修改参数时不需要。</p><p id="9c97" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在<code class="fe mv mw mx my b">train</code>方法结束时，调用zero方法清除渐变。如果没有这一点，PyTorch将累加梯度，这将导致奇怪的行为。代码的其他部分与TensorFlow非常相似。像TensorFlow一样，PyTorch也有很好的教程、社区和文档。</p><p id="a505" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">你会发现构建任何神经网络的一切，但最重要的部分是亲笔签名的系统，这是训练的基础。</p><p id="fdc0" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">如果你想了解自动签名系统在内部是如何工作的，可以看看Karpathy关于反向传播的视频。在这个视频中，Karpathy用Python从零开始构建了一个亲笔签名的系统。高中数学就够看懂了，如果不是数学大朋友也不是问题。</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="lw lu l"/></div><p class="ks kt gk gi gj ku kv bd b be z dk translated"><a class="ae kw" href="https://www.youtube.com/watch?v=VMj-3S1tku0" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=VMj-3S1tku0</a></p></figure><p id="1096" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">下一次，当你看到DALL-E生成的一张图片，一辆自动驾驶的汽车，或者只是想知道Google Assistant如何理解你说的话，你就会知道魔法是如何工作的，以及一种算法(梯度下降)是如何为我们编写这些很酷的算法的。</p></div></div>    
</body>
</html>