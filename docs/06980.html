<html>
<head>
<title>How to Turn the Web Into Data With Python and Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Python和Scrapy把Web变成数据</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-turn-the-web-into-data-with-python-and-scrapy-7bad725cf5a?source=collection_archive---------10-----------------------#2020-11-23">https://betterprogramming.pub/how-to-turn-the-web-into-data-with-python-and-scrapy-7bad725cf5a?source=collection_archive---------10-----------------------#2020-11-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fa88" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python和Scrapy支持的网络抓取指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/60a5aaf070abecc9843b5343182e13a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5at3Ce1CX2q9U78DPeF7A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ba19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本教程将是你学习使用Python进行web抓取的终极指南。首先，我会带你看一些基本的例子，让你熟悉网页抓取。稍后，我们将使用这些知识从<a class="ae lu" href="https://www.livescore.cz/" rel="noopener ugc nofollow" target="_blank"> Livescore </a>中提取足球比赛数据。</p><p id="05e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事不宜迟，我们开始吧</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4315" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">入门指南</h1><p id="7694" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了让我们开始，您需要启动一个新的Python3项目并安装<a class="ae lu" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank">Scrapy</a>(Python的一个网络抓取和网络爬行库)。我在本教程中使用Pipenv，但是您也可以使用pip和venv——或者conda。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="339d" class="ne md it na b gy nf ng l nh ni">pipenv install scrapy</span></pre><p id="776c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这一点上，你有Scrapy，但你仍然需要创建一个新的网络抓取项目，为此，Scrapy为我们提供了一个命令行，为我们做工作。</p><p id="c562" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们使用Scrapy CLI创建一个名为<code class="fe nj nk nl na b">web_scraper</code>的新项目。</p><p id="e3cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您像我一样使用Pipenv，请使用:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="937a" class="ne md it na b gy nf ng l nh ni">pipenv run scrapy startproject web_scraper .</span></pre><p id="9f8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">否则，使用以下命令从虚拟环境启动:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="39e3" class="ne md it na b gy nf ng l nh ni">scrapy startproject web_scraper .</span></pre><p id="dff6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将在当前目录中创建一个具有以下结构的基本项目:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="4866" class="ne md it na b gy nf ng l nh ni">scrapy.cfg<br/>web_scraper/<br/>    __init__.py<br/>    items.py<br/>    middlewares.py<br/>    pipelines.py<br/>    settings.py<br/>    spiders/<br/>        __init__.py</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4b40" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">用XPath查询构建我们的第一个蜘蛛</h1><p id="c4bc" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们将从一个非常简单的例子开始我们的web抓取教程。首先，我们将在网站的HTML中定位<a class="ae lu" href="https://livecodestream.dev/" rel="noopener ugc nofollow" target="_blank">实时代码流</a>标志。正如我们所知，它只是一个文本而不是图像，所以我们将简单地提取这个文本。</p><h2 id="bfe6" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">代码</h2><p id="9ebf" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">首先，我们需要为这个项目创建一个新的蜘蛛。我们可以通过创建新文件或使用CLI来实现。</p><p id="a309" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们已经知道我们需要的代码，我们将在这个路径<code class="fe nj nk nl na b">/web_scraper/spiders/live_code_stream.py</code>创建一个新的Python文件。</p><p id="d45a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是该文件的内容:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f2a7" class="ne md it na b gy nf ng l nh ni">import scrapy</span><span id="325b" class="ne md it na b gy nx ng l nh ni">class LiveCodeStreamSpider(scrapy.Spider):<br/>    name = "lcs"</span><span id="c8bd" class="ne md it na b gy nx ng l nh ni">    start_urls = [<br/>        "https://livecodestream.dev/"<br/>    ]</span><span id="8618" class="ne md it na b gy nx ng l nh ni">    def parse(self, response):<br/>        yield {<br/>            'logo': response.xpath("/html/body/header/nav/a[1]/text()").get()<br/>        }</span></pre><h2 id="555a" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">代码解释</h2><ol class=""><li id="52de" class="ny nz it la b lb mu le mv lh oa ll ob lp oc lt od oe of og bi translated">首先，我们导入了Scrapy库。我们需要它的功能来创建一个Python网络蜘蛛。然后这个蜘蛛将被用来抓取指定的网站，并从中提取有用的信息。</li><li id="a36c" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">我们创建了一个类，并命名为<code class="fe nj nk nl na b">LiveCodeStreamSpider</code>。基本上，它继承自<code class="fe nj nk nl na b">scrapy.Spider</code>——这就是我们将它作为参数传递的原因。</li><li id="4cc9" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">现在，重要的一步是使用一个名为<code class="fe nj nk nl na b">name</code>的变量为你的蜘蛛定义一个唯一的名字。请记住，您不允许使用现有蜘蛛的名称。同样，你也不能用这个名字去创造新的蜘蛛。它必须在整个项目中是唯一的。</li><li id="7d88" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">之后，我们使用<code class="fe nj nk nl na b">start_urls</code>列表传递网站URL。</li><li id="7d87" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">最后，我们创建了一个名为<code class="fe nj nk nl na b">parse()</code>的方法，它将在HTML代码中定位徽标并提取其文本。在Scrapy中，有两种方法可以找到源代码中的HTML元素:CSS和XPath。</li></ol><p id="097c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您甚至可以使用一些外部库，如<a class="ae lu" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>和<a class="ae lu" href="https://lxml.de/" rel="noopener ugc nofollow" target="_blank"> lxml </a>，但是对于这个例子，我们使用了XPath。</p><p id="e382" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">确定任何HTML元素的XPath的一个快速方法是在Chrome DevTools中打开它。现在，只需右键单击该元素的HTML代码，并将鼠标光标悬停在弹出菜单中的“Copy”上。最后，单击“复制XPath”菜单项。</p><p id="cff0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看下面的截图可以更好地理解它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/eb8b756331a9823010e5d933c3eabff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q4ylu2JjugBImAlw.jpg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Chrome DevTools查找XPath</p></figure><p id="6085" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">顺便说一下，我在元素的实际XPath后面使用了<code class="fe nj nk nl na b">/text()</code>,只从该元素中检索文本，而不是完整的元素代码。</p><p id="80ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">注意:</strong>不允许对上述变量、列表或函数使用任何其他名称。这些名称是在Scrapy库中预先定义的。所以你必须照原样使用它们。否则，程序将无法正常运行。</p><h2 id="c167" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">跑蜘蛛</h2><p id="5615" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">由于我们已经在命令提示符下的<code class="fe nj nk nl na b">web_scraper</code>文件夹中，让我们执行我们的蜘蛛，并使用下面的代码将结果填充到一个新文件<code class="fe nj nk nl na b">lcs.json</code>中。是的，使用JSON格式，我们得到的结果将是结构良好的。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="800e" class="ne md it na b gy nf ng l nh ni">pipenv run scrapy crawl lcs -o lcs.json<!-- --> </span></pre><p id="9bac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="de12" class="ne md it na b gy nf ng l nh ni">scrapy crawl lcs -o lcs.json</span></pre><h2 id="fb12" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">结果</h2><p id="9355" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">当上面的代码执行时，我们会在项目文件夹中看到一个新文件<code class="fe nj nk nl na b">lcs.json</code>。</p><p id="1a4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是该文件的内容:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7a04" class="ne md it na b gy nf ng l nh ni">[<br/>{"logo": "Live Code Stream"}<br/>]</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="555c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">另一个带有CSS查询选择器的蜘蛛</h1><p id="aac4" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们大多数人都热爱运动，说到足球，这是我个人的最爱。</p><p id="ebed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">世界各地经常组织足球比赛。有几个网站在比赛进行的时候提供比赛结果的直播。但这些网站大多不提供官方API。</p><p id="d9e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">反过来，它为我们创造了一个机会，通过直接抓取他们的网站，使用我们的网络抓取技能来提取有意义的信息。</p><p id="e128" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">比如我们来看看<a class="ae lu" href="https://www.livescore.cz/" rel="noopener ugc nofollow" target="_blank"> Livescore </a>网站。</p><p id="444c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在他们的主页上，他们很好地展示了今天(你访问网站的日期)将要进行的锦标赛和比赛。</p><p id="4c3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以检索如下信息:</p><ul class=""><li id="efac" class="ny nz it la b lb lc le lf lh on ll oo lp op lt oq oe of og bi translated">锦标赛名称</li><li id="aba1" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt oq oe of og bi translated">比赛时间</li><li id="d4ae" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt oq oe of og bi translated">团队1名称(例如，国家、足球俱乐部等。)</li><li id="ca76" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt oq oe of og bi translated">团队1目标</li><li id="7c21" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt oq oe of og bi translated">团队2名称(例如，国家、足球俱乐部等。)</li><li id="9469" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt oq oe of og bi translated">团队2目标</li><li id="fece" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt oq oe of og bi translated">等等。</li></ul><p id="354a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的代码示例中，我们将提取今天匹配的锦标赛名称。</p><h2 id="1fe6" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">代码</h2><p id="6c4d" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">让我们在项目中创建一个新的蜘蛛来检索锦标赛名称。我将这个文件命名为<code class="fe nj nk nl na b">livescore_t.py</code>。</p><p id="f1d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是您需要在<code class="fe nj nk nl na b">/web_scraper/web_scraper/spiders/livescore_t.py</code>中输入的代码:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="65f8" class="ne md it na b gy nf ng l nh ni">import scrapy</span><span id="afb7" class="ne md it na b gy nx ng l nh ni">class LiveScoreT(scrapy.Spider):<br/>    name = "LiveScoreT"</span><span id="f9e5" class="ne md it na b gy nx ng l nh ni">    start_urls = [<br/>        "https://livescore.cz/"<br/>    ]</span><span id="f92e" class="ne md it na b gy nx ng l nh ni">    def parse(self, response):<br/>        for ls in response.css('#soccer_livescore .tournament'):<br/>            yield {<br/>                'tournament': ls.css('.nation a::text').get()<br/>            }</span></pre><h2 id="a1bf" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">代码解释</h2><ol class=""><li id="d8a5" class="ny nz it la b lb mu le mv lh oa ll ob lp oc lt od oe of og bi translated">像往常一样，进口刺痒。</li><li id="0d1d" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">创建一个继承<code class="fe nj nk nl na b">scrapy.Spider</code>属性和功能的类。</li><li id="ea9d" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">给我们的蜘蛛起一个独特的名字。在这里，我使用了<code class="fe nj nk nl na b">LiveScoreT</code>，因为我们将只提取锦标赛名称。</li><li id="eae4" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">下一步是提供<code class="fe nj nk nl na b">livescore.cz</code>的网址。</li><li id="08ec" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">最后，运行<code class="fe nj nk nl na b">parse()</code>函数循环遍历所有包含锦标赛名称的匹配元素，并使用<code class="fe nj nk nl na b">yield</code>将其连接在一起。最后，我们将收到今天所有匹配的锦标赛名称。</li><li id="f4e9" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">需要注意的一点是，这次我使用了CSS选择器，而不是XPath。</li></ol><h2 id="8e62" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">运行新创建的蜘蛛</h2><p id="42a0" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">是时候看看我们的蜘蛛在行动了。运行下面的命令让蜘蛛抓取Livescore网站的主页。web抓取的结果将被添加到一个名为<code class="fe nj nk nl na b">ls_t.json</code>的JSON格式的新文件中。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6d14" class="ne md it na b gy nf ng l nh ni">pipenv run scrapy crawl LiveScoreT -o ls_t.json</span></pre><p id="e7f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，你知道该怎么做了。</p><h2 id="d38c" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">结果</h2><p id="2de1" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">这是我们的网络蜘蛛在11月18日从<code class="fe nj nk nl na b"><a class="ae lu" href="https://www.livescore.cz/" rel="noopener ugc nofollow" target="_blank">livescore.cz</a></code>中提取的。请记住，输出可能每天都在变化。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f6a3" class="ne md it na b gy nf ng l nh ni">[<br/>{"tournament": "International - World Cup Qualification CONMEBOL"},<br/>{"tournament": "Brazil - Serie A"},<br/>{"tournament": "International - UEFA Nations League A Grp. 3"},<br/>{"tournament": "International - UEFA Nations League A Grp. 4"},<br/>{"tournament": "International - UEFA Nations League C Grp. 1"},<br/>{"tournament": "International - UEFA Nations League D Grp. 1"},<br/>{"tournament": "International - UEFA Nations League D Grp. 2"},<br/>{"tournament": "..."}<br/>]</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4d58" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">更高级的用例</h1><p id="4a09" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在这一节中，我们不仅要检索锦标赛名称，还要进一步获取锦标赛及其比赛的完整详细信息。</p><p id="8571" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe nj nk nl na b">/web_scraper/web_scraper/spiders/</code>里面新建一个文件，命名为<code class="fe nj nk nl na b">livescore.py</code>。现在，输入下面的代码。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="5fe3" class="ne md it na b gy nf ng l nh ni">import scrapy</span><span id="71ce" class="ne md it na b gy nx ng l nh ni">class LiveScore(scrapy.Spider):<br/>    name = "LiveScore"</span><span id="15a0" class="ne md it na b gy nx ng l nh ni">    start_urls = [<br/>        "https://www.livescore.cz/yesterday.php"<br/>    ]</span><span id="4d1d" class="ne md it na b gy nx ng l nh ni">    def parse(self, response):<br/>        table_tr = response.css('tr')<br/>        <br/>        tournaments = []</span><span id="fd16" class="ne md it na b gy nx ng l nh ni">        for tr in table_tr:<br/>          if tr.css('.tournament'):<br/>            tournaments.append({<br/>                    'name': tr.css('.nation a::text').get(),<br/>                    'matches': []<br/>                })<br/>          elif tr.css('.match'):<br/>            team_score = tr.css('.col-score strong::text').get()</span><span id="b0e4" class="ne md it na b gy nx ng l nh ni">            if team_score is not None:<br/>                team_1_score = team_score.split(':')[0]<br/>                team_2_score = team_score.split(':')[1]<br/>            else:<br/>                team_1_score = None<br/>                team_2_score = None</span><span id="5d58" class="ne md it na b gy nx ng l nh ni">            tournaments[-1]['matches'].append({<br/>                'time': tr.css('.match .col-time time::attr(datetime)').get(),<br/>                'state': tr.css('.match .col-state span::text').get(),<br/>                'team_1_name': tr.css('.col-home a::text').get(),<br/>                'team_1_score': team_1_score,<br/>                'team_2_name': tr.css('.col-guest a::text').get(),<br/>                'team_2_score': team_2_score<br/>            })<br/>        <br/>        for t in tournaments:<br/>            yield {<br/>                'tournament': t<br/>            }</span></pre><h2 id="bb06" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">代码解释</h2><p id="427b" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">这个文件的代码结构与我们前面的例子相同。这里，我们刚刚用新功能更新了<code class="fe nj nk nl na b">parse()</code>方法。</p><p id="993c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本上，我们从页面中提取了所有的HTML <code class="fe nj nk nl na b">&lt;tr&gt;&lt;/tr&gt;</code>元素。然后，我们循环浏览它们，找出每一个是锦标赛还是比赛。如果是锦标赛，我们提取它的名字。在匹配的情况下，我们提取其<code class="fe nj nk nl na b">time</code>、<code class="fe nj nk nl na b">state</code>，以及两个队的<code class="fe nj nk nl na b">name</code>和<code class="fe nj nk nl na b">score</code>。</p><h2 id="30ab" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">运行示例</h2><p id="7aeb" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在控制台中键入以下命令，并执行它。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c239" class="ne md it na b gy nf ng l nh ni">pipenv run scrapy crawl LiveScore -o ls.json</span></pre><h2 id="f6f7" class="ne md it bd me nm nn dn mi no np dp mm lh nq nr mo ll ns nt mq lp nu nv ms nw bi translated">结果</h2><p id="d7cd" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">以下是检索内容的示例:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a19b" class="ne md it na b gy nf ng l nh ni">[{<br/>    "tournament": {<br/>        "name": "International - World Cup Qualification CONMEBOL",<br/>        "matches": [{<br/>            "time": "2020-11-18T00:00:00+01:00",<br/>            "state": null,<br/>            "team_1_name": "Uruguay",<br/>            "team_1_score": "0",<br/>            "team_2_name": "Brazil",<br/>            "team_2_score": "2"<br/>        }, {<br/>            "time": "2020-11-18T00:00:00+01:00",<br/>            "state": null,<br/>            "team_1_name": "Paraguay",<br/>            "team_1_score": "2",<br/>            "team_2_name": "Bolivia",<br/>            "team_2_score": "2"<br/>        }, {<br/>            "time": "2020-11-18T01:30:00+01:00",<br/>            "state": null,<br/>            "team_1_name": "Peru",<br/>            "team_1_score": "0",<br/>            "team_2_name": "Argentina",<br/>            "team_2_score": "2"<br/>        }]<br/>    }<br/>}]</span></pre><p id="2c55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在有了这些数据，我们可以做任何我们想做的事情，比如用它来训练我们自己的神经网络来预测未来的游戏。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b471" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="c45e" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">数据分析师经常使用网络抓取，因为这有助于他们收集数据来预测未来。同样，企业使用它来从网页中提取电子邮件，因为这是一种有效的方法来产生线索。我们甚至可以用它来监控产品的价格。</p><p id="0b8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，web抓取有很多用例，Python完全有能力做到。</p><p id="7bc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你还在等什么？现在尝试抓取您最喜爱的网站。</p><p id="4cf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div></div>    
</body>
</html>