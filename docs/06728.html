<html>
<head>
<title>Twitter Scrapers Are All Broken. What Should We Do?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推特刮刀都坏了。我们做什么呢</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/twitter-scrapers-are-all-broken-what-should-we-do-62a7349bfca6?source=collection_archive---------0-----------------------#2020-10-29">https://betterprogramming.pub/twitter-scrapers-are-all-broken-what-should-we-do-62a7349bfca6?source=collection_archive---------0-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4f1f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Twitter更新前端时该怎么办</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b6d31afe93916f453e0b5c0a7b8c930a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3_bQUtyVQl466zna1FASw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Pexels上的Ivan Samkov</p></figure><p id="bf9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">编辑2021年4月9日:</strong></p><p id="0fc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嘿！在这里更新。在这一点上，我看不到像<a class="ae lu" href="https://github.com/taspinar" rel="noopener ugc nofollow" target="_blank"> Taspinar的Twitterscraper </a> repo这样的图书馆的前进道路，由于Twitter在2020年的变化，这些图书馆可以快速收集大量的推文。</p><p id="3aa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文向读者介绍了Selenium以及如何用Twitter实现它。也就是说，如果你试图尽快获得一些数据，并且不需要了解Selenium atm，@ <a class="ae lu" href="https://github.com/Altimis" rel="noopener ugc nofollow" target="_blank"> Altimis </a>有一个repo，可以很好地为Twitter包装Selenium。这是回购的链接。<a class="ae lu" href="https://github.com/Altimis/Scweet" rel="noopener ugc nofollow" target="_blank">链接此处</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="2892" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每当Twitter更新其前端，刮刀就会断裂，数据科学家会抱怨，需要临时修复。在本文中，让我们快速浏览一下为什么个人会抓取数据而不是使用官方API的原因。我们还将讨论通过无头浏览器的临时补救措施，以及为什么这种(低效的)补救措施充其量应该是临时的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/2b053f335b3da299f1c944766ffd9af1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Prvnzaku59FoVnqWkq85g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="md me ep" href="https://medium.com/u/a250c85fa851?source=post_page-----62a7349bfca6--------------------------------" rel="noopener" target="_blank">马丁·贝克</a>的《<a class="ae lu" href="https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1" rel="noopener" target="_blank">如何从推特</a>中抓取推文》的更新截图</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="75c2" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated"><strong class="ak">为什么刮？</strong></h1><p id="1258" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">上周，我需要Twitter的历史数据来进行一个查询，然而似乎所有的Twitter信息回收都被破坏了！我需要在互联网上少数有才华的人对这些回购进行修改之前进行修复，并决定这是一个很好的机会来谈谈抓取和无头浏览器，并为处于相同困境的其他人编写一个快速的Selenium实现。</p><p id="11fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我将在这篇文章中指出的，除了手工收集Twitter数据，Selenium可能是最不理想的方法——然而，它是一个通向其他主题的极好的入口，从网络分析和自然语言处理，当然还有web抓取。</p><h2 id="09cb" class="nc mg it bd mh nd ne dn ml nf ng dp mp lh nh ni mr ll nj nk mt lp nl nm mv nn bi translated">接近新人刮</h2><p id="520f" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">当需要以有组织的格式收集数据时，个人通常可以选择使用相关网站的官方API，或者自己收集数据。不幸的是，选择一个选项而不是另一个选项的原因经常没有被讨论。我遇到过编写过刮刀但从未接触过API的人，以及其他(更常见的)使用过API但从未构建过刮刀的人。</p><p id="98d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，当接近铲运机时，首先要谈论的是道德、责任和最佳实践——这些话题在课堂上不常讨论。</p><p id="efb6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与学生通常在课堂环境中处理的经过净化、有序的数据不同，搜集数据需要知道如何负责任地搜集数据，以及如何有效地清理和转换这些数据。</p><p id="6a87" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在深入下面的代码之前，我鼓励你阅读一下<a class="md me ep" href="https://medium.com/u/6d06cd5d5310?source=post_page-----62a7349bfca6--------------------------------" rel="noopener" target="_blank">詹姆斯·登斯莫尔</a>的作品“<a class="ae lu" href="https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01" rel="noopener" target="_blank">网络抓取中的道德规范</a>”它非常短，为网络抓取的新手提供了一个明确的方向。虽然他文章中的所有指导方针都很关键，但我经常强调的一点是:</p><blockquote class="np"><p id="f6f4" class="nq nr it bd ns nt nu nv nw nx ny lt dk translated">“从数据中创造新价值的<strong class="ak">目的，而不是复制它。”</strong></p></blockquote></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="550e" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">Twitter刮刮卡关闭</h1><p id="fe31" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">Twitter的API限制用户查询过去七天内发布的推文。对于纵向分析来说，这是非常有限的，访问超过一周的数据会引发一系列全新的问题，涉及到所收集数据的定价和规模。</p><p id="a8ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Twitter抓取器允许个人从任何时间点收集数据，并在查询推文时具有接受开始和结束日期的功能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/050b09c5c949713312cd99a8980d09ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JX2D7XHkXEyroS6fgSoTpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GitGub上Taspinar/Twitterscraper的截图</p></figure><p id="e6ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，这些抓取器依赖于Twitter的前端，这意味着如果前端有变化，抓取器就会停止工作。上面的问题线索显示了关于最近一个特殊刮刀断裂的讨论。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="33e5" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">一种临时的硒溶液</h1><p id="1431" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">一般来说，对从网站收集数据感兴趣的个人应该尝试(按以下顺序):</p><ol class=""><li id="df97" class="oa ob it la b lb lc le lf lh oc ll od lp oe lt of og oh oi bi translated">浏览官方网站的API和文档。</li><li id="6cc1" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">查看网络流量和请求。<em class="no">(我推荐试试Postman进行测试。)</em></li><li id="7c05" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">使用像<a class="ae lu" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>或<a class="ae lu" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"> bs4 </a> + requests这样的库。<em class="no">(还有其他类似的解决方案——这些恰好都是Python库。)</em></li><li id="8dc9" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">使用Selenium这样的无头浏览器。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/f3858d561ad32cf257d53b7db8c710cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZEgunA_2Lv8NEI2VAMPvuw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自selenium.dev</p></figure><p id="1f40" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种特殊情况下，我需要一个快速的解决方案来收集过去三个月中来自一个单独的<a class="ae lu" href="https://twitter.com/search-advanced?lang=en" rel="noopener ugc nofollow" target="_blank">高级搜索查询</a>的推文。注意:Selenium是最不理想和最不实用的选择，尤其是Twitter，它有如此多的库从站点收集数据，但没有一个库可以随时工作。</p><p id="a662" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还建议直接查看<a class="ae lu" href="https://www.selenium.dev/documentation/en/" rel="noopener ugc nofollow" target="_blank"> Selenium的文档</a>——它非常直接地引导读者了解使用Selenium进行抓取的最重要方面。</p><h2 id="9a27" class="nc mg it bd mh nd ne dn ml nf ng dp mp lh nh ni mr ll nj nk mt lp nl nm mv nn bi translated">使用硒刮擦</h2><p id="b432" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">抓取功能本身非常简单。首先，初始化<code class="fe op oq or os b">webdriver</code>(像用户一样自动使用)并创建一个空的<code class="fe op oq or os b">DataFrame</code>来填充。</p><p id="b34e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意——这里有一个很棒的库，可以设置浏览器，而不必处理相应的Chrome版本:<a class="ae lu" href="https://pypi.org/project/webdriver-manager/" rel="noopener ugc nofollow" target="_blank">链接。</a></p><pre class="kj kk kl km gt ot os ou ov aw ow bi"><span id="561d" class="nc mg it os b gy ox oy l oz pa">from selenium import webdriver<br/>from selenium.webdriver.common.keys import Keys<br/>import pandas as pd</span><span id="a479" class="nc mg it os b gy pb oy l oz pa">browser_path = 'C:/vnineteen/chromedriver.exe'</span><span id="2306" class="nc mg it os b gy pb oy l oz pa"># setting the chromedriver path and initializing driver<br/>driver = webdriver.Chrome(executable_path=browser_path)</span><span id="aac2" class="nc mg it os b gy pb oy l oz pa"># create master df to append to<br/>master_df = pd.DataFrame()</span></pre><p id="5eab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，创建一个函数，在继续脚本之前等待一定的秒数。这是用Selenium进行刮擦的一个非常重要的部分，这样你的刮擦器看起来就像一个真正的用户，不会因为你的刮擦器的动作而使网站负担过重。</p><pre class="kj kk kl km gt ot os ou ov aw ow bi"><span id="76be" class="nc mg it os b gy ox oy l oz pa">from time import sleep<br/>import random</span><span id="8bd0" class="nc mg it os b gy pb oy l oz pa">def sleep_for(opt1, opt2):<br/>    time_for = random.uniform(opt1, opt2)<br/>    time_for_int = int(round(time_for))<br/>    sleep(abs(time_for_int - time_for))<br/>    for i in range(time_for_int, 0, -1):<br/>        sleep(1)</span></pre><p id="a446" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，遍历您创建的Twitter高级搜索URL(将您在这里搜索的URL从<a class="ae lu" href="https://twitter.com/search-advanced?lang=en" rel="noopener ugc nofollow" target="_blank">复制并粘贴到URL</a>变量中)，向下滚动页面指定的次数，创建包含每条tweet的列表，并将这个列表转换的数据帧附加到主数据帧。注意，在将这些数据转换成pandas数据帧之前，有更有效的方法来附加这些数据，但是对于我们当前的速度需求，这就足够了。</p><p id="39c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有一个要定义的变量叫做<code class="fe op oq or os b">post_element_xpath</code>。这是每个tweet的元素；我们将使用该元素在向下滚动网页后检索每条tweet。*请注意，下面的代码附加到pandas dataframe上——这是非常低效的，但在概念上很容易理解，特别是如果你更多地来自人文+ Excel背景，而不是计算机科学背景。</p><pre class="kj kk kl km gt ot os ou ov aw ow bi"><span id="dd1c" class="nc mg it os b gy ox oy l oz pa">from progressbar import ProgressBar<br/>pbar = ProgressBar()</span><span id="a961" class="nc mg it os b gy pb oy l oz pa">urls = ['<a class="ae lu" href="https://twitter.com/search?q=simpsons%20(predicted%20OR%20covid)&amp;src=typed_query']" rel="noopener ugc nofollow" target="_blank">https://twitter.com/search?q=simpsons%20(predicted%20OR%20covid)&amp;src=typed_query']</a></span><span id="873f" class="nc mg it os b gy pb oy l oz pa"># how many times should the browser scroll down<br/>scroll_down_num = 5  </span><span id="16e5" class="nc mg it os b gy pb oy l oz pa"># the element we are obtaining from the webpage<br/>post_element_xpath = '//div/div/article/div/div'</span><span id="012d" class="nc mg it os b gy pb oy l oz pa"># loop through your list of urls<br/>for url in pbar(urls):<br/>    driver.get(url)<br/>    sleep_for(10, 15)  # sleep a while</span><span id="1da2" class="nc mg it os b gy pb oy l oz pa">    # scroll x number of times<br/>    for i in range(0, scroll_down_num):<br/>        # scroll down<br/>        driver.find_element_by_xpath('//body').send_keys(Keys.END)<br/>        sleep_for(4, 7)</span><span id="6892" class="nc mg it os b gy pb oy l oz pa">    # get a list of tweets<br/>    post_list = driver.find_elements_by_xpath(post_element_xpath)</span><span id="ce8d" class="nc mg it os b gy pb oy l oz pa">    # get the text only from each element<br/>    post_text = [x.text for x in post_list]</span><span id="690e" class="nc mg it os b gy pb oy l oz pa">    # create temporary dataset of each tweet<br/>    temp_df = pd.DataFrame(post_text, columns={'all_text'})</span><span id="62e6" class="nc mg it os b gy pb oy l oz pa">    # append the temporary dataset to the dataset we will save<br/>    master_df = master_df.append(temp_df) </span></pre><p id="339a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">无论个人是否登录，Twitter都限制向下滚动的次数，这意味着如果你对访问更多的推文感兴趣，我建议通过编辑你通过<code class="fe op oq or os b">driver.get()</code>传递的URL字符串，生成一个开始和结束日期的列表进行查询。</p><p id="8967" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们收集的推文将包含用户句柄、推文、点赞数、回复等。作为每个字符串中的单独行。因此，为了解析结果数据，使用<code class="fe op oq or os b">.splitlines()</code>创建一个元素列表，从抓取中返回的每一行一个元素。</p><p id="5881" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是我们收集的推文的一个例子。</p><pre class="kj kk kl km gt ot os ou ov aw ow bi"><span id="1ea2" class="nc mg it os b gy ox oy l oz pa">Jeremy Gothem<br/><a class="ae lu" href="http://twitter.com/jeremygutsche" rel="noopener ugc nofollow" target="_blank">@jeremyg</a>othem<br/>·<br/>Jun 4<br/>Hotel Pivots - Hotels are repurposing their rooms during COVID-19<br/>1</span></pre><p id="806f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于元素根据tweet和配置文件的类型而变化(一条tweet可能包含6个元素，另一条可能包含8个元素)，所以为解析增加了一些灵活性，允许脚本在有限的列表索引范围内搜索适当的元素。</p><p id="2a14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">运行解析函数，将其应用于数据帧的文本列。然后导出数据集。</p><pre class="kj kk kl km gt ot os ou ov aw ow bi"><span id="687a" class="nc mg it os b gy ox oy l oz pa">def parse_text(text):</span><span id="9f9b" class="nc mg it os b gy pb oy l oz pa">    # split by new line<br/>    text_list = str.splitlines(text) </span><span id="68ef" class="nc mg it os b gy pb oy l oz pa">    # get the username (always the first list element)<br/>    username = text_list[0]  </span><span id="655f" class="nc mg it os b gy pb oy l oz pa">    # within the first few elements, find the element<br/>    # with the @ symbol, this will be the user handle<br/>    handle = ''.join(x for x in text_list[1:3] if '@' in x)</span><span id="3d73" class="nc mg it os b gy pb oy l oz pa">    # get the date, using the single dot to identify its <br/>    # index location<br/>    dot_position = text_list[1:4].index('·')  <br/>    date = text_list[dot_position + 2]  # date comes after dot</span><span id="9cfa" class="nc mg it os b gy pb oy l oz pa">    # check if its a reply to someone else<br/>    if text_list[4] == "Replying to ":<br/>        reply_to = True<br/>        reply_to_handle = text_list[5]<br/>        text = text_list[6]<br/>    else:<br/>        reply_to = False<br/>        reply_to_handle = ''<br/>        # find the longest string within list index 4:6<br/>        # this will be the tweet text<br/>        text = max(text_list[4:6], key=len)</span><span id="5988" class="nc mg it os b gy pb oy l oz pa">    # return the variables we have parse from the text<br/>    return pd.Series([username, handle,<br/>                      date, reply_to, reply_to_handle, text])</span><span id="e571" class="nc mg it os b gy pb oy l oz pa"># run the parse function via pandas apply<br/>df[['username', 'handle', 'date', 'reply_to', 'reply_to_handle', 'atext']<br/>   ] = df['text'].apply(parse_text)</span><span id="6fd9" class="nc mg it os b gy pb oy l oz pa"># export csv<br/>df.to_csv('output.csv')</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="0faa" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">前进</h1><p id="30ed" class="pw-post-body-paragraph ky kz it la b lb mx ju ld le my jx lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">如果你正在学习数据挖掘、自然语言处理或网络分析，Twitter是一个很好的起点。虽然Twitter API是非常好的入门工具(你可以使用像<a class="ae lu" href="https://www.tweepy.org/" rel="noopener ugc nofollow" target="_blank"> Tweepy </a>这样的库来轻松使用它)，但遗憾的是，使用该API进行纵向分析是行不通的，如果Twitter抓取器不起作用，我希望这段代码可以作为权宜之计。</p><p id="43d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，我建议增加关于开始和结束日期的额外自动化，以使用这个刮刀查询额外的数据——从7月到10月，我能够为一个非常合适的主题收集大约3000条推文。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8481" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">Scweet回购脚本(从4月编辑开始)</h1><ol class=""><li id="676c" class="oa ob it la b lb mx le my lh pc ll pd lp pe lt of og oh oi bi translated">在里面放一个文件夹和光盘</li><li id="dbd4" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">饭桶克隆<a class="ae lu" href="https://github.com/Altimis/Scweet.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Altimis/Scweet.git</a></li><li id="3a25" class="oa ob it la b lb oj le ok lh ol ll om lp on lt of og oh oi bi translated">运行下面的脚本</li></ol><pre class="kj kk kl km gt ot os ou ov aw ow bi"><span id="362c" class="nc mg it os b gy ox oy l oz pa">import pandas as pd</span><span id="3675" class="nc mg it os b gy pb oy l oz pa">import os<br/>from os import chdir, getcwd</span><span id="0354" class="nc mg it os b gy pb oy l oz pa"># sets the path to the current folder you're cd'ed into<br/>folder_loc = os.path.dirname(os.path.realpath("__file__"))<br/>os.chdir(folder_loc)</span><span id="fddf" class="nc mg it os b gy pb oy l oz pa"># make sure you git cloned the repo into your working directory <br/># folder<br/>from Scweet.Scweet.scweet import scrap</span><span id="be8a" class="nc mg it os b gy pb oy l oz pa"><br/># set parameters<br/>list_handles = ['theo_goe','handle2','etc']<br/>output_file_name = 'output_file.csv'</span><span id="d3ac" class="nc mg it os b gy pb oy l oz pa">start_date = "2020-11-01"<br/>max_date = "2020-12-15"<br/># interval of how many days to search in between<br/>interval_in = 30</span><span id="5ccb" class="nc mg it os b gy pb oy l oz pa"># make an empty dataframe to fill<br/>tweets_df = pd.DataFrame()</span><span id="05dc" class="nc mg it os b gy pb oy l oz pa">for handle in list_handles:<br/>    # <a class="ae lu" href="https://github.com/Altimis/Scweet" rel="noopener ugc nofollow" target="_blank">https://github.com/Altimis/Scweet</a><br/>    data = scrap(start_date=start_date, max_date=max_date, from_account = handle, interval=interval_in, <br/>          headless=True, display_type="Top", save_images=False, <br/>                 resume=False, filter_replies=True, proximity=True)<br/>    tweets_df = tweets_df.append(data)</span><span id="820f" class="nc mg it os b gy pb oy l oz pa">tweets_df.to_csv(output_file_name)</span></pre></div></div>    
</body>
</html>