<html>
<head>
<title>iOS On-Device Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">iOS设备上的语音识别</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/ios-speech-recognition-on-device-e9a54a4468b5?source=collection_archive---------0-----------------------#2019-11-02">https://betterprogramming.pub/ios-speech-recognition-on-device-e9a54a4468b5?source=collection_archive---------0-----------------------#2019-11-02</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="f88c" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">SFSpeechRecognizer已在iOS13中更新，允许在设备上、无数据和离线的情况下识别和分析语音</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/86ba12752ceee4a99bf0742ba3d7cee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2PmDc2EwzIKf02IF8tU9JQ.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片由<a class="ae kz" href="https://pixabay.com/users/theglassdesk-149631/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1109588" rel="noopener ugc nofollow" target="_blank"> Becca Clark </a>从<a class="ae kz" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1109588" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄</p></figure><p id="2d5d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">苹果在今年的WWDC 2019期间展示了其在机器学习和人工智能领域的进步。iOS 13中的设备上语音识别功能就是一个展示他们野心的功能。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="6690" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">范围</h1><p id="8087" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">设备上的语音识别通过将用户的数据保存在云之外来增加用户的隐私。苹果公司试图通过这种增强的语音识别来大大推动基于语音的人工智能。</p><p id="d8f6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">新升级的语音识别API允许您做各种事情，如使用语音分析指标跟踪语音质量和语音模式。</p><p id="ec77" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">从提供基于录音的自动反馈到比较个人的语音模式，在人工智能领域，使用设备上的语音识别可以做很多事情。</p><p id="7af8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">当然，对于这种新的设备上语音识别，需要考虑某些权衡。没有像你在云上那样的持续学习。这可能会导致设备精度降低。此外，语言支持目前仅限于大约10种语言。</p><p id="d3ca" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">尽管如此，设备上的支持让您可以无限期地进行语音识别。这是对之前服务器一分钟记录限制的一次重大胜利。</p><p id="a2b9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><code class="fe na nb nc nd b">SFSpeechRecognizer</code>是驱动语音识别的引擎。</p><p id="a937" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">iOS 13 SFSpeechRecognizer足够智能，可以识别你讲话中的标点符号。</p><p id="e7f6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">说一个点加一个句号。类似地，逗号、破折号和问号将返回相应的标点符号: (，—？).</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="6917" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">我们的目标</h1><p id="8660" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">开发一个录制实时音频的设备上语音识别iOS应用程序。下面举例说明了我们在本文结束时将实现的目标:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ne"><img src="../Images/39221798aaf765368501a6da2a8d3c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/1*ur7cevoZiT9gVnHfI69bTQ.gif"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">我们应用程序的屏幕截图。</p></figure><p id="d926" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">你注意到了吗？</p><p id="57b5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><em class="nf">上面的截图是在飞行模式下拍摄的。</em></p><p id="dc3f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">不再浪费时间，让我们打开麦克风，开始构建设备上语音识别应用的旅程。</p><p id="34a8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在接下来的章节中，我们将跳过UI和美学部分，直接进入语音和音频框架。让我们开始吧。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="dec5" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">添加隐私使用说明</h1><p id="aadb" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">首先，您需要在您的<code class="fe na nb nc nd b">info.plist</code>中包含麦克风和语音识别的隐私使用说明，如下所示。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ng"><img src="../Images/c5f3aceb598d14987a88dd8ad212e61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vssP_deTS7uDwTV8.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">不添加它肯定会导致运行时崩溃。</p></figure><p id="e0ef" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">接下来，在您的<code class="fe na nb nc nd b">ViewController</code>类中使用<code class="fe na nb nc nd b">import Speech</code>来访问您的应用程序中的语音框架。</p><h2 id="d0cd" class="nh me iu bd mf ni nj dn mj nk nl dp mn lj nm nn mp ln no np mr lr nq nr mt ns bi translated">请求权限</h2><p id="0a6d" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">为了使用语音识别，我们需要请求用户授权。以下代码为您完成了这项工作:</p><pre class="kk kl km kn gu nt nd nu nv aw nw bi"><span id="6ae9" class="nh me iu nd b gz nx ny l nz oa">SFSpeechRecognizer.requestAuthorization{authStatus in<br/><br/>            OperationQueue.main.addOperation {<br/>               switch authStatus {<br/>                    case .authorized:<br/>                    case .restricted:<br/>                    case .notDetermined:<br/>                    case .denied:<br/>               }<br/>            }<br/>}</span></pre><p id="4384" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><code class="fe na nb nc nd b">SFSpeechRecognizer</code>负责通过<code class="fe na nb nc nd b">SFSpeechRecognitionTask</code>生成你的转录。为此，我们必须初始化我们的<code class="fe na nb nc nd b">SFSpeechRecognizer</code>:</p><pre class="kk kl km kn gu nt nd nu nv aw nw bi"><span id="5a68" class="nh me iu nd b gz nx ny l nz oa">var speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en_IN"))</span></pre><p id="1070" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在上面的代码中，您需要传递在整个电话中使用的地区标识符。我的情况是英语(印度)。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="f0d9" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">语音识别:幕后</h1><p id="3f4f" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">设备上语音识别的工作原理如下图所示:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ob"><img src="../Images/8948f9b991dc187d446e4fc325a588c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XU3Hj4VFdN2ZIvs7aFxa8w.jpeg"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">语音识别流程</p></figure><p id="f667" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">从上图可以看出，任何语音识别应用程序都有四个支柱:</p><ul class=""><li id="8e0c" class="oc od iu lc b ld le lg lh lj oe ln of lr og lv oh oi oj ok bi translated"><code class="fe na nb nc nd b">AVAudioEngine</code></li><li id="30ba" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated"><code class="fe na nb nc nd b">SFSpeechRecognizer</code></li><li id="8770" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated"><code class="fe na nb nc nd b">SFRecognitionTask</code></li><li id="fdf3" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated"><code class="fe na nb nc nd b">SFSpeechAudioBufferRecognitionRequest</code></li></ul><p id="097d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在接下来的章节中，我们将会看到它们在构建我们的语音识别应用程序中所扮演的角色。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="076b" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">履行</h1><h2 id="87f5" class="nh me iu bd mf ni nj dn mj nk nl dp mn lj nm nn mp ln no np mr lr nq nr mt ns bi translated">设置音频引擎</h2><p id="f148" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated"><code class="fe na nb nc nd b">AVAudioEngine</code>负责接收来自麦克风的音频信号。它为语音识别提供我们的输入。</p><pre class="kk kl km kn gu nt nd nu nv aw nw bi"><span id="0fae" class="nh me iu nd b gz nx ny l nz oa">let audioEngine = AVAudioEngine()<br/>let audioSession = AVAudioSession.sharedInstance()<br/>try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)<br/>try audioSession.setActive(true, options: .notifyOthersOnDeactivation)<br/><br/>let inputNode = audioEngine.inputNode<br/><br/>inputNode.removeTap(onBus: 0)<br/>        let recordingFormat = inputNode.outputFormat(forBus: 0)<br/>        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer: AVAudioPCMBuffer, when: AVAudioTime) in<br/>            self.recognitionRequest?.append(buffer)<br/>        }<br/>        <br/>audioEngine.prepare()<br/>try audioEngine.start()</span></pre><p id="16bf" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">上面的代码在<code class="fe na nb nc nd b">inputNode</code>上安装了一个tap，并设置输出的缓冲区大小。</p><p id="0e9c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">一旦该缓冲区大小被填满(通过您说话或录音时的音频信号)，它就会被发送到<code class="fe na nb nc nd b">SFSpeechAudioBufferRecognitionRequest</code>。</p><p id="2b15" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在让我们看看<code class="fe na nb nc nd b">SFSpeechAudioBufferRecognitionRequest</code>如何与<code class="fe na nb nc nd b">SFSpeechRecognizer</code>和<code class="fe na nb nc nd b">SFSpeechRecognitionTask</code>一起工作，以便将语音转录为文本。</p><h2 id="56ca" class="nh me iu bd mf ni nj dn mj nk nl dp mn lj nm nn mp ln no np mr lr nq nr mt ns bi translated">启用设备上的语音识别</h2><p id="ae0f" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">以下代码在电话上启用设备上的语音识别:</p><pre class="kk kl km kn gu nt nd nu nv aw nw bi"><span id="e2d2" class="nh me iu nd b gz nx ny l nz oa">recognitionRequest.requiresOnDeviceRecognition = true</span></pre><p id="0983" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">将<code class="fe na nb nc nd b">requiresOnDeviceRecognition</code>设置为<code class="fe na nb nc nd b">false</code>将使用Apple Cloud进行语音识别。</p><p id="8563" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">请注意，设备上的语音识别只能在iOS 13、macOS Catalina和更高版本的设备上使用。它需要苹果的A9或新处理器，该处理器在iOS中受iPhone6s及以上设备的支持。</p><h2 id="58e6" class="nh me iu bd mf ni nj dn mj nk nl dp mn lj nm nn mp ln no np mr lr nq nr mt ns bi translated">创建语音识别任务</h2><p id="e08e" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">SFSpeechRecognitionTask用于通过<code class="fe na nb nc nd b">SFSpeechRecognizer</code>运行<code class="fe na nb nc nd b">SFSpeechAudioBufferRecognitionRequest</code>。作为回报，它提供了结果实例，从中我们可以访问不同的语音属性。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oq or l"/></div></figure><p id="5a7c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在上面的代码中，发生了很多事情。让我们把它分解成几块。</p><ul class=""><li id="aa8a" class="oc od iu lc b ld le lg lh lj oe ln of lr og lv oh oi oj ok bi translated">首先，当按下<code class="fe na nb nc nd b">startRecording</code>时，我们取消任何先前的识别任务。</li><li id="a295" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated">接下来，我们使用SFSpeechRecognizer和识别请求创建识别任务。</li><li id="0274" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated">将<code class="fe na nb nc nd b">shouldReportPartialResults</code>设置为true允许在每次发声期间访问中间结果。</li><li id="d912" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated"><code class="fe na nb nc nd b">result.bestTranscription</code>返回可信度最高的转录。在它上面调用<code class="fe na nb nc nd b">formattedString</code>属性给出转录的文本。</li><li id="a6f9" class="oc od iu lc b ld ol lg om lj on ln oo lr op lv oh oi oj ok bi translated">我们可以访问其他属性，如<code class="fe na nb nc nd b">speakingRate</code>、<code class="fe na nb nc nd b">averagePauseDuration</code>或<code class="fe na nb nc nd b">segments</code>。</li></ul><h2 id="566e" class="nh me iu bd mf ni nj dn mj nk nl dp mn lj nm nn mp ln no np mr lr nq nr mt ns bi translated">SFVoiceAnalytics</h2><p id="f2db" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated"><code class="fe na nb nc nd b">SFVoiceAnalytics</code>是一个新引入的类，它包含一组语音指标，用于跟踪语音结果中的音调、闪烁和抖动等特征。</p><p id="0e4b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">可以从转录的段属性中访问它们:</p><pre class="kk kl km kn gu nt nd nu nv aw nw bi"><span id="64dd" class="nh me iu nd b gz nx ny l nz oa">for segment in result.bestTranscription.segments {<br/>                    guard let voiceAnalytics = segment.voiceAnalytics else { continue }<br/><br/>                    let pitch = voiceAnalytics.pitch<br/>                    let voicing = voiceAnalytics.voicing.acousticFeatureValuePerFrame<br/>                    let jitter = voiceAnalytics.jitter.acousticFeatureValuePerFrame<br/>                    let shimmer = voiceAnalytics.shimmer.acousticFeatureValuePerFrame<br/>}</span></pre><h2 id="b623" class="nh me iu bd mf ni nj dn mj nk nl dp mn lj nm nn mp ln no np mr lr nq nr mt ns bi translated">开始录音和转录</h2><p id="ada2" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">现在我们已经定义了四个组件中的每一个，是时候合并支柱了，以便开始在<code class="fe na nb nc nd b">UITextView</code>中记录和显示转录。下面的代码片段可以帮你做到这一点。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oq or l"/></div></figure></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="3ebe" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">结论</h1><p id="8ad4" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">上面的实现步骤应该会返回一个类似于本文开头的屏幕截图的结果。该应用程序的完整源代码可以在这个<a class="ae kz" href="https://github.com/anupamchugh/iowncode/tree/master/iOS13OnDeviceSpeechRecognition" rel="noopener ugc nofollow" target="_blank"> Github资源库</a>中找到。</p><p id="4f27" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这就是我对iOS 13中的设备语音识别的总结。当与声音分类器和NLP一起使用时，这个新的升级将会很方便。</p><p id="d80b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我希望你喜欢读这篇文章。现在开始使用新的<code class="fe na nb nc nd b">SFSpeechRecognizer</code>构建你自己的基于语音的人工智能应用。</p></div></div>    
</body>
</html>