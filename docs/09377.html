<html>
<head>
<title>A Friendly Guide to NLP: TF-IDF With Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP友好指南:TF-IDF和Python示例</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/a-friendly-guide-to-nlp-tf-idf-with-python-example-5fcb26286a33?source=collection_archive---------1-----------------------#2021-08-17">https://betterprogramming.pub/a-friendly-guide-to-nlp-tf-idf-with-python-example-5fcb26286a33?source=collection_archive---------1-----------------------#2021-08-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="62f2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将实现与Scikit-Learn的tfidf矢量器进行比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cc15cb96947f0a643ac1ca57226fa59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xw19xjYH_oyaIoX2znlPjw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图</p></figure><p id="f227" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">这是</em> <a class="ae lv" href="https://eugenia-anello.medium.com/nlp-tutorial-series-d0baaf7616e0" rel="noopener"> <em class="lu"> NLP教程系列</em> </a> <em class="lu">的第三篇帖子。本指南将让您逐步了解如何从头开始实现TF-IDF，并将获得的结果与已经实现的Scikit-learn的</em>tfidf矢量器<em class="lu">进行比较。</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="0e72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">处理大量文本以执行分类、语音识别或翻译的模型需要额外的步骤来处理这些类型的数据。文本数据需要转换成其他东西，数字，计算机可以理解。有许多技术可用于使用此类数据创建新要素。</p><p id="cce2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中一个是<strong class="la iu">词频-逆文档频</strong>，也叫TF-IDF。这个长名字看起来很吓人，但是这种方法的思想很简单。这是一种技术，用于量化一个文档中以及同时跨一组文档的单词的重要性。</p><p id="f59f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，有许多像“the”、“is”、“I”这样的常用词在句子中频繁出现，但对带来信息没有显著贡献。如果我们只看频率这个词，这些词会显得比其他词更重要。因此，引入TF-IDF是为了在数据集中拥有更强大的特征。</p><p id="daf7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">目录:</p><ul class=""><li id="5d38" class="md me it la b lb lc le lf lh mf ll mg lp mh lt mi mj mk ml bi translated">标准TF-IDF</li><li id="9df5" class="md me it la b lb mm le mn lh mo ll mp lp mq lt mi mj mk ml bi translated"><a class="ae lv" href="#0b70" rel="noopener ugc nofollow"> Sklearn TF-IDF </a></li><li id="e0b9" class="md me it la b lb mm le mn lh mo ll mp lp mq lt mi mj mk ml bi translated"><a class="ae lv" href="#ff3f" rel="noopener ugc nofollow">一个简单的例子</a></li><li id="0c92" class="md me it la b lb mm le mn lh mo ll mp lp mq lt mi mj mk ml bi translated"><a class="ae lv" href="#ba33" rel="noopener ugc nofollow">用Python实现</a></li><li id="9a1d" class="md me it la b lb mm le mn lh mo ll mp lp mq lt mi mj mk ml bi translated"><a class="ae lv" href="#facf" rel="noopener ugc nofollow">与Scikit-Learn的比较</a></li></ul><h1 id="d7fd" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">标准TF-IDF</h1><p id="1098" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">它由两个术语组合而成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/58f5bf559e0f8d5b03a0b1c427b1e4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYuqqICc7nyNGBEn4Fg4Ag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="c16d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la iu">词频</strong> (TF)是单词<code class="fe np nq nr ns b">t</code>在文档d中的出现频率，换句话说，就是该单词在文档中的计数与总字数的比值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/5e5895e96909e8b60ece2e5aa6dddb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOMCF5_plhul9yAS8eSl9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="183c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们之前说过的，频率这个术语不足以提供有效的度量。我们还需要把它和另一个术语结合起来，叫做<strong class="la iu">逆文档频率</strong>。这是一个分数的对数变换，计算方法是将语料库中的文档总数除以包含该单词的文档数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/beafca219f3671cc0c34336cd9c65f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAv0xsTO24_ywJOuJM_ObA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="e274" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更好的方法是精确地指出对数是以10为底的，而在Sci-Kit learn中，在实现</p><h1 id="0b70" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">Sklearn TF-IDF</h1><p id="b152" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">在前一段中，已经介绍了TF-IDF的标准定义。现在，我将展示在sklearn中计算的<code class="fe np nq nr ns b">tf-idfs</code>的<code class="fe np nq nr ns b"><a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">TfidfTransformer</strong></a></code>和<code class="fe np nq nr ns b"><a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">TfidfVectorizer</strong></a></code>与标准方法有何不同:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/96cb2b39e59ccb71678f8fcfc7c7376c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8TT1FmB6Kvl5PoDvozbWtQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="6cd3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以很容易地观察到<code class="fe np nq nr ns b">sklearn</code>版本的IDF在分子和分母上都加了1，以避免零除。此外，等于1的常数被加到对数项上。</p><p id="805d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，如果我们有<code class="fe np nq nr ns b">n=3</code>文档和<code class="fe np nq nr ns b">df(t)=3</code>，这意味着该单词出现在所有文档中，那么根据Scikit-learn定义，IDF(t)等于<code class="fe np nq nr ns b">ln((1+3)/(1+3))+1 = 1</code>，而在标准情况下等于<code class="fe np nq nr ns b">IDF(t) = log10(3/3) = 0</code>。</p><p id="838e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦计算出TF和IDF分数，我们就可以最终通过以下公式获得TF-IDF向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/58f5bf559e0f8d5b03a0b1c427b1e4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYuqqICc7nyNGBEn4Fg4Ag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="a874" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们使用<strong class="la iu">欧几里德范数</strong>对结果TF-IDF向量进行归一化之后:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f405edac98967b0f1f3244b9efb29418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*vjRaMGht1qn28IygtH1rng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用L2规范标准化。图片来源:作者</p></figure><p id="3f93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，给定原始tf-idfs的向量v = [0.06，0.06，0.022，0.022]，我们应用L2范数，获得第一个文档的TF-IDFs:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/d4ef0435e45fd0ffc3fd9b40d2d889cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*truOQ6z4lEnCyGerRQfw7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><h1 id="ff3f" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">一个简单的例子</h1><p id="1727" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">让我们看一个简单的例子来理解前面解释的概念。我们可能有兴趣分析一下关于《权力的游戏》的评论:</p><p id="1143" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">回顾1 </strong>:权力的游戏是一部惊艳的电视剧！</p><p id="cc25" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">回顾2 </strong>:权力的游戏是最好看的电视剧！</p><p id="350f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">回顾3 </strong>:权力的游戏太棒了。</p><p id="5df7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在表中，我按照Sklearn方法显示了获得TF-IDFs的所有计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/4a45c5e43125e3716b45fff6ba24be51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4QxhVr1r8bGMafMfXXqog.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表中列出了获得TF-IDFs的所有计算方法。图片鸣谢:作者。</p></figure><p id="4929" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很明显，最常见的词，如“游戏”、“of”、“权力”、“is”，具有最小的IDF，1，这意味着它们将具有0.3左右的低权重。</p><p id="ebcd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，像“惊人的”和“伟大的”这样的词具有更高的TF-IDF值，因为这些词中的每一个都只出现在一篇评论中。</p><p id="83cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，像“so”和“the”这样的常用词也贡献更多，因为它们并不出现在所有的句子中，我们没有删除停用词以使方法尽可能简单。</p><h1 id="ba33" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">用Python实现</h1><p id="1da3" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">教程中最重要的部分现在开始。让我们导入库:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="6efc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到之前示例中考虑的评论:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="2b2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在TF-IDF中，我们不考虑标点符号，所以需要去掉。此外，我们将字符串转换成一个列表，以方便应用程序。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="af20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到这三个列表，我们还想获得唯一单词的列表:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/4e9ee0371e5ad6a5fc798371f77c7a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N3fkxF8gPLnDnpDg1XcOqA.png"/></div></div></figure><p id="006b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一步是获得文档中每个单词的<strong class="la iu">词频</strong>分数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="fc8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TF分数的结果如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/71982b63dfaa47f88bf10154258b3776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQIXSlUS5ilk_Y4_L_mBJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="bddb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们要按照Sklearn方法计算<strong class="la iu"> IDF </strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="5991" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们最终可以获得IDF值，如上表所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/f80c130d8935984ca8e5b78653576948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0e6s2g3UKiQHV7P8JB9WQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure><p id="a371" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后也是最重要的一步是为每个单词和每个文档获得<strong class="la iu"> TF-IDF分数</strong>。这些稳健性度量是通过将TF和IDF值相乘获得的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="c6ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面，我展示了通过这个函数得到的最终输出:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/b8c79fa5e9f89b74a6f5df09b3b0d20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwKA_zX48myQgLZ_hZGnfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从头开始实现获得的输出。图片来源:作者</p></figure><h1 id="facf" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">与Scikit-Learn的比较</h1><p id="9d02" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">此时，我们可以直接应用Scikit-Learn实现的TfidfVectorizer:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/3652d09ca9c7e1fcc1356b8171f86b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqnedbW6tIjPRtQwE-4w_w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Sklearn获得的结果。图片来源:作者</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/b8c79fa5e9f89b74a6f5df09b3b0d20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwKA_zX48myQgLZ_hZGnfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出是通过从头开始实现获得的。图片来源:作者</p></figure><p id="4443" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用更少的代码行获得了同样的结果！是不是很神奇？</p><h1 id="1c1e" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">最后的想法</h1><p id="dba5" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我希望这篇教程对你理解TF-IDF是如何工作的有所帮助。起初，我在理解为什么标准方法和Sklearn结果不同时遇到了一些问题。</p><p id="76c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我更好地阅读了TfidfVectorizer的文档和GitHub代码之后，我了解到这种方法有一些修改。瞧。当然还有其他修改要添加。</p><p id="6343" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">比如去掉停用词，我推荐做。函数<code class="fe np nq nr ns b">TfidfVectorizer</code>也有很多参数来改变获取TF-IDFs值的方式。请检查<a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" rel="noopener ugc nofollow" target="_blank">文档</a>。GitHub的代码在这里是<a class="ae lv" href="https://github.com/eugeniaring/Medium-Articles/tree/main/NLP" rel="noopener ugc nofollow" target="_blank"/>。感谢阅读。祝您愉快！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="20b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你喜欢我的文章吗？<a class="ae lv" href="https://eugenia-anello.medium.com/membership" rel="noopener"> <em class="lu">成为会员</em> </a> <em class="lu">每天无限获取数据科学新帖！这是一种间接的支持我的方式，不会给你带来任何额外的费用。如果您已经是会员，</em> <a class="ae lv" href="https://eugenia-anello.medium.com/subscribe" rel="noopener"> <em class="lu">订阅</em> </a> <em class="lu">每当我发布新的数据科学和python指南时，您都会收到电子邮件！</em></p></div></div>    
</body>
</html>