# 令人惊讶的简单流式数据解决方案

> 原文：<https://betterprogramming.pub/the-surprisingly-simple-solution-for-streaming-data-6f3471c6d408>

![](img/86d92d4bb152aeebab591e4409d73c90.png)

弗兰基·查马基在 [Unsplash](https://unsplash.com/s/photos/data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

之前，我已经介绍过为什么[函数式编程为当今软件工程核心的大数据问题提供了概念基础](https://medium.com/better-programming/why-everyone-should-learn-functional-programming-today-c96a5b10d27d)。

现在，让我们看看这些功能概念是如何应用于构建一种称为*流的大数据数据结构的。*

# 我们所说的溪流是什么意思？

什么是*流，*到底是什么？它是数据中结构化事件的有序序列。

这些可能是实际事件，如鼠标点击或页面浏览，也可能是更抽象的东西，如客户订单、银行交易或传感器读数。不过，通常情况下，事件并不是大型数据模型的完全呈现视图，而是随着时间的推移而变化的微小且可测量的事物。

每个事件都是一个数据点，我们预计每秒钟会有数百个甚至数百万个这样的事件。所有这些事件一起按顺序形成了我们的流。

我们如何存储这种数据？我们可以把它写到数据库表中，但是如果我们每秒钟插入数百万行，我们的数据库将很快崩溃。

所以传统的关系数据库已经过时了。

# 进入消息代理

为了处理流数据，我们使用一种特殊的数据基础设施，称为*消息代理。*

消息代理独特地适应了事件流的挑战。它们不提供数据索引，并且是为快速插入而设计的。另一方面，我们可以快速选择最新的事件，看一看，然后继续下一个。

这个系统的两端——一端插入，另一端读取——分别被称为*生产者*和*消费者*。

我们将在我们的流中产生数据，然后从中消费数据。您可能还会从它的基本数据结构中认出这种设计，即*队列。*

# 内存缓冲区

所以现在我们知道如何插入和读取数据。但是在这两者之间它是如何储存的呢？

一种选择是把这些都留在记忆中。插入会将新事件添加到内存的内部队列中。消费者读取数据会将事件从存储器中移除。然后，我们可以保存一组指向队列前端和末端的指针。

但是内存是昂贵的，我们并不总是有很多内存。当我们用完内存时会发生什么？我们的消息代理将不得不离线，将其内存刷新到磁盘，或者中断其操作。

# 磁盘上的缓冲器

另一种选择是将数据写入本地磁盘。您可能习惯于认为磁盘很慢。当然可以。但是今天使用现代固态硬盘或虚拟化磁盘(如亚马逊的 EBS(弹性块存储))进行磁盘访问对于我们的目的来说已经足够快了。

现在，我们的数据可以随着 SSD 的大小而扩展，我们可以在服务器上运行了。或者更好的是，如果我们是云提供商，我们可以根据需要添加虚拟化磁盘进行扩展。

# 过时的数据

但是等一下。我们将把数百万个事件铲进我们的消息代理。我们不是会很快用完磁盘空间吗？

这就是为什么我们对数据有一个生存时间(TTL)。我们的数据会因老化而存储不足。该设置通常是可配置的。假设我们把它设置为一个小时。我们流中的事件将只存储一个小时，之后，它们将永远消失。

另一种看待它的方式是把磁盘上的流想象成一个循环缓冲区。消息代理只缓冲最后一个小时的数据，这意味着该数据的 *c* 消费者必须最多落后一个小时。

# 介绍你的新朋友，阿帕奇·卡夫卡

事实上，我刚刚描述的系统正是阿帕奇卡夫卡式的工作方式。Kafka 是目前最流行的大数据解决方案之一，也是最好的流媒体开源系统。

下面是我们如何使用 Java 在 Kafka 中创建一个生产者并向其写入数据。

```
Properties properties = new Properties();
properties.put("bootstrap.servers", "localhost:9092");
properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");KafkaProducer kafkaProducer = new KafkaProducer(properties);
kafkaProducer.send(new ProducerRecord("mytopic", 0, "test message")); // Push a message to topic "mytopic"
```

另一边是我们的消费者，他们将会阅读这条信息。

```
Properties properties = new Properties();
properties.put("bootstrap.servers", "localhost:9092");
properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
properties.put("group.id", "mygroup");
KafkaConsumer kafkaConsumer = new KafkaConsumer(properties);List topics = new ArrayList();
topics.add("mytopic");
kafkaConsumer.subscribe(topics); // Subscribe to "mytopic"ConsumerRecords records = kafkaConsumer.poll(1); // Get back the next record, blocking until it's available
```

这里有一些细节是卡夫卡行话特有的。首先，我们必须将我们的消费者/生产者代码指向 Kafka 代理，并配置我们希望它如何在主题之外来回传输数据。然后，我们必须通过指定主题来告诉它获取哪种数据。

一个*主题*本质上是我们正在读写的流的名字。当我们向 Kafka 生成数据时，我们必须指定我们要写哪个主题。同样，当我们创建一个消费者时，我们必须订阅至少一个主题。

注意，我们在这个 API 中没有任何修改数据的命令。我们所能做的就是按下一个`ProducerRecord`，并以`ConsumerRecords`的形式取回数据。

这些都很好，但是中间会发生什么呢？

# 这都是因为日志

Kafka 的基本数据结构是*日志*。

你对原木很熟悉，对吧？如果你想知道服务器上发生了什么，你可以查看系统日志。你不需要查询日志——你只需要从头到尾读一遍。

在拥有大量日志数据的服务器上，数据通常会被轮换，因此旧的日志会被丢弃，只留下您最感兴趣的最近事件。

卡夫卡数据也是这样。我们在 Kafka 中的流存储在循环日志文件中。(实际上，一个单独的主题将被分割成一堆日志文件，这取决于我们如何对主题进行分区。)

那么，我们的数据消费者如何知道它从哪里停止了呢？

它只是保存一个表示它在流中位置的偏移值。把这个当成书签。如果用户关闭并不得不从停止的地方继续读取，偏移量可以让用户恢复。

# 现在我们有了实时数据

现在我们有了实时流中的数据，我们可以实时地对其进行分析。我们下一步做什么的细节将不得不留给另一篇文章。

可以说，一旦我们有了流中的数据，我们现在就可以开始使用流处理器来转换数据、聚合数据，甚至查询数据。

# 它是功能性的

正如我们将看到的许多大数据系统的情况一样，Kafka 在其设计中使用了[计算的功能模型](https://medium.com/better-programming/why-everyone-should-learn-functional-programming-today-c96a5b10d27d)。

注意，Kafka 中的数据是不可变的。我们从来没有进入卡夫卡的数据，并修改它。我们所能做的就是插入和读取数据。即使这样，我们的阅读也仅限于顺序访问。

顺序存取很便宜，因为 Kafka 将数据一起存储在磁盘上。因此，即使每秒插入数百万个事件，它也能够提供对数据块的高效访问。

但是等一下。如果数据是不可变的，那么我们如何在 Kafka 中更新一个值呢？

很简单，我们再做一次插入。卡夫卡有一个关于信息的*键*的概念。如果我们按下同一个键两次，如果我们启用了一个名为*日志压缩*的设置，那么 Kafka 将确保删除同一个键的旧值。这都是 Kafka 自动完成的——我们从不手动设置值，只是推送更新的值。我们甚至可以推送空值来删除记录。

通过避免可变的数据结构，Kafka 允许我们的流系统扩展到荒谬的高度。

# 为什么不是数据仓库？

乍一看，流似乎是一个笨拙的解决方案。难道我们不能设计一个可以处理我们的写容量的数据库——类似于数据仓库——然后在处理时查询它吗？

在某些情况下，是的，数据仓库已经足够好了。对于某些类型的用途，它甚至可能是更可取的。如果我们知道我们不需要我们的数据是实时的，那么维护流媒体基础设施的成本可能会更高。

从数据仓库中处理数据的周转时间会很慢，会延迟几个小时甚至几天，但也许我们对每日报告感到满意。让我们称这些解决方案为*批处理*系统。

批处理更常见于提取-转换-加载(ETL)系统和商业智能部门。

# 批处理的局限性

在很多情况下，批处理不够好。

考虑银行处理交易的情况。我们收到了一系列交易，向我们展示了我们的客户如何实时使用他们的信用卡。

现在，假设我们想要检测数据中的欺诈交易。我们能用数据仓库系统做到这一点吗？

大概不会。如果我们的查询需要几个小时才能运行，那么等到我们知道交易是否是欺诈性的时就太晚了。

# 我能在哪里使用这个？

流媒体并不能解决所有问题。对于在现代世界中变得越来越相关的某些类型的问题，这些是关键的概念，但是并不是每个人都在构建实时系统。

但是卡夫卡在这些小众案例之外还是有用的。

消息代理在扩展任何系统时都至关重要。面向服务的架构的一个常见模式是允许服务通过 Kafka 相互对话，这与 HTTP 调用相反。从 Kafka 读取数据本质上是异步的，这使得它非常适合一般的消息传递层。

考虑使用 Kafka 或类似的消息代理(如 Amazon 的 Kinesis)在任何时候传输数据，只要您有大量可以异步处理的写操作。

如果您是一家小公司，消息传递层可能看起来有些多余，但是如果您有任何发展的意图，那么在成长的烦恼开始产生伤害之前部署这个解决方案将会带来回报。

# 结论

正如我们所看到的，函数式编程概念已经进入了大数据世界的基础设施组件。Kafka 是一个很好的例子，它使用了一个非常基本的不可变数据结构——日志——并取得了很好的效果。

但这些组件不仅仅是尖端机器学习公司中使用的利基系统。它们是基本工具，可以为每个人提供扩展优势，从大型企业到快速成长的初创公司。