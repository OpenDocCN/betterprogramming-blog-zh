# 未经检查的人工智能可以反映人类的行为

> 原文：<https://betterprogramming.pub/unchecked-ai-can-mirror-human-behavior-2ce1ce76f914>

## 怎样才能尽力减轻偏见？

![](img/7416d77fa3ac91764b0b326c522af170.png)

凯利·西克玛在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片。

人们对人工智能(AI)有着无尽的兴趣。根据斯坦福大学发布的《人工智能指数 2019 年度报告》，1998 年至 2018 年间，经过同行评审的人工智能论文数量增长了 300%以上。在 2018 年年中至 2019 年年中，斯坦福大学以人为本的人工智能研究所(AI Institute)确定的超过 3，600 篇关于道德使用人工智能的全球新闻文章中，人工智能的可能框架和指南、人脸识别应用程序的使用、数据隐私、大技术的作用和算法偏见等主题占据了主导地位。

这突出了理解偏倚如何进入数据集以及在努力减少偏倚时提高认识的重要性。人工智能击中了人类最大的痛处:它揭示了先入为主的观念如何影响善意应用的结果。虽然从来没有更多的数据可用于做出合格的决策，但不能保证这些决策可以成功地投入使用。出现了多重故障(此列表并不详尽):

*   2015 年，谷歌面临另一场争议，其[图片服务](https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/)将黑人照片标记为大猩猩。谷歌是如何修复这个问题的？其实并没有。相反，根据 2018 年《连线》的一份报告，它屏蔽了所有标记为“大猩猩”的图像。
*   2016 年，在运行了不到 24 小时后，[微软的人工智能聊天机器人](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)被关闭，因为 Twitter 用户将它训练成了一个侮辱性的纳粹爱好者。
*   2019 年，巴西的研究人员[发现](https://www.washingtonpost.com/news/the-intersect/wp/2016/08/10/study-image-results-for-the-google-search-ugly-woman-are-disproportionately-black/)“在谷歌上搜索‘美女’的图片，返回白人的图像的可能性远远大于黑人和亚洲人，搜索‘丑女’的图片，返回黑人和亚洲人的图像的可能性大于白人。”

鉴于这些事件，我们必须问自己如何才能尽最大努力减少偏见。我们必须意识到关于数据集有几个偏差来源，但是我们如何处理这是人类的决定。如果我们基于人类的错误判断来训练模型，它会很快变得危险。

# 5 种最常见的偏见

如果我们从统计学的角度来看这个话题，有五种方式可以让偏见渗透到结果中。

## **确认偏差**

确认偏见是一种倾向，即寻找、解读、偏爱和回顾那些肯定或支持个人早期信念或价值观的数据。因此，确认偏差是一种强有力的认知偏差，它会错误地做出基于证据的决策，从而对社会的正常运转产生重要影响。

这方面的一个例子是当你有选择地记住信息或对给你的信息做出有偏见的解释。[研究表明](https://link.springer.com/content/pdf/10.3758%2FBF03196318.pdf)我们甚至可以被操纵去记忆虚假的童年记忆。这表明当人们以一种有偏见的方式分析数据时，他们有时甚至不会注意到(另一种符合这一类别的心理现象是[一厢情愿](https://www.logicallyfallacious.com/logicalfallacies/Wishful-Thinking))。

## **选择偏差**

[选择偏倚](https://www.statisticshowto.com/what-is-bias/)是通过选择没有实现适当随机化的个体、群体或数据进行分析而引入的偏倚，从而确保获得的样本不能代表待分析的人群。术语“选择偏倚”通常是指由抽样方法引起的统计分析的偏倚。因此，必须考虑选择偏差。这项研究的一些结论可能是错误的。

## **异常值**

异常值是极端的数据值。例如，一位 110 岁的客户或储蓄账户中有 1000 万美元的消费者。您可以通过仔细检查数据来识别异常值，尤其是在分布值时。因为异常值是极端的数据值，所以根据计算出的“平均值”来决定是危险的换句话说，极端行为会对所谓的平均水平产生重大影响。你的结论必须基于中位数(平均值)才能得到准确的结果。

## **过拟合和欠拟合**

拟合不足意味着模型对现实给出了过于简单的描述。过度拟合是相反的(即过度复杂的图片)。过度拟合有导致特定假设被视为真理的风险，而实际情况并非如此。

如何抵消这种偏见？最直接的方法是询问模型是如何验证的。如果您收到一个有点呆滞的表情作为反应，很有可能分析结果是所谓的未验证结果，因此，可能不适用于整个数据库。总是询问数据分析师他们是否做过训练或测试样本。如果答案是否定的，那么分析结果很可能不适用于所有客户。

## **混杂变量**

基本上，当额外的因素影响了你没有考虑的变量时，就会发生这种情况。在实验中，自变量通常会影响因变量。举个例子，如果你想调查锻炼的需求是否会导致体重下降，那么健身的需求就是你的自变量，体重下降就是你的因变量。

干扰因素是影响你的因变量的所有其他因素。它们是对你的因变量有潜在影响的附加因素。加重因素会导致两个主要问题:增加的[方差](https://www.statisticshowto.com/probability-and-statistics/variance/)和引入的[偏差](https://www.statisticshowto.com/what-is-bias/)。

必须确认从研究和分析结果中得出的结论没有受到扭曲的影响。揭露有偏见的结果不仅仅是相关分析师的责任。根据正确的数据得出有效的结论是所有直接相关人员(包括市场参与者和分析师)的共同责任。

# 没有人类就没有人工智能

因此，当我们处理偏见、错误的应用可能性和错误的结果时，我们总是要问自己它们是如何产生的。社会规范、恐惧和社会变迁远在任何人工智能计算和技术进步之前就存在了。绝大多数人工智能应用都是在没有恶意的情况下开发的。但同样清楚的是，许多应用具有对社会有害的影响。

正是因为这个原因，绝对有必要将人工智能不仅作为一种编程语言来研究，而且作为一个概念来研究它所有的复杂性及其对整个社会的重大影响。

没有人类的 AI 是不可能的。因此，其他科学学科，如心理学、历史学、哲学、伦理学、社会学、政治学、健康和神经科学对于减轻偏见至关重要。减轻偏见不仅是数据科学的当务之急，对其他学科也是如此。

这一领域的许多出版物使我对这个主题的广泛讨论充满希望，并将继续下去。这不应该是指责别人，而是从所犯的错误中学习，消除可能的错误来源，然后一起开发未来的人工智能应用，这对我们所有人都有好处。