# 用 Python 向数据生成器发送多个文件

> 原文：<https://betterprogramming.pub/multiple-files-to-data-generator-less-ram-fast-training-5ddfff3e4cbd>

## 在 Colab 中用更少的内存进行更快的训练

![](img/c2a5607bc664ed9c5006d40205e28bf4.png)

弗兰基·查马基在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片。

在当前的深度学习时代，数据是最大的问题。如果数据量很小，那么对于训练深度神经网络来说，这显然是一个很大的问题。即使数据量巨大，对于没有足够资源的人来说，仍然是个大问题。

大量数据带来的问题是没有足够的资源将数据加载到内存中。我们大多数人在 [Colab](https://colab.research.google.com/) 中训练我们的模型，它最初只给我们 12 GB 的内存(有时是 25/35 GB)。最近，我面临着同样的问题，仅仅加载一半的数据就使我的运行时崩溃。我们大多数人都知道显而易见的解决方案:使用 Python 生成器。

Python 生成器是一个不会将整个输入加载到内存中的函数。它只需要我们当时需要的数据量。对于深度学习，我们不需要某个时刻的所有数据。我们需要特定时间的`batch_size`数量的数据。所以它实际上返回了一个迭代器，它给出了我们需要的数据。

假设您有一个大小为 80 GB 的`CSV`文件，那么您不能将整个数据集加载到内存中。在 Python 中，我们有一个可以打开 CSV 文件并逐行读取的 lib。我们将用它来制造我们的发电机:

该代码块将打印 CSV 文件的每一行。但与我们的传统方法不同，它不会将整个数据加载到内存中。它会一行一行地读，然后把那一行扔给我们使用。

我知道有些人此刻正在无聊地阅读。每个人都知道 Python generator，但这不是本文的全部内容。上述解决方案适用于单个文件。但是，当您有三个、四个或更多 CSV 文件时，您会怎么做呢？

再次使用 Python 生成器？

这就是我之前面临的问题。以我小小的 Python 经验，我实现了一些功能，今天就和大家分享一下:

这是我在最近的研究中使用的代码，用来合并所有四个文件(总共 47 GB ),并只获得数据的`batch_size`( 16，32，64，128，...).不要被代码的庞大所淹没。我会一步步解释。

在这部分代码中，我将所有文件名作为参数和`batch_size`传递。我用 [pandas](https://pandas.pydata.org/) ( `pd`)读取 CSV，但与传统方式不同的是，我在`read_csv`上传递了`chunksize`参数。然后，它将返回一个 reader 对象，而不是一个数据帧。我将所有的 reader 对象追加到一个列表中并返回。我将在后面的代码中使用它。

在这一部分，所有的数据帧对象都被发送。在我的例子中，我必须使用`id`作为主值，将所有数据框对象合并到一个数据框中。Pandas 已经有了一个内置的功能(`pd.merge`)。在此函数中，您需要发送左右数据框、连接类型以及需要用作主要变量的变量。

现在，这是我在`model fit`函数中使用的主生成器函数。起初，我使用`get_all`函数来获取所有的读者。现在该函数将无限期运行。它需要多少数据将取决于模型，而不是数据集中的数据量或生成器函数。所以它会无限运行，循环服务于数据。

`reader.get_chunk()`将返回`chunksize`的数据帧。我们拥有来自所有 reader 对象的所有块，并使用之前的`merge_all`函数合并它们。然后，我在代码末尾使用`yield`关键字将这些值分开并返回。

我对这部分代码做了什么？深度学习模型不会只对数据进行一次训练。它重复读取数据—传统上每个时期读取一次。如果我们使用来自`numpy`数组的数据，它会自己管理数据的重复。但是由于我们使用的是发电机，我们必须手动操作。所以在第一行，我跟踪已经读取的数据数量，在`if`条件下，我检查是否已经到达数据的末尾。如果是，那么我只是再次获取所有的 reader 对象，并将`cnt`重置为 0。因此，它将再次从文件的开头开始为我提供数据。

这个话题让我吃了不少苦头，在谷歌搜索了很多次后，我想出了这个解决方案。我希望这篇文章能节省你的时间和网络带宽。