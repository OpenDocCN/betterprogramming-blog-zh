<html>
<head>
<title>Build a Natural Language Classifier With Bert and Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Bert和Tensorflow构建自然语言分类器</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41?source=collection_archive---------3-----------------------#2020-12-23">https://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41?source=collection_archive---------3-----------------------#2020-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bbf7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将先进的变压器模型应用于您的语言问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/026c4f3f3d8bad0c059163993fadc671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SG5plzIHxE0tn5ZK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔尔·那仁在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="257d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像伯特和GPT-3这样的高性能转换器模型正在将大量以前琐碎的基于语言的任务转换成点击的工作，节省了大量时间。</p><p id="d1fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在大多数行业，语言优化的最新浪潮才刚刚开始——迈出他们的第一小步。但是这些幼苗分布很广，而且发芽很快。</p><p id="93b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种采用在很大程度上得益于难以置信的低准入门槛。如果你知道TensorFlow或PyTorch的基础知识，并花一点时间来掌握这个<code class="fe lv lw lx ly b">Transformers</code>库——你已经成功了一半。</p><p id="df3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了<code class="fe lv lw lx ly b">Transformers</code>库，只需要三行代码就可以初始化一个尖端的ML模型——一个由谷歌、脸书和OpenAI等公司花费数十亿美元研究构建的模型。</p><p id="f184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将带您了解使用Google的BERT构建一个利用transformers强大功能的分类模型的步骤。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="cf15" class="md me it ly b gy mf mg l mh mi"><strong class="ly iu">Transformers</strong><br/>- Finding Models<br/>- Initializing<br/>- Bert Inputs and Outputs</span><span id="3a52" class="md me it ly b gy mj mg l mh mi"><strong class="ly iu">Classification</strong><br/>- The Data<br/>- Tokenization<br/>- Data Prep<br/>- Train-Validation Split<br/>- Model Definition<br/>- Train</span><span id="7ef6" class="md me it ly b gy mj mg l mh mi"><strong class="ly iu">Results</strong></span></pre><p id="4c67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你更喜欢视频，我在这里介绍同样的过程:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mk ml l"/></div></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="8be2" class="mt me it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">变形金刚(电影名)</h1><h2 id="e08a" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">寻找模型</h2><p id="6052" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们将使用BERT，这可能是最著名的变压器架构。</p><p id="53f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了弄清楚我们需要使用BERT做什么，我们前往HuggingFace模型页面(HuggingFace构建了Transformer框架)。</p><p id="289a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦到了那里，我们会在头版上同时找到<code class="fe lv lw lx ly b">bert-base-cased</code>和<code class="fe lv lw lx ly b">bert-base-uncased</code>。<code class="fe lv lw lx ly b">cased</code>意味着模型区分大写和小写字符，而<code class="fe lv lw lx ly b">uncased</code>不区分。</p><h2 id="4983" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">初始化</h2><p id="b8ef" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">如果我们点击模型，我们会发现更多具体的细节。在这个页面上，我们可以看到初始化模型的代码。因为我们使用TensorFlow，所以我们的代码将使用<code class="fe lv lw lx ly b">TFAutoTokenizer</code>和<code class="fe lv lw lx ly b">TFAutoModel</code>分别代替<code class="fe lv lw lx ly b">AutoTokenizer</code>和<code class="fe lv lw lx ly b">AutoModel</code>:</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="41b9" class="md me it ly b gy mf mg l mh mi">from transformers import TFAutoTokenizer, TFAutoModel<br/>tokenizer = TFAutoTokenizer.from_pretrained("bert-base-cased")<br/>bert = TFAutoModel.from_pretrained("bert-base-cased")</span></pre><h2 id="274c" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">Bert输入和输出</h2><p id="73d0" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">当将文本数据输入到我们的模型中时，需要注意一些事情。首先，我们必须使用<code class="fe lv lw lx ly b">tokenizer.encode_plus(...)</code>将我们的文本转换成输入id和注意力屏蔽张量(稍后会详细介绍)。</p><p id="ab59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT希望这两个张量都作为输入。一个映射到<code class="fe lv lw lx ly b">"input_ids"</code>，另一个映射到<code class="fe lv lw lx ly b">"attention_mask"</code>。</p><p id="1de5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一端，BERT默认输出两个张量(更多可用)。那些是<code class="fe lv lw lx ly b">"last_hidden_state"</code>和<code class="fe lv lw lx ly b">"pooler_output"</code>。</p><p id="dee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">pooler输出只是最后一个隐藏状态，由线性层和Tanh激活函数稍作进一步处理，这也将它的维度从3D(最后一个隐藏状态)减少到2D (pooler输出)。</p><p id="261a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">稍后，我们将消耗最后一个隐藏状态张量，并丢弃pooler输出。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="dc93" class="mt me it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">分类</h1><h2 id="12bd" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">数据</h2><p id="fee5" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" rel="noopener ugc nofollow" target="_blank">对电影评论的情感分析</a>是一个有趣的数据集(不足为奇)，用于训练和测试情感分析模型。我们可以以编程方式下载并提取它，就像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><p id="5677" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们把它读成一首<code class="fe lv lw lx ly b">Pandas DataFrame</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><p id="de76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们快速看一下数据:</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="f64e" class="md me it ly b gy mf mg l mh mi">df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/bc58f45810dc05d905f518ba0442ea35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WlUr9DbfztVEOv-v6rLMAw.png"/></div></div></figure><p id="ce0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集充满了这些重复，或短语片段。我们可以通过键入以下命令来删除这些内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><h2 id="2bbf" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">标记化</h2><p id="420b" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">在<code class="fe lv lw lx ly b">text</code>列中有我们的文本数据，我们现在需要对其进行标记。我们将使用BERT记号赋予器，因为我们稍后将使用BERT转换器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><p id="9615" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们首先导入transformers库，并为使用的<code class="fe lv lw lx ly b">bert-base-cased</code>模型初始化一个标记器。型号列表可在<a class="ae ky" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">这里</a>找到。然后我们定义一个处理标记化的函数<code class="fe lv lw lx ly b">tokenize</code>。</p><p id="e5c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用BERT分词器的<code class="fe lv lw lx ly b">encode_plus</code>方法将一个句子转换成<code class="fe lv lw lx ly b">input_ids</code>和<code class="fe lv lw lx ly b">attention_mask</code>张量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/eca4808f858644b5334324985d2bb1d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5ieKCJYkK8itKjs6KColg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd od"> Xids </strong>和<strong class="bd od"> Xmask </strong>分别是我们完整的<strong class="bd od"> input_ids </strong>和<strong class="bd od"> attention_mask </strong>张量。</p></figure><p id="bca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入id是与特定单词唯一关联的整数列表。</p><p id="24ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意掩码是与输入id数组中的id相对应的1和0的列表，BERT读取该列表，并仅将注意应用于与注意掩码值1相对应的id。这允许我们避免关注填充标记。</p><p id="2b99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的<code class="fe lv lw lx ly b">encode_plus</code>论点是:</p><ul class=""><li id="ccc4" class="oe of it lb b lc ld lf lg li og lm oh lq oi lu oj ok ol om bi translated">我们的<code class="fe lv lw lx ly b">sentence</code>。这只是一个代表一条tweet的字符串。</li><li id="845b" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">我们的编码输出的<code class="fe lv lw lx ly b">max_length</code>。我们使用值<code class="fe lv lw lx ly b">32</code>,这意味着每个输出张量的长度为32。</li><li id="d08a" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">我们用<code class="fe lv lw lx ly b">truncation=True</code>剪切长度超过32个标记的序列。</li><li id="c33e" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">对于短于32个标记的序列，我们使用<code class="fe lv lw lx ly b">padding='max_length'</code>用零填充它们，长度达到32。</li><li id="3b5d" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">BERT使用几个特殊的标记来标记序列的开始/结束，用于填充、未知单词和屏蔽单词。我们添加那些使用<code class="fe lv lw lx ly b">add_special_tokens=True</code>的。</li><li id="fec7" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">伯特也接受两个输入，即<code class="fe lv lw lx ly b">input_ids</code>和<code class="fe lv lw lx ly b">attention_mask</code>。我们用<code class="fe lv lw lx ly b">return_attention_mask=True</code>提取注意力面具。</li><li id="7a9c" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">默认情况下，记号赋予器将返回一个记号类型IDs tensor——这是我们不需要的，所以我们使用<code class="fe lv lw lx ly b">return_token_type_ids=False</code>。</li><li id="c646" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">最后，我们使用TensorFlow，所以我们使用<code class="fe lv lw lx ly b">return_tensors='tf'</code>返回TensorFlow张量。如果使用PyTorch，使用<code class="fe lv lw lx ly b">return_tensors='pt'</code>。</li></ul><p id="8997" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们返回两个张量。我们预先初始化我们的两个<code class="fe lv lw lx ly b">Xids</code>和<code class="fe lv lw lx ly b">Xmask</code>数组，然后使用一个简单的for循环用我们编码的张量填充它们——将<code class="fe lv lw lx ly b">tokenizer</code>应用到我们数据集的每个句子。</p><h2 id="ab69" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">数据准备</h2><p id="2c28" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们现在需要为训练准备数据。为此，我们将对我们的目标标签进行一次性编码，为每个样本创建一个简单的双输出向量，其中<code class="fe lv lw lx ly b">[1, 0]</code>表示负面情绪，<code class="fe lv lw lx ly b">[0, 1]</code>表示正面情绪。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/52e609c5420d977656850ff5a92150cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HxkzFesjg4kmUorO.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一键编码。前(上)后(下)。</p></figure><p id="2efc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个过程可能需要一些时间。我喜欢保存编码后的数组，这样如果有任何问题或者为了将来的测试，我们可以从这里开始。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><p id="14cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了所有编码的数组，我们将它们加载到一个TensorFlow dataset对象中。使用数据集，我们可以轻松地对数据进行重组、重排和批处理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><h2 id="4c3f" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">训练-验证分割</h2><p id="22e7" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">训练模型之前的最后一步是将数据集分成训练集、验证集和(可选的)测试集。在这里，我们将坚持简单的90–10训练验证分割。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><h2 id="217b" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">模型定义</h2><p id="274d" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们的数据现在已经准备好了，我们可以定义我们的模型架构了。我们将使用伯特，其次是LSTM层，和一些简单的神经网络层。伯特后面的最后几层是我们的分类器。</p><p id="3ab9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的分类器消耗来自BERT的输出隐藏状态张量——使用它们来预测我们看到的是积极情绪还是消极情绪。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure><h2 id="99ac" class="md me it bd mu nk nl dn my nm nn dp nc li no np ne lm nq nr ng lq ns nt ni nu bi translated">培养</h2><p id="5d4f" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">我们现在可以训练我们的模型了。首先，我们设置优化器(Adam)、损失函数和准确性度量。然后，我们编译模型，训练！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ml l"/></div></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="c650" class="mt me it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">结果</h1><p id="55e9" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">第一，这种模式需要长时间的训练。我发现更方便的做法是对数据集进行子采样，找出哪些有效(但主要是哪些无效)，并从那里继续前进。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/1bb22d01ca68e8b43d87b894c0c0823b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AaGoHtXA1NJhPC_fCfdg-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终模型指标。</p></figure><p id="ca1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终模型的训练集准确率为89.6%，验证集准确率为97.16%！对于难以阅读的推文，经过最少的预处理，这是一个令人印象深刻的结果。</p><p id="3e49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将解释保持在最低限度，目的是展示预处理数据和训练模型的完整过程——没有太多的膨胀。</p><p id="c619" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你确实想了解更多关于变形金刚的一般知识，或者我们在这里采取的具体步骤，我已经在几篇文章中更深入地介绍了变形金刚:<a class="ae ky" href="https://towardsdatascience.com/evolution-of-natural-language-processing-8e4532211cfe" rel="noopener" target="_blank">这里</a>，<a class="ae ky" href="https://towardsdatascience.com/how-transformers-work-6cb4629506df" rel="noopener" target="_blank">这里</a>，以及<a class="ae ky" href="https://towardsdatascience.com/tensorflow-and-transformers-df6fceaf57cc" rel="noopener" target="_blank">这里</a>。</p><p id="62b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae ky" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或者在下面的评论中告诉我。如果你想要更多这样的内容，我也会在YouTube上发布。</p><p id="1042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="eaba" class="mt me it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">来源</h1><p id="1666" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div></div>    
</body>
</html>