<html>
<head>
<title>High Level Overview of Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark的高级概述</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/high-level-overview-of-apache-spark-c225a0a162e9?source=collection_archive---------0-----------------------#2019-04-22">https://betterprogramming.pub/high-level-overview-of-apache-spark-c225a0a162e9?source=collection_archive---------0-----------------------#2019-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="17b8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是火花？让我们看看引擎盖下面</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b12946d9ae4a5f55053111f518d32e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tP-dw4Oj_42BYbkdtYbjMA.png"/></div></div></figure><p id="97b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我的上一篇文章中，我们介绍了一个问题:大量的、永无止境的数据流，以及它的解决方案:<a class="ae lq" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>。在第二部分中，我们将关注Spark的内部架构和数据结构。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><blockquote class="ly"><p id="2a8e" class="lz ma it bd mb mc md me mf mg mh lp dk translated"><em class="mi">在拓荒时代，他们用牛来拉重物，当一头牛拉不动一根木头时，他们就不去养一头更大的牛。我们不应该尝试更大的计算机，而是更多的计算机系统——格蕾丝·赫柏</em></p></blockquote><p id="9ecf" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">随着数据规模以惊人的速度快速增长，我们需要一种方法来快速处理潜在的数Pb数据，而我们根本无法让一台计算机以合理的速度处理这么多数据。这个问题可以通过创建一个机器集群来为您执行工作来解决，但是这些机器如何协同工作来解决常见的问题呢？</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="43df" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">遇见火花</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/08378f31e55f3c6b927e616517b16b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HwnSXuF99iFBtNYT"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">照片由<a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae lq" href="https://unsplash.com/@jeztimms?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jez Timms </a>拍摄</p></figure><p id="8b64" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Spark是用于大规模数据处理的集群计算框架。Spark为其统一计算引擎提供了一套三种语言的库(<a class="ae lq" href="https://www.java.com/" rel="noopener ugc nofollow" target="_blank"> Java </a>、<a class="ae lq" href="https://www.scala-lang.org/" rel="noopener ugc nofollow" target="_blank"> Scala </a>、<a class="ae lq" href="http://python.org" rel="noopener ugc nofollow" target="_blank"> Python </a>)。这个定义实际上意味着什么？</p><p id="6df3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">统一</strong> —有了Spark，就不需要从多个API或系统中拼凑出一个应用。Spark为您提供了足够的内置API来完成这项工作。</p><p id="dba6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">计算引擎</strong> — Spark处理来自各种文件系统的数据加载，并在其上运行计算，但不会永久存储任何数据本身。Spark完全在内存中运行，提供无与伦比的性能和速度。</p><p id="ffcf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">库</strong> — Spark由一系列为数据科学任务构建的库组成。Spark包括用于SQL ( <a class="ae lq" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank"> Spark SQL </a>)、机器学习(<a class="ae lq" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> MLlib </a>)、流处理(Spark流和结构化流)和图形分析(<a class="ae lq" href="https://spark.apache.org/graphx/" rel="noopener ugc nofollow" target="_blank"> GraphX </a>)的库。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="292a" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">Spark应用程序</h1><p id="d8c7" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">每个Spark应用程序都由一个<strong class="kw iu">驱动程序</strong>和一组分布式工作进程(<strong class="kw iu">执行器</strong>)组成。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="620d" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">火花驱动器</h1><p id="092f" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">驱动程序运行我们的应用程序的<code class="fe nq nr ns nt b">main()</code>方法，并且在那里创建<code class="fe nq nr ns nt b">SparkContext</code>。火花驱动器有以下职责:</p><ul class=""><li id="4417" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">在集群中的一个节点上运行，或者在一个客户机上运行，并使用集群管理器调度作业执行。</li><li id="9b9a" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">响应用户的程序或输入。</li><li id="fe18" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">在执行者之间分析、安排和分配工作。</li><li id="5011" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">存储关于正在运行的应用程序的元数据，并方便地在webUI中公开它。</li></ul></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="ba5b" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">火花执行者</h1><p id="ace0" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">执行器是负责任务执行的分布式进程。每个Spark应用程序都有自己的执行器集，这些执行器在单个Spark应用程序的生命周期中保持活动状态。</p><ul class=""><li id="3b20" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">执行器执行Spark作业的所有数据处理。</li><li id="724a" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">将结果存储在内存中，仅在驱动程序特别指示时才保存到磁盘。</li><li id="5d34" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">完成后将结果返回给驱动程序。</li><li id="588a" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">每个节点可以有从每个节点一个执行器到每个内核一个执行器的任意位置</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9bd300b28fe62fe044dbbbcaf2269e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*GZG2aogNS8Jg14jOM2rjmQ.png"/></div></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="79b8" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">Spark的应用工作流程</h1><p id="0286" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">当您向Spark提交一个任务进行处理时，会有很多工作在幕后进行。</p><ol class=""><li id="000a" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp oj oa ob oc bi translated">我们的独立应用程序被启动，并初始化它的<code class="fe nq nr ns nt b">SparkContext</code>。一个app只有有了<code class="fe nq nr ns nt b">SparkContext</code>才能被称为驱动。</li><li id="e39d" class="nu nv it kw b kx od la oe ld of lh og ll oh lp oj oa ob oc bi translated">我们的驱动程序向集群管理器请求资源来启动它的执行器。</li><li id="4a98" class="nu nv it kw b kx od la oe ld of lh og ll oh lp oj oa ob oc bi translated">集群管理器启动执行器。</li><li id="5809" class="nu nv it kw b kx od la oe ld of lh og ll oh lp oj oa ob oc bi translated">我们的司机运行我们实际的火花代码。</li><li id="0a3d" class="nu nv it kw b kx od la oe ld of lh og ll oh lp oj oa ob oc bi translated">执行器运行任务，并将结果发送回驱动程序。</li><li id="8435" class="nu nv it kw b kx od la oe ld of lh og ll oh lp oj oa ob oc bi translated"><code class="fe nq nr ns nt b">SparkContext</code>被停止，所有执行器被关闭，将资源返回集群。</li></ol></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="62af" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">最高温度，重新审视</h1><p id="710f" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">让我们更深入地看看我们在<a class="ae lq" href="https://hackernoon.com/why-we-need-apache-spark-51c8a57aa57a" rel="noopener ugc nofollow" target="_blank">第一部分</a>中写的Spark job，找出各个国家的最高温度。这个抽象隐藏了很多设置代码，包括我们的<code class="fe nq nr ns nt b">SparkContext</code>的初始化。让我们填补空白:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/c70d457a25dfe08a7bc1b57cf6ce092d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8COf1mt_AxIt66ft83z-yg.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">最高温度火花设置</p></figure><p id="51ff" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">记住Spark是一个框架，在这里是用Java实现的。直到第16行，Spark才需要做任何工作。当然，我们初始化了我们的<code class="fe nq nr ns nt b">SparkContext</code>，但是将数据加载到RDD是需要发送给我们的执行器的第一部分代码。</p><p id="4a69" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">到现在为止，你可能已经多次看到“RDD”这个词，是时候给它下定义了。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="f0ea" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">Spark架构概述</h1><p id="4d7f" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">Spark有一个定义良好的分层架构，具有松散耦合的组件，基于两个主要的抽象:</p><ul class=""><li id="be6e" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">弹性分布式数据集</li><li id="31ee" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">有向无环图</li></ul></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="2b2b" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">弹性分布式数据集</h1><p id="d23d" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">rdd本质上是Spark的构建模块——一切都是由它们组成的。甚至Sparks更高级别的API(数据帧、数据集)也是由幕后的rdd组成的。弹性分布式数据集意味着什么？</p><ul class=""><li id="fecb" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">弹性——由于Spark运行在一个机器集群上，硬件故障导致的数据丢失是一个非常现实的问题，因此rdd具有容错能力，可以在出现故障时重建自身</li><li id="1b81" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">分布式—单个RDD存储在集群中的一系列不同节点上，不属于单个源(也不存在单点故障)。这样，我们的集群可以在我们的RDD上并行运行。</li><li id="a80c" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">数据集—值的集合(您可能已经知道了)。</li></ul><p id="0189" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们在Spark中使用的所有数据都将存储在某种形式的RDD中——因此完全理解它们是非常必要的。</p><p id="c6d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Spark提供了一系列构建在rdd之上的“高级”API，旨在抽象掉复杂性，即数据帧和数据集。Scala和Python中的spark-submit和Spark shell侧重于读取-评估-打印循环(REPLs ),面向经常希望对数据集进行重复分析的数据科学家。RDD仍然需要理解，因为它是Spark中所有数据的底层结构。</p><p id="210f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">RDD通俗地等同于“分布式数据结构”。一个<code class="fe nq nr ns nt b">JavaRDD&lt;String&gt;</code>实际上就是一个<code class="fe nq nr ns nt b">List&lt;String&gt;</code>,分布在我们集群中的每个节点上，每个节点获得我们列表中的几个不同的块。有了Spark，我们需要始终在分布式环境中思考。</p><p id="5607" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">rdd的工作方式是将数据分成一系列分区，存储在每个执行器节点上。然后，每个节点将只在自己的分区上执行工作。这就是Spark如此强大的原因——如果一个执行程序死亡，或者一个任务失败，Spark可以从原始源代码中重建它需要的分区，并重新提交任务来完成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/26b9bda6aefc1f63f49e5d042b6dffcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aXGUcCy3qHD3U66COrTTA.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">星火RDD在遗嘱执行人之间分配</p></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="8bba" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">RDD行动</h1><p id="49d4" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">rdd是不可变的，这意味着一旦它们被创建，就不能以任何方式被修改；他们只能被改造。转变rdd的概念是Spark的核心，Spark作业可以被认为是这些步骤的任意组合:</p><ul class=""><li id="ef53" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">将数据加载到RDD。</li><li id="b42a" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">改造RDD。</li><li id="5b9d" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">在RDD上执行操作。</li></ul><p id="dda6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">事实上，我编写的每一个Spark作业都完全由这些类型的任务组成，带有香草味的Java。</p><p id="9e3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Spark定义了一组使用rdd的API，这些API可以分为两大类——转换和动作。</p><ul class=""><li id="eedf" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">转换从现有的RDD创建一个新的。</li><li id="53fa" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">在对驱动程序的RDD运行计算后，操作会向驱动程序返回一个或多个值。</li></ul><p id="9d12" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，映射函数<code class="fe nq nr ns nt b">weatherData.map()</code>，是一个通过函数传递RDD的每个元素的转换。</p><p id="c47b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“Reduce”是一个RDD动作，它使用某个函数聚合RDD的所有元素，并将最终结果返回给驱动程序。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="5630" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">懒惰评估</h1><blockquote class="om on oo"><p id="ad74" class="ku kv op kw b kx ky ju kz la lb jx lc oq le lf lg or li lj lk os lm ln lo lp im bi translated">“我选择一个懒惰的人去做艰苦的工作。因为一个懒惰的人会找到一个简单的方法去做。——比尔·盖茨”</p></blockquote><p id="dda9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Spark中的所有转换都是懒惰的。这意味着，当我们告诉Spark通过现有RDD的转换创建一个RDD时，它不会生成该数据集，直到对它或它的一个子对象执行特定的操作。Spark将执行转换和触发转换的动作。这使得Spark的运行效率大大提高。</p><p id="4bf8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们重新检查前面Spark例子中的函数声明，以确定哪些函数是动作，哪些是转换:</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="6f97" class="ox mp it nt b gy oy oz l pa pb">16: JavaRDD&lt;String&gt; weatherData = sc.textFile(inputPath);</span></pre><p id="6e31" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第16行既不是动作也不是转换——它是我们的<code class="fe nq nr ns nt b">JavaSparkContext</code>的<code class="fe nq nr ns nt b">sc</code>的函数。</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="63cb" class="ox mp it nt b gy oy oz l pa pb">17: JavaPairRDD&lt;String, Integer&gt; tempsByCountry = weatherData.mapToPair(new Func.....</span></pre><p id="fbc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">17号线是对<code class="fe nq nr ns nt b">weatherData</code> RDD的改造。在其中，我们将<code class="fe nq nr ns nt b">weatherData</code>的每一行映射到一对由(城市，温度)组成的行。</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="942a" class="ox mp it nt b gy oy oz l pa pb">26: JavaPairRDD&lt;String, Integer&gt; maxTempByCountry = tempsByCountry.reduce(new Func....</span></pre><p id="88a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第26行也是一个转换，因为我们正在迭代键值对。这是<code class="fe nq nr ns nt b">tempsByCountry</code>的一个转变，我们将每个城市的温度降低到有记录以来的最高温度。</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="0dee" class="ox mp it nt b gy oy oz l pa pb">31: maxTempByCountry.saveAsHadoopFile(destPath, String.class, Integer.class, TextOutputFormat.class);</span></pre><p id="cbf6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，在第31行，我们触发了一个火花动作；将RDD保存到我们的文件系统中。因为Spark订阅了惰性执行模型，所以直到这一行Spark才生成<code class="fe nq nr ns nt b">weatherData</code>、<code class="fe nq nr ns nt b">tempsByCountry</code>和<code class="fe nq nr ns nt b">maxTempsByCountry</code>，最后保存我们的结果。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="5cec" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">有向无环图</h1><p id="29c6" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">每当在RDD上执行一个动作时，Spark都会创建一个DAG——一个没有有向循环的有限有向图(否则我们的作业将永远运行下去)。</p><p id="8b99" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请记住，一个图只不过是一系列相连的顶点和边，这个图也不例外。DAG中的每个顶点是火花函数；在RDD上执行的一些操作(<code class="fe nq nr ns nt b">map</code>、<code class="fe nq nr ns nt b">mapToPair</code>、<code class="fe nq nr ns nt b">reduceByKey</code>等)。</p><p id="d4bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<code class="fe nq nr ns nt b">MapReduce</code>中，DAG由两个顶点组成:Map → Reduce。</p><p id="3f51" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们上面的<code class="fe nq nr ns nt b">MaxTemperatureByCountry</code>示例中，DAG稍微复杂一些:</p><p id="18fa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe nq nr ns nt b">parallelize → map → mapToPair → reduce → saveAsHadoopFile</code></p><p id="3f57" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">DAG允许Spark优化其执行计划，并最大限度地减少洗牌。我们将在后面的帖子中更深入地讨论DAG，因为它超出了Spark概述的范围。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="6d7c" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">评估循环</h1><p id="721a" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">用我们的新词汇，让我们重新检查我在<a class="ae lq" href="https://hackernoon.com/why-we-need-apache-spark-51c8a57aa57a" rel="noopener ugc nofollow" target="_blank">第一部分</a>中定义的<code class="fe nq nr ns nt b">MapReduce</code>的问题，引用如下:</p><blockquote class="om on oo"><p id="381b" class="ku kv op kw b kx ky ju kz la lb jx lc oq le lf lg or li lj lk os lm ln lo lp im bi translated">MapReduce擅长批量数据处理，但是在重复分析和小反馈循环方面就落后了。在计算之间重复使用数据的唯一方法是将其写入外部存储系统(类似HDFS)</p></blockquote><p id="7d93" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在计算之间重复使用数据？听起来像一个RDD，可以在上面执行多种操作！假设我们有一个文件<code class="fe nq nr ns nt b">data.txt</code>，想要完成两个计算:</p><ul class=""><li id="6e78" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp nz oa ob oc bi translated">文件中所有行的总长度。</li><li id="ca92" class="nu nv it kw b kx od la oe ld of lh og ll oh lp nz oa ob oc bi translated">文件中最长一行的长度。</li></ul><p id="fad8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<code class="fe nq nr ns nt b">MapReduce</code>中，每个任务都需要一个单独的任务或者一个奇特的<code class="fe nq nr ns nt b">MulitpleOutputFormat</code>实现。Spark只需简单的四个步骤即可轻松实现:</p><ol class=""><li id="be04" class="nu nv it kw b kx ky la lb ld nw lh nx ll ny lp oj oa ob oc bi translated">将<code class="fe nq nr ns nt b">data.txt</code> <em class="op"> </em>的内容加载到一个RDD中。</li></ol><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="c4ae" class="ox mp it nt b gy oy oz l pa pb">JavaRDD&lt;String&gt; lines = sc.textFile("data.txt");</span></pre><p id="a667" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2.将每一行都映射到它的长度(为了简洁起见，使用了<a class="ae lq" href="https://aws.amazon.com/lambda/" rel="noopener ugc nofollow" target="_blank"> Lambda </a>函数)。</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="0894" class="ox mp it nt b gy oy oz l pa pb">JavaRDD&lt;Integer&gt; lineLengths = lines.map(s -&gt; s.length());</span></pre><p id="4909" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">3.要求解总长度:减少<code class="fe nq nr ns nt b">lineLengths</code> <em class="op"> </em>以找到总的线长度总和，在这种情况下是RDD中每个元素的总和。</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="e1cd" class="ox mp it nt b gy oy oz l pa pb">int totalLength = lineLengths.reduce((a, b) -&gt; a + b);</span></pre><p id="b9f2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">4.求解最大长度:减少<code class="fe nq nr ns nt b">lineLengths</code> <em class="op"> </em>以找到最大的线长度。</p><pre class="kj kk kl km gt ot nt ou ov aw ow bi"><span id="61b7" class="ox mp it nt b gy oy oz l pa pb">int maxLength = lineLengths.reduce((a, b) -&gt; Math.max(a,b));</span></pre><p id="2ab6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，第三步和第四步是RDD操作，所以它们向我们的驱动程序返回一个结果，在本例中是一个Java int。还记得Spark很懒，在看到一个动作之前拒绝做任何工作。在这种情况下，直到第三步它才会开始任何真正的工作。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h1 id="649f" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">后续步骤</h1><p id="54e6" class="pw-post-body-paragraph ku kv it kw b kx nl ju kz la nm jx lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">到目前为止，我们已经介绍了我们的数据问题及其解决方案——Apache Spark。我们回顾了Spark的架构和工作流，它的旗舰产品内部抽象(RDD)，以及它的执行模型。</p><p id="07f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，我们将研究Java中的函数和语法，随着对框架的深入研究，我们会越来越了解技术。</p></div></div>    
</body>
</html>