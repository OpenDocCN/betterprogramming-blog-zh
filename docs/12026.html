<html>
<head>
<title>Run TensorRT in 10 Bash Commands</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在10个Bash命令中运行TensorRT</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/run-tensorrt-in-10-bash-commands-b89a207d61c5?source=collection_archive---------13-----------------------#2022-05-05">https://betterprogramming.pub/run-tensorrt-in-10-bash-commands-b89a207d61c5?source=collection_archive---------13-----------------------#2022-05-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fa83" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">TensorRT引擎推理的接口，以及正在使用的YOLOv4引擎的示例</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/abe2d67b28339bb7820be8fc78216d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pS-8WXgWN_St8r8J7ubIpw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://pixabay.com/users/lauratara-6167959/" rel="noopener ugc nofollow" target="_blank">劳拉塔拉</a>在<a class="ae kv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> pixabay </a>拍摄的照片</p></figure><p id="d006" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我认为优化推理是人工智能开发中最重要的部分之一。让我们想象一个典型的工作流程，并假设我们有一个工程师，他使用PyTorch框架训练了一个神经网络来检测图像中的文本。</p><p id="f8d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">准备好部署后，工程师在AWS上启动了一个虚拟机，他选择的实例是最便宜的深度学习EC2实例，即著名的<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/g4/" rel="noopener ugc nofollow" target="_blank"> g4dn.xlarge </a>，带有一个负担得起的Nvidia Tesla T4图形处理单元。为了利用硬件，工程师使用了CUDA框架，PyTorch支持开箱即用。</p><p id="ef69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很好，推理比在CPU上快得多，但仍然有320个图灵张量核心没有得到正确的使用！</p><p id="9f32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">张量核不仅能够最大化您可以从EC2实例中获得的价值，而且还可以带来难以置信的性能提升(速度提高了10到30倍，这不是一个错误)。</p><p id="7fa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们把例子举得更远一点，并假设给定任务的算法需要的不是一个，也不是两个模型，而是一个架构，比如说五个不同的网络？</p><p id="f3ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，网络通常会有不同的格式，并且来自不同的框架。将几个框架作为依赖项会很快变得很麻烦。不仅如此，当涉及到GPU内存分配时，框架之间还会经常发生冲突。</p><p id="2f92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我发现将每个模型导出为ONNX格式是非常有益的一步，ONNX格式是开放神经网络交换的缩写，最初由微软的人开发。</p><p id="fc25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">知道网络将接收的确切数据形状和输入类型，并准备好权重，工程师可以使用PyTorch内置模块(<a class="ae kv" href="https://pytorch.org/docs/stable/onnx.html" rel="noopener ugc nofollow" target="_blank"> torch.onnx </a>)将其模型直接导出到ONNX中。不再需要模型基类，因为<code class="fe ls lt lu lv b">.onnx</code>格式包括模型图和权重，torch不再是依赖关系。</p><p id="e828" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，ONNX导出的模型可以用<code class="fe ls lt lu lv b">onnxruntime</code>模块进行推理。我在回购协议中提供了一个例子，这里是<a class="ae kv" href="https://github.com/piotrostr/infer-trt/blob/master/src/onnx_model.py" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="d9c3" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">要求</h1><ul class=""><li id="e00d" class="mo mp iq ky b kz mq lc mr lf ms lj mt ln mu lr mv mw mx my bi translated"><code class="fe ls lt lu lv b">amd64/linux</code>机器架构</li><li id="61c6" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">带张量内核的Nvidia GPU</li><li id="30a7" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><code class="fe ls lt lu lv b">nvidia-docker</code></li><li id="e083" class="mo mp iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><code class="fe ls lt lu lv b">.onnx</code>格式的模型(或<code class="fe ls lt lu lv b">.caffemodel</code>)</li></ul><h1 id="3c61" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">设置</h1><p id="b701" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">首先，可以派生/克隆包含代码的repo。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="de24" class="nl lx iq lv b gy nm nn l no np"><a class="ae kv" href="https://github.com/piotrostr/infer-trt" rel="noopener ugc nofollow" target="_blank">https://github.com/piotrostr/infer-trt</a></span></pre><p id="2f0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该示例使用了<code class="fe ls lt lu lv b">opencv</code>库，该库可以使用可以在<a class="ae kv" href="https://docs.opencv.org/4.x/d7/d9f/tutorial_linux_install.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>中找到的官方指令来构建。</p><p id="1d33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">即使使用<code class="fe ls lt lu lv b">8-core-cpu</code>构建它也需要相当长的时间，所以我建议安装来自<a class="ae kv" href="https://anaconda.org/conda-forge/opencv" rel="noopener ugc nofollow" target="_blank"> conda-forge </a>的二进制文件。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="43cb" class="nl lx iq lv b gy nm nn l no np">conda install -y -c conda-forge/label/gcc7 opencv</span></pre><p id="531c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TensorRT设置相当复杂，因此建议使用预构建的nvcr.io容器。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="a022" class="nl lx iq lv b gy nm nn l no np">docker pull nvcr.io/nvidia/tensorrt:22.03-py3</span></pre><p id="f708" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要用GPU运行容器，还需要nvidia-docker，对于基于debian的发行版来说，安装非常快。</p><p id="2447" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要运行容器，需要包含一些选项，为了不每次都面临长时间的docker运行，可以方便地将这些选项放入<code class="fe ls lt lu lv b">docker-compose.yml</code>。</p><h1 id="4e5f" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">使用</h1><p id="f0e0" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">通常情况下，<code class="fe ls lt lu lv b">yolo.onnx</code>重量可能是训练的结果。如前所述，如果使用PyTorch或TF2，很容易导出ONNX。</p><p id="488c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个例子，预先训练的权重可以从ONNX repo中卷曲或获取。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="b3f7" class="nl lx iq lv b gy nm nn l no np">curl \<br/>  -o yolo.onnx <a class="ae kv" href="https://github.com/onnx/models/blob/main/vision/object_detection_segmentation/yolov3/model/yolov3-10.onnx" rel="noopener ugc nofollow" target="_blank">https://github.com/onnx/models/blob/main/vision/object_detection_segmentation/yolov3/model/yolov3-10.onnx</a></span></pre><p id="2d71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了获得给定模型的TensorRT引擎，可使用<code class="fe ls lt lu lv b">trtexec</code>工具从<code class="fe ls lt lu lv b">onnx</code>重量文件中导出。</p><p id="7f8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该工具的可执行文件在<code class="fe ls lt lu lv b">nvcr.io</code>容器的bin中。</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="37ac" class="nl lx iq lv b gy nm nn l no np">./trtexec \<br/>  --onnx=./yolo.onnx \<br/>  --best \<br/>  --workspace=1024 \<br/>  --saveEngine=./yolo.trt \<br/>  --optShapes=input:1x3x416x416</span></pre><p id="345b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b">trt_model.py</code>包含一个用于继承的基类。一旦预处理和后处理方法被覆盖以匹配每个给定模型所需的步骤，就可以进行推理了。凭借其高级API:</p><pre class="kg kh ki kj gt nh lv ni nj aw nk bi"><span id="1df1" class="nl lx iq lv b gy nm nn l no np">#/usr/bin/env python<br/>import cv2<br/>import pycuda.autoinit<br/>import tensorrt as trt</span><span id="0468" class="nl lx iq lv b gy nq nn l no np">from yolo import YOLO</span><span id="5c2b" class="nl lx iq lv b gy nq nn l no np">yolo = YOLO(trt.Logger())<br/>img = cv2.imread(‘some_img.png’)<br/>labels, confidences, bboxes = yolo(img)</span></pre><h1 id="1136" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">许可证</h1><p id="b715" class="pw-post-body-paragraph kw kx iq ky b kz mq jr lb lc mr ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">麻省理工学院<a class="ae kv" href="https://github.com/piotrostr/infer-trt" rel="noopener ugc nofollow" target="_blank">https://github.com/piotrostr/infer-trt</a></p></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><p id="d6f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ny">原载于</em>【http://github.com】<em class="ny"/><em class="ny">。</em></p></div></div>    
</body>
</html>