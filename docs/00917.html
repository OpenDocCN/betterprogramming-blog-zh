<html>
<head>
<title>Face Detection and Recognition With CoreML and ARKit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于CoreML和ARKit的人脸检测和识别</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/face-detection-and-recognition-with-coreml-and-arkit-8b676b7448be?source=collection_archive---------2-----------------------#2019-07-28">https://betterprogramming.pub/face-detection-and-recognition-with-coreml-and-arkit-8b676b7448be?source=collection_archive---------2-----------------------#2019-07-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d0b2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用ARKit实现人脸检测功能，用CoreML模型实现人脸识别</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bbbd63a819afce1fd4c7a72776e1b6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWM2EvPIeMyXiCs149eW3g.png"/></div></div></figure></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><blockquote class="lb lc ld"><p id="8adb" class="le lf lg lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">以下文章提供了更新版本:</p></blockquote><div class="mb mc gp gr md me"><a href="https://heartbeat.fritz.ai/face-recognition-and-detection-on-ios-using-native-swift-code-core-ml-and-arkit-feed10c468da" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">使用本地Swift代码、Core ML和ARKit在iOS上进行人脸识别和检测</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">利用原生Swift库在iOS应用中执行人脸识别和检测</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">heartbeat.fritz.ai</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ks me"/></div></div></a></div><h1 id="2973" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">创建单一视图应用程序</h1><p id="8328" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">首先，我们需要创建一个带有单视图应用程序的iOS项目:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d26a970eca6c87aa54ed97d86a05d110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sSq6SuGzK68mBbmVIsu4Lg.png"/></div></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">创建单视图应用程序</p></figure><p id="253b" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">现在你已经有了你的项目，而且因为我不喜欢使用故事板，这个应用程序是以编程方式完成的，这意味着没有按钮或开关来切换，只有纯代码🤗。</p><p id="99ee" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">你必须删除<code class="fe nx ny nz oa b">main.storyboard</code>并像这样设置你的<code class="fe nx ny nz oa b">AppDelegate.swift</code>文件:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">AppDelegate.swift</p></figure><p id="a11e" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">确保从部署信息中删除故事板“Main”。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><h1 id="7880" class="mt mu it bd mv mw od my mz na oe nc nd jz of ka nf kc og kd nh kf oh kg nj nk bi translated">创建场景并将其添加到子视图中</h1><p id="9738" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">我们只有一个ViewController，这将是我们应用程序的主要入口点。</p><p id="928b" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">在这个阶段，我们需要导入ARKit并实例化一个<code class="fe nx ny nz oa b">ARSCNView</code>,它自动渲染来自设备摄像头的实时视频作为场景背景。它还会自动移动其SceneKit相机，以匹配设备的真实移动，这意味着我们不需要锚来跟踪我们添加到场景中的对象的位置。</p><p id="76ac" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">我们需要给它一个屏幕边界，这样摄像机会话就可以占据整个屏幕:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">实例化ARSCNView</p></figure><p id="7427" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">在<code class="fe nx ny nz oa b">ViewDidLoad</code>方法中，我们将设置一些东西，例如委托，我们还需要查看帧统计数据，以便监控帧丢失:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">在ViewDidLoad方法中设置场景</p></figure></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><h1 id="175a" class="mt mu it bd mv mw od my mz na oe nc nd jz of ka nf kc og kd nh kf oh kg nj nk bi translated">开始一个<code class="fe nx ny nz oa b">ARFaceTrackingConfiguration</code>会话</h1><p id="6ab7" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">现在我们需要用一个<code class="fe nx ny nz oa b">ARFaceTrackingConfiguration</code>开始一个会话，这个配置让我们可以访问仅适用于iPhone X、Xs和Xr的前置原深感摄像头。以下是苹果文档中更详细的解释:</p><blockquote class="lb lc ld"><p id="77db" class="le lf lg lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">面部跟踪配置通过设备的前置摄像头检测用户的面部。运行此配置时，AR会话会检测用户的面部(如果在正面摄像头图像中可见)，并向其锚点列表添加一个代表面部的ARFaceAnchor对象。每个脸部锚点提供了关于脸部的位置和方向、其拓扑以及描述脸部表情的特征的信息。</p><p id="a7b9" class="le lf lg lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">来源:<a class="ae oi" href="https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration" rel="noopener ugc nofollow" target="_blank">苹果</a></p></blockquote><p id="05bb" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">ViewDidLoad方法应该如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">ViewDidLoad()方法</p></figure></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><h1 id="d95a" class="mt mu it bd mv mw od my mz na oe nc nd jz of ka nf kc og kd nh kf oh kg nj nk bi translated">训练人脸识别模型</h1><p id="d33f" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">有多种方法可以创建与CoreML兼容的. mlmodel文件，以下是常用的方法:</p><ol class=""><li id="4425" class="oj ok it lh b li lj ll lm nn ol np om nr on ma oo op oq or bi translated"><a class="ae oi" href="https://pypi.org/project/turicreate/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="lh iu"><em class="lg">Turicreate</em></strong></a><strong class="lh iu"><em class="lg">:</em></strong>正是python库简化了定制机器学习模型的开发，更重要的是你可以将你的模型导出为Xcode可以解析的. mlmodel文件。</li><li id="0d73" class="oj ok it lh b li os ll ot nn ou np ov nr ow ma oo op oq or bi translated"><strong class="lh iu"><em class="lg">【MLImageClassifierBuilder():</em></strong>这是Xcode的一个内置解决方案，它提供了一个拖放界面来训练一个相对简单的模型。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/d13e2f2ca502081b8c419aea9b2f9222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWDStrIWMHhqcYtj0QJRIg.png"/></div></div><p class="nt nu gj gh gi nv nw bd b be z dk translated"><strong class="bd oy"><em class="oz">MLImageClassifierBuilder</em></strong></p></figure><p id="4122" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">我已经创建了多个模型来测试这两种解决方案，因为我没有一个大的数据集，所以我决定使用<strong class="lh iu"><em class="lg">MLImageClassifierBuilder()</em></strong>和一组67张图片，分别是<em class="lg">‘Omar MHAIMDAT</em>’(这是我的名字)和一组261张我在<a class="ae oi" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"><strong class="lh iu"><em class="lg">unsplash</em></strong></a><strong class="lh iu">T37】上找到的<em class="lg">‘未知’</em>的脸 </strong></p><p id="b12c" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">打开playground并编写以下代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated"><strong class="ak"><em class="oz">MLImageClassifierBuilder</em></strong></p></figure><p id="6849" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">我建议将最大迭代次数设置为20，并添加一个裁剪增强，这将为每个图像添加4个裁剪图像实例。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><h1 id="9981" class="mt mu it bd mv mw od my mz na oe nc nd jz of ka nf kc og kd nh kf oh kg nj nk bi translated">捕捉相机帧并将其注入模型</h1><p id="1efe" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">我们需要用场景委托<code class="fe nx ny nz oa b">ARSCNViewDelegate</code>来扩展我们的ViewController。我们需要两个委托方法，一个用于设置人脸检测，另一个用于在检测到人脸时更新场景:</p><h2 id="2775" class="pa mu it bd mv pb pc dn mz pd pe dp nd nn pf pg nf np ph pi nh nr pj pk nj pl bi translated">面部检测:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">人脸检测</p></figure><p id="7033" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">不幸的是，当我睁开眼睛或嘴巴时，场景不会更新。在这种情况下，我们需要相应地更新场景。</p><h2 id="d70a" class="pa mu it bd mv pb pc dn mz pd pe dp nd nn pf pg nf np ph pi nh nr pj pk nj pl bi translated">更新场景:</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="4b19" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">我们获取整个面部的几何图形和贴图，并更新节点。</p><h2 id="edc7" class="pa mu it bd mv pb pc dn mz pd pe dp nd nn pf pg nf np ph pi nh nr pj pk nj pl bi translated">获取相机框架:</h2><p id="b542" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">这变得很有趣，因为<code class="fe nx ny nz oa b">ARSCNView</code>从<code class="fe nx ny nz oa b">AVCaptureSession</code>继承而来，意味着我们可以得到一个<code class="fe nx ny nz oa b">cvPixelFuffer</code>来填充我们的模型。</p><p id="1bf1" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">下面是从我们的<code class="fe nx ny nz oa b">sceneView</code>属性获取它的简单方法:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="88b3" class="pa mu it bd mv pb pc dn mz pd pe dp nd nn pf pg nf np ph pi nh nr pj pk nj pl bi translated">将相机帧注入模型:</h2><p id="c9a3" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">现在，我们可以检测人脸并拥有每一个相机帧，我们准备为我们的模型提供一些内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">didUpdate渲染器</p></figure></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><h1 id="abd8" class="mt mu it bd mv mw od my mz na oe nc nd jz of ka nf kc og kd nh kf oh kg nj nk bi translated">在已识别的面孔上方显示姓名</h1><p id="d944" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">最后，也可能是最令人沮丧的部分是在识别出的人脸上方投射3D文本。仔细想想，我们的配置不如<code class="fe nx ny nz oa b">ARWorldTrackingConfiguration</code>强大，它提供了对大量方法和类的访问。相反，我们使用前置摄像头，可以实现的事情很少。</p><p id="21b6" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">然而，我们仍然可以在屏幕上投影3D文本，尽管它不会跟踪面部运动并相应地改变。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">实例化SCNText</p></figure><p id="8939" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">现在我们有了SCNText对象，我们需要用相应的face更新它，并将其添加到<code class="fe nx ny nz oa b">rootNode</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="nt nu gj gh gi nv nw bd b be z dk translated">使用与面部相关联的名称更新场景</p></figure></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><h1 id="11d5" class="mt mu it bd mv mw od my mz na oe nc nd jz of ka nf kc og kd nh kf oh kg nj nk bi translated">最终结果:</h1><p id="d34f" class="pw-post-body-paragraph le lf it lh b li nl ju lk ll nm jx ln nn no lq lr np nq lu lv nr ns ly lz ma im bi translated">这是人脸检测和识别的最终结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/afd131ae3055b5334e7845a9da309089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*bVGRNGkBNVndmGgwyScqWg.gif"/></div></figure><p id="ceb8" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">如果你喜欢这首曲子，请鼓掌并与你的朋友分享。如果你有任何问题，不要犹豫给我发电子邮件到omarmhaimdat@gmail.com。</p><p id="619b" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated">这个项目可以从我的Github帐户下载</p><div class="mb mc gp gr md me"><a href="https://github.com/omarmhaimdat/WhoAreYou" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">omarmhaimdat/你是谁</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">github.com</p></div></div></div></a></div><p id="01ec" class="pw-post-body-paragraph le lf it lh b li lj ju lk ll lm jx ln nn lp lq lr np lt lu lv nr lx ly lz ma im bi translated"><a class="ae oi" href="https://github.com/omarmhaimdat/WhoAreYou" rel="noopener ugc nofollow" target="_blank"> <strong class="lh iu">下载项目</strong> </a></p></div></div>    
</body>
</html>