# 先保存，后解析:为网络抓取的不同方法辩护

> 原文：<https://betterprogramming.pub/save-first-parse-later-in-defense-of-a-different-approach-to-web-scraping-9edfe65adf04>

## 通过使用这种策略，可以减少页面请求，但仍然可以获得您需要的所有信息

![](img/fced0a32b31e102cfeb7ee38ba406011.png)

斯蒂芬·斯坦鲍尔在 Unsplash[拍摄的照片](https://unsplash.com?utm_source=medium&utm_medium=referral)

当一个人开发一个大的 web 抓取项目(有成百上千的页面要抓取)时，经常会在你写的代码中发现错误，破坏脚本，或者可能没有保存你想要的确切信息或你想要的格式。这个问题可能会导致需要重新开始 web 抓取过程，这样做的话，您将会向以前访问过的网页组重新发送不必要的请求。

亲爱的程序员朋友，这一点也不酷！

为什么不先把这些网页保存在你的电脑上，作为`.html`文件，然后把你的开发代码指向这些文件(顺便说一下，这些文件包含了你想要抓取的页面的所有静态 HTML)？因此，当所有东西都在本地顺利运行并且信息已经正确保存时，您可以喝一杯啤酒或茶，然后就可以结束一天的工作了。(你的机器里有你需要抓取的页面的完整 HTML，记得吗？).或者，您可能想对网站进行最后一次检查，以保证所有要搜集的信息都是最新的？在此之后，HTML 文件可以被丢弃，或者在给定时间作为网站的历史记录保存。

将这种先保存，后解析(SFPL)的方法整合到你的 web 抓取开发例程中的好处将会在一会儿展示出来。现在，我想简单谈谈最近的项目，它帮助我重新思考我的整个网络抓取过程。

# **改变这一切的项目(至少对我来说)**

在我与[joo Oliveira](https://linktr.ee/joaomdeoliveira)的指导过程中，我们面临的挑战是创建一个 Power BI 仪表板，其中包含我们选择的一个立法议会的立法数据。这个项目本身受到了之前一个类似项目的启发，该项目由 Joviano Silveira 创建，并在[这个 YouTube 播放列表](https://www.youtube.com/playlist?list=PLQpSyz5rZmJr3OLXzK9DDwLU14VOpAbXh)中完全可用(他的视频和代码是葡萄牙语的，因为他正在与葡语观众交谈)。

在巴西，每个城市和联邦州都有自己的议会。恕我直言，当选的政治家太多了。把他们的座位数量减少一半，我们可能会有一些具体的改善。

回到这个项目，我们被告知用 Python 从所选的 Parliament 网站抓取数据，保存在 SQL Server 数据库中，然后让 Power BI dashboard 通过使用数据库连接来使用这些数据。这些是由一个假想的客户制定的具体要求，我们应该遵循这些要求。我们可以使用`pyodbc` Python 模块来执行 SQL 查询并填充 SQL Server 表。

虽然我已经有了网络抓取领域的专业经验，但我面临着一个有趣的新挑战，从我选择的网站上抓取信息。原因是我想保存在 SQL Server 数据库中的大多数键/值对要么只被`<b></b>` HTML 标记包围，要么根本没有被标记包围。这意味着从父 HTML 标签中获取内部文本会混淆所有的信息，对我的项目毫无用处。要求下一个兄弟姐妹也没用。

在与这个问题斗争了一段时间后，我意识到我对网站提出了太多的要求；这让我产生了在本地电脑上保存所有我需要的页面的想法。接下来，我开始向这些本地 HTML 文件发送请求，并不断修改我的代码。

最终，当我无法让 BeautifulSoup 或 Selenium 获得一些特定的文本信息时，我决定从周围的`<div>`标签中提取外部的 HTML，并使用正则表达式来捕捉仍然缺少的内容。一旦我的代码在我的机器上完美运行，我就对网站进行了最后一次运行，并获得了最新更新的信息。互联网连接很有帮助，完整的代码运行时没有错误。此外，这种策略避免了对目标网页无数不必要的请求。

# **讨论利弊**

首先，我们需要考虑网络抓取是一个现实，并且已经存在。我们都需要不同项目的数据，自动化从互联网收集公共数据的过程是非常有用和合法的，只要它在一些道德限制内使用。我认为，除了`robots.txt`文件中的指导原则之外，在这一领域要遵循的最重要的原则是一些哲学和道德教科书中已经讨论过的原则:“己所不欲，勿施于人”；更妙的是:“*己所不欲，勿施于人*”

在我的特殊情况下，我只是想，如果我是网站所有者，我不会高兴地知道一些程序员迷失在他的编码过程中，并对我的网站提出了太多的请求。这个推理足以让我想到上面提到的解决方案:将页面内容作为原始 HTML 文件保存在本地，并在以后弄清楚如何通过访问这些文件来提取我需要的信息。当然，其他人以前也想到过这个解决方案，这并不否定我上面报道的个人经验。

这种在本地保存页面的方法还有其他优点，例如:

*   如果您的互联网或网站服务器暂时关闭(或永久关闭)，您可以访问网站内容；
*   你可以从站点内容中保留一个特定的“快照”,用于历史目的，如果原始站点已经改变，它将不会被改变；
*   您避免了由于过多的请求而被网站阻止的可能性；
*   代码中的“先保存”部分比“以后解析”部分更容易推广和用于其他项目。这将有助于在您未来的 web 抓取需求中重用代码。
*   在“稍后解析”部分，您不需要在页面之间等待几秒钟，因为您正在请求保存在机器中的 HTML 文件。但是，请记住，在向互联网网站上的新页面发出请求之前，先等几秒钟是一种良好的习惯。

当然，人们可以想到缺点列表中的一些项目，例如:

*   这不是从网站获取动态生成内容的有效方式；
*   您需要在要抓取的网页上循环两次，第一次将 HTML 文件保存在本地，第二次从这些文件中获取您想要解析的特定部分。
*   你将会在你的计算机中使用一些额外的存储空间，即使是暂时的(但是记住现在存储空间更便宜，HTML 文件通常也很小)。

# **最终想法**

T2 SFPL 的方法绝对不是所有网络抓取项目的灵丹妙药。它甚至可能有一些我还没有意识到的问题(所以欢迎在评论区指出它们)。然而，我认为这种方法对我来说已经足够好了，以至于从现在开始成为我新的网络抓取主要方法。

亲爱的读者，非常感谢你花时间和精力阅读我的文章。

*快乐编码*！