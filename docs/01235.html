<html>
<head>
<title>Scraping Memes From Reddit With the Python Reddit API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python Reddit API从Reddit抓取迷因</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/scraping-memes-from-reddit-55842273b3e1?source=collection_archive---------5-----------------------#2019-08-25">https://betterprogramming.pub/scraping-memes-from-reddit-55842273b3e1?source=collection_archive---------5-----------------------#2019-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9f45" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有趣的周末项目——编写代码从r/programmer humore中抓取迷因</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6643a65c22a0a1dd8a191ba4fc3c7796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ib_tTUOfxL0yZHQeeY_h3Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@konkarampelas?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Kon Karampelas </a>在<a class="ae ky" href="https://unsplash.com/search/photos/reddit?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="545c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很长一段时间，我曾经认为从互联网上抓取数据太无聊了，在DevTools中检查网页，找到你感兴趣的DOM节点——对我来说似乎太麻烦了。</p><p id="d2d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直到有一天，我用<a class="ae ky" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"><em class="lv">Beautiful Soup</em></a><strong class="lb iu"/>试了一下，看到使用解析的dom和收集感兴趣的数据是多么容易，我真的受到了启发。</p><p id="c612" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那以后，我一直在探索抓取的世界，最近遇到了<a class="ae ky" href="https://praw.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> PRAW </a>，它是Python Reddit API包装器，使访问Reddit数据变得非常容易。</p><p id="a0ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在探索了这个包一段时间后，我真的想做一个有趣的小周末项目，还有什么比编写代码从<a class="ae ky" href="https://www.reddit.com/r/ProgrammerHumor/comments/bolfnr/that_code_you_copy_which_runs_smooth_af/" rel="noopener ugc nofollow" target="_blank"> r/ProgrammerHumor </a>中抓取迷因更好的呢。</p><p id="17b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于本教程，我们需要:</p><ul class=""><li id="980b" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">Python。</li><li id="8e4b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">Reddit账户。</li><li id="2bb8" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">访问Reddit API的客户端ID和客户端密码。</li><li id="77e6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">用户代理。</li><li id="2dee" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://docs.python.org/3/library/urllib.html" rel="noopener ugc nofollow" target="_blank"> urllib </a>。</li><li id="eeac" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">熊猫</a></li></ul><p id="dd88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们知道了需求，让我们首先创建一个Reddit应用程序，并获取我们的客户端ID、客户端密码和用户代理。</p><p id="c5ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进入<a class="ae ky" href="https://www.reddit.com/prefs/apps" rel="noopener ugc nofollow" target="_blank"> <em class="lv">应用偏好</em> </a> <strong class="lb iu"> </strong>，点击<em class="lv">创建应用</em>或创建另一个应用，这将带你进入此屏幕。对于重定向URL，输入<code class="fe mk ml mm mn b"><a class="ae ky" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank">http://localhost:8080</a></code>，如<a class="ae ky" href="https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application" rel="noopener ugc nofollow" target="_blank">文档</a>中所述。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/efa7ad10eb198ac9fe02069285878556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3f6GfvGuHJIcqum74k3xBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建-Reddit-应用程序</p></figure><p id="6333" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你输入了细节，点击<em class="lv">创建应用</em>，你将被带到这个屏幕。请在此处记下您的客户ID和客户密码，因为我们稍后将需要它们来进行身份验证。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/e9fc8f331b2ed65e83f5a25a6f15055b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C-xVOOFOqV877jdZeCZ4sw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">客户端id、密码和用户代理</p></figure><p id="6df0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经创建了一个应用程序，让我们安装Python依赖项。</p><p id="5ec4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打开终端/cmd并运行:</p><pre class="kj kk kl km gt mq mn mr ms aw mt bi"><span id="9842" class="mu mv it mn b gy mw mx l my mz">pip install praw pandas</span></pre><p id="f76f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们已经有了客户端ID、密码和用户代理，所以我们现在可以继续编写代码并开始使用Reddit API。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PRAW初始化</p></figure><p id="db56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面这段代码将让我们访问<code class="fe mk ml mm mn b">praw.Reddit</code> <strong class="lb iu"> </strong>实例，现在我们可以访问子编辑，从特定的Reddit获取帖子、评论等。</p><p id="9b95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将从/r/ProgrammerHumor获取帖子，并遍历这些帖子，收集我们想要的数据并保存图像文件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="40bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面这段代码获取subreddit r/progammer humor<strong class="lb iu"/>并获取热门帖子，将其限制为10个。</p><p id="ad75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，您可以遍历帖子并保存适当的数据。查看<a class="ae ky" href="https://praw.readthedocs.io/en/latest/getting_started/quick_start.html" rel="noopener ugc nofollow" target="_blank"> PRAW的文档</a>，更深入地了解所有可用的方法。</p><p id="92ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经可以访问URL，我们将使用urllib简单地将迷因下载到本地文件系统，这是一个用于在互联网上交互和获取数据的高级接口。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">下载到文件系统</p></figure><p id="d7f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们只需遍历URL并检查它们是否是允许的扩展名之一，我们使用<code class="fe mk ml mm mn b">urllib.urlretrieve</code>将它们下载到文件系统，最后，我们可以使用<a class="ae ky" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> pandas </a>将数据保存到CSV中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">导出到CSV</p></figure><p id="107a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是最终的脚本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nb l"/></div></figure><p id="e4ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">访问我的<a class="ae ky" href="https://github.com/asjadanis/reddit-scrapper" rel="noopener ugc nofollow" target="_blank"> GitHub获取GitHub回购</a>。</p><div class="nd ne gp gr nf ng"><a href="https://github.com/asjadanis/reddit-scrapper" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">asjadanis/reddit-scrapper</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">一个使用PRAW的python脚本，可以让你从你最喜欢的子编辑中下载媒体文件，并将帖子信息导出到…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">github.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ks ng"/></div></div></a></div></div></div>    
</body>
</html>