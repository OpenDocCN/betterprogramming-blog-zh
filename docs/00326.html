<html>
<head>
<title>Why We Need Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么我们需要Apache Spark</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/why-we-need-apache-spark-51c8a57aa57a?source=collection_archive---------0-----------------------#2019-04-16">https://betterprogramming.pub/why-we-need-apache-spark-51c8a57aa57a?source=collection_archive---------0-----------------------#2019-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="46a8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">面对海量的新数据，我们需要一个工具来快速消化它们Spark就是答案</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f6a29856d9ae3be49e9fd35002ec7b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hGNAJ9aT4U8UmEiB"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ky" href="https://unsplash.com/@grakozy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Greg Rakozy </a>拍摄的照片</p></figure><p id="ee06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据就在我们身边。IDC估计2013年“数字世界”的规模为4.4千兆字节(1万亿千兆字节)。目前，数字世界每年增长40%，到2020年，IDC预计数字世界将达到440亿字节，相当于物理世界中每颗恒星的一位数据。</p><p id="3638" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有很多数据，但我们不会丢弃任何数据。我们需要一种方法来大规模存储越来越多的数据，防止因硬件故障导致的数据丢失。最后，我们需要一种通过快速反馈循环来消化所有这些数据的方法。感谢宇宙，我们有Hadoop和Spark。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5342" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">一个例子</h1><p id="04e8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了演示Spark的有用性，让我们从一个示例数据集开始。500 GB的样本天气数据包括:<strong class="lb iu"> C <em class="mz">国家|城市|日期|温度</em>T7】</strong></p><p id="1825" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们被要求为这些数据计算每个国家的最高温度，我们从本地Java程序开始，因为Java是您第二喜欢的编程语言:</p><h2 id="799c" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">Java解决方案</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/1061f9983c600516e5fba7ef1757577d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBH5hNW29GxyyaOZERh4-w.png"/></div></div></figure><p id="8a9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在500 GB大小的情况下，利用这种本地Java方法，即使是这样一个简单的任务也要花将近<strong class="lb iu">五个小时</strong>才能完成。</p><blockquote class="nn no np"><p id="ff8c" class="kz la mz lb b lc ld ju le lf lg jx lh nq lj lk ll nr ln lo lp ns lr ls lt lu im bi translated">" Java糟透了，我就用Ruby写这个，然后很快就开始，Ruby是我的最爱."</p></blockquote><h2 id="114a" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">红宝石溶液</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c0dff35271ce9e4de17f293f1efd4266.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*scGwOCrcH8oMbTqh-gZ5aQ.png"/></div></figure><p id="989f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Ruby成为你的最爱并不意味着它更适合这个任务。I/O根本不是Ruby的强项，所以Ruby比Java需要更长的时间来找到最高温度。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="a4c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" rel="noopener ugc nofollow" target="_blank"> Apache MapReduce </a>(我打赌你认为我会说Spark) <em class="mz">可以最好地解决按城市查找最高温度的问题。这就是MapReduce的亮点:将城市映射为键，将温度映射为值，我们将在更短的时间内找到我们的结果，大约15分钟，而以前在Java中需要5个多小时。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d868f46d912fc159cf2e19d3344f7fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*MjkqSYIjWSeRorn-14p3aA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最大温度映射</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/807f1bcb63cf6b84da5b75008c8db587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*JhQNfx24rdcbtidO3HFTEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最高温度降低器</p></figure><p id="9d92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MapReduce是解决这个问题的完美可行的解决方案。与原生Java解决方案相比，这种方法的运行速度要快得多，因为MapReduce框架在将地图任务委托给我们的工作集群方面表现出色。行被从我们的文件并行地输入到每个集群节点，而它们被一次一行地输入到我们本地Java的main方法中。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6aea" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">问题是</h1><p id="4d05" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">计算每个国家的最高温度本身是一项新颖的任务，但这并不是开创性的分析。真实世界的数据带来了更麻烦的模式和复杂的分析，推动我们使用工具来填补我们特定的空缺。</p><p id="10bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们被要求按国家和城市查找最高温度，而不是最高温度，然后我们被要求按天进行分解，会怎么样？如果我们把它混在一起，然后被要求找出平均气温最高的国家，会怎么样？或者如果你想找到你的栖息地，那里的温度永远不会低于58度或高于68度(<a class="ae ky" href="https://en.wikipedia.org/wiki/Antananarivo" rel="noopener ugc nofollow" target="_blank">塔那那利佛马达加斯加</a>看起来还不错)。</p><p id="819a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MapReduce擅长批量数据处理，但在重复分析和小反馈循环方面却落后了。在计算之间重用数据的唯一方法是将其写入外部存储系统(类似HDFS)。MapReduce在执行Reduce步骤之前，会在每个作业中写出其地图的内容。这意味着每个MapReduce作业将完成在其开始时定义的单个任务。</p><p id="caca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想要进行上述分析，将需要三个单独的MapReduce作业:</p><ol class=""><li id="4917" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated"><em class="mz">最高温度映射器，最高温度降低器，最高温度运行器</em></li><li id="2bf9" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><em class="mz">maxtemperatebycitymapper，MaxTemperatureByCityReducer，maxtemperatebycitryrunner</em></li><li id="ec47" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><em class="mz">maxtemperatebycitybydaymapper，MaxTemperatureByCityByDayReducer，maxtemperatebycitybydayrunner</em></li></ol><p id="8144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显这很容易失控。</p><p id="2db4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MapReduce中的数据共享速度很慢，这是由于分布式文件系统的优势:复制、序列化以及最重要的磁盘IO。许多MapReduce应用程序可能会花费高达90%的时间来读写磁盘。</p><p id="af16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">认识到上述问题后，研究人员开始开发一个专门的框架，可以完成MapReduce所不能完成的任务:在一个连接的机器集群上进行内存计算。</p><h1 id="ef71" class="mc md it bd me mf ok mh mi mj ol ml mm jz om ka mo kc on kd mq kf oo kg ms mt bi translated">火花:解决方案</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/b12946d9ae4a5f55053111f518d32e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tP-dw4Oj_42BYbkdtYbjMA.png"/></div></div></figure><p id="553a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark为我们解决了这个问题。Spark为我们提供了紧密的反馈循环，并允许我们快速处理多个查询，而且开销很小。</p><p id="7608" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上三个映射器都可以嵌入到同一个Spark作业中，如果需要的话，可以输出多个结果。上面几行可以很容易地用来根据我们具体的工作要求设置正确的键。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/3302df9312d0accae70828a89b6b7cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Om2_B09Bwi1sQ5XxJBmOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用RDDs实现MaxTemperatureMapper的Spark</p></figure><p id="50c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于类似的任务，Spark的迭代速度也将比MapReduce快十倍，因为Spark完全在内存中运行<strong class="lb iu">——因此它永远不必从磁盘中写入/读取，这通常是一种缓慢而昂贵的操作。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="14d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache Spark是一个非常强大的数据分析和转换工具。如果这篇文章引起了兴趣，请继续关注:在接下来的几篇文章中，我们将深入探讨Apache Spark框架的细节。</p></div></div>    
</body>
</html>