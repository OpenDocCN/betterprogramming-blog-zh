<html>
<head>
<title>Understanding Decision Trees in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习中的决策树</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/understanding-decision-trees-in-machine-learning-86d750e0a38f?source=collection_archive---------6-----------------------#2020-11-05">https://betterprogramming.pub/understanding-decision-trees-in-machine-learning-86d750e0a38f?source=collection_archive---------6-----------------------#2020-11-05</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="a2a1" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">决策树背后的数学以及如何使用Python和sklearn实现它们</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/448063b924447d5847d146d80ac55f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivA9KcvgjdDvwJKWwD-j9A.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@toddquackenbush?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">托德·夸肯布什</a>在<a class="ae kz" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="0132" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">决策树</h1><p id="7848" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">决策树是一种监督机器学习，主要用于分类问题。</p><p id="69de" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">决策树基本上是贪婪的、自顶向下的、递归的划分。“贪婪”是因为在每一步我们都尽可能选择最好的分割。“自顶向下”是因为我们从包含所有记录的根节点开始，然后将进行分区。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj na"><img src="../Images/ad827f861cea75cb3e38ee6145c3eee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*ElmRaZnR9XqaEOL6eE3H4Q.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="0edd" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv">根节点</strong>→决策树中最顶端的节点称为<em class="nb">根节点</em>。<br/> <strong class="mb iv">决策节点</strong> →分裂成更多子节点的子节点称为<em class="nb">决策节点</em>。<br/> <strong class="mb iv">叶/终端节点</strong>→不分裂的节点称为<em class="nb">叶节点</em> / <em class="nb">终端节点</em>。</p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="d71d" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">数据集</h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nc"><img src="../Images/363ff8ed3a644ba03b4a708fd47dc788.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*AJMZxZWUl3fNys4eofuKww.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="f130" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">我采用了一个小型数据集，其中包含身体质量指数和年龄特征以及目标变量糖尿病。</p><p id="2273" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们预测一个给定年龄和身体质量指数的人是否会有糖尿病。</p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="cd23" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">数据集表示</h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj gk"><img src="../Images/4b5aa4a12a50d23ec5a95cf257ef845c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zearicNIQsifCVIc-FFN5A.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="3bd5" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">我们不能画一条线来得到一个决策边界。我们一次又一次地拆分数据，以获得决策边界。这就是决策树算法的工作原理。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nd"><img src="../Images/1db544643b1bc97b4cc7b045dc31de6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncVj4gKuTZ6aX_niHOuUog.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="414b" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">这就是决策树中划分的方式。</p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="9a50" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">决策树理论中的重要术语</h1><h2 id="f6be" class="ne li iu bd lj nf ng dn ln nh ni dp lr mi nj nk lt mm nl nm lv mq nn no lx np bi translated">熵</h2><p id="376e" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">熵是随机性或不确定性的度量。熵水平的范围在<code class="fe nq nr ns nt b"> o</code>和<code class="fe nq nr ns nt b"> 1</code>之间。如果熵为0，说明这是一个纯子集(没有随机性)。如果熵为1，说明随机性高。熵用<strong class="mb iv"> H(S) </strong>表示。</p><h2 id="6dff" class="ne li iu bd lj nf ng dn ln nh ni dp lr mi nj nk lt mm nl nm lv mq nn no lx np bi translated">公式</h2><p id="89a3" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated"><strong class="mb iv">熵</strong>=-(P(0)* log(P(0))+P(1)* log(P(1)))</p><p id="85c1" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">P(0) →概率<code class="fe nq nr ns nt b">class 0</code></p><p id="f3c4" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">P(1) →概率<code class="fe nq nr ns nt b">class 1</code></p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="bf1b" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">熵与概率的关系</h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nu"><img src="../Images/9c872ff1708992348e6dd3a6324e1898.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*VtGOUPvRzJvvc66LDikmGQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="d142" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">如果熵为0，意味着这是一个纯子集(无随机性)(要么全是，要么全否)。如果熵为1，则意味着随机性高</p><p id="9591" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们画一个图P(1)-类1的概率对熵。</p><p id="d2b7" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">从上面的解释我们知道，如果P(1)为0，熵=0 <br/>如果P(1)为1，熵=0 <br/>如果P(1)为0.5，熵=1</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nv"><img src="../Images/a1505d541b61479644deef45358bde35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*ZybZda_ncIisfLHt1nJj1Q.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="e365" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">熵级别始终介于0和1之间。</p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="e393" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">信息增益</h1><p id="31b9" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">通过从原始熵中减去每个分支的加权熵来计算分裂的信息增益。我们将使用它来决定决策树节点中属性的排序。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nw"><img src="../Images/2260edf384bb90b71cb39d2193058d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*m1EtelgF1oA0qUwdPCuaJg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="1cb3" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">H(S) →熵<br/>A→属性<br/>S→示例集{ x }<br/>V→A的可能值<br/>Sv→子集</p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="238c" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">决策树是如何工作的？</h1><p id="636b" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">在我们的数据集中，我们有两个属性，身体质量指数和年龄。我们的样本数据集有七条记录。</p><p id="9749" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们开始用这个数据集构建一个决策树。</p><h2 id="9baf" class="ne li iu bd lj nf ng dn ln nh ni dp lr mi nj nk lt mm nl nm lv mq nn no lx np bi translated">第一步。根节点</h2><p id="27b7" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">在决策树中，我们从根节点开始。让我们把所有的记录(我们给定的数据集中有7条)作为我们的训练样本。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nc"><img src="../Images/363ff8ed3a644ba03b4a708fd47dc788.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*AJMZxZWUl3fNys4eofuKww.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="02da" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">它有三个<em class="nb">是</em>和四个<em class="nb">否</em>。<br/>0类的概率是4/7。七条记录中有四条属于0类<br/><strong class="mb iv">P(0)= 4/7</strong><br/>1类的概率是3/7。七个记录中有三个属于类别1。<br/> <strong class="mb iv"> P(1)=3/7 </strong></p><p id="ab6d" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv">计算根节点的熵</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nx"><img src="../Images/4b8a47b8124d6a1d1f1c5f1983ca6b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*QSiCr3mkS9b3GPdAnEblYg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><h2 id="f57d" class="ne li iu bd lj nf ng dn ln nh ni dp lr mi nj nk lt mm nl nm lv mq nn no lx np bi translated">第二步。分裂是如何发生的？</h2><p id="e489" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">我们有两个属性身体质量指数和年龄。基于这些属性，分裂是如何发生的？我们如何检查拆分的有效性？</p><ol class=""><li id="f37d" class="ny nz iu mb b mc mv mf mw mi oa mm ob mq oc mu od oe of og bi translated">如果我们选择属性身体质量指数作为分裂变量，并且≤30作为分裂点，我们得到一个纯子集。</li></ol><p id="59ea" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">[在数据集中的每个数据点考虑分裂点。因此，如果数据点是唯一的，那么n个数据点将有n-1个分裂点。因此，根据分裂变量和分裂点，我们得到高信息增益，分裂被选中。如果是一个大型数据集，通常只考虑值分布的某些百分点(10%、20%、30%)处的分割点，因为这是一个小型数据集，通过查看数据点，我选择≤30作为分割点。]</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oh"><img src="../Images/2efb508caa3f31967d16fedd5dba86fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PuzA6ZjBlRj-Px1BYCElNw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="ab30" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">纯子集的熵=0。</p><p id="b2a0" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们计算另一个子集的熵。这里我们得到三个<em class="nb">是</em>和一个<em class="nb">否</em>。<br/> P(0)=1/4【四个记录中的一个】<br/> P(1)=3/4【四个记录中的三个】</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oi"><img src="../Images/99b4d45f082b4035fdb9619ce8a3777f.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*zeuWIMA6xBY9nhvDgSmw-A.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="e25e" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">我们必须计算信息增益来决定选择哪个属性进行分裂。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/cc66112a79d81c616e749729e6b555be.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*Lxt5GnIfYATre9IYVqQmyA.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="a0ed" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">2.让我们选择属性年龄作为分裂变量，选择≤45作为分裂点。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ok"><img src="../Images/a97621a39dca5e0f26979c1225eaad46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mrQrodNUPMVbSFTjVD5U0g.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="f979" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">首先，我们来计算真子集的熵。它有一个<em class="nb">是</em>和一个<em class="nb">否</em>。这意味着高度的不确定性。熵是1。</p><p id="48b4" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们计算伪子集的熵。它有两个<em class="nb">是</em>和三个<em class="nb">否</em>。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ol"><img src="../Images/3a3dfcb4af924bf25f5bd177a3e34b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*C61MAikNy0vWCa-82wwRyw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="76ee" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们计算一下信息增益。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj om"><img src="../Images/71a64002ab32deecf7be4a743e13c462.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*MZjQMdSoiDSbD1J_N7lI7g.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="414f" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">我们必须选择具有高信息增益的属性。在我们的例子中，只有身体质量指数属性具有高信息增益。因此选择身体质量指数属性作为分裂变量。</p><p id="076d" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">通过属性身体质量指数分割后，我们得到一个纯子集(叶节点)和一个不纯子集。让我们根据属性年龄再次分割这个不纯的子集。然后我们有两个纯子集(叶节点)。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj on"><img src="../Images/92175fd2591db0339c4f63db3e40acc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvHLJgUu5QYBL8vpfcFWoA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="04eb" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">现在我们已经创建了一个包含纯子集的决策树。</p></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="cec4" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">使用sklearn的决策树的Python实现</h1><ol class=""><li id="f50f" class="ny nz iu mb b mc md mf mg mi oo mm op mq oq mu od oe of og bi translated">导入库。</li></ol><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="a0b9" class="ne li iu nt b gz ov ow l ox oy"><strong class="nt iv">import </strong>numpy <strong class="nt iv">as </strong>np<br/><strong class="nt iv">import </strong>pandas <strong class="nt iv">as </strong>pd<br/><strong class="nt iv">import </strong>matplotlib.pyplot <strong class="nt iv">as </strong>plt<br/><strong class="nt iv">import </strong>seaborn <strong class="nt iv">as </strong>sns</span></pre><p id="afb8" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">2.加载数据。</p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="738a" class="ne li iu nt b gz ov ow l ox oy">df=pd.read_csv(<strong class="nt iv">"Diabetes1.csv"</strong>)<br/>df.head()</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oz"><img src="../Images/df348719489cde8de51d51d22c5bcae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*sExghFIQyy0Bj5y8uvWvBg.png"/></div></figure><p id="dc4c" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv"> 3。分割x和y变量。</strong></p><p id="256a" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">身体质量指数和年龄属性作为<em class="nb"> x </em>变量。<br/>糖尿病属性(目标变量)作为<em class="nb"> y </em>变量。</p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="b2b4" class="ne li iu nt b gz ov ow l ox oy">x=df.iloc[:,:2]<br/>y=df.iloc[:,2:]</span><span id="b846" class="ne li iu nt b gz pa ow l ox oy">x.head(3)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pb"><img src="../Images/a1fc554ac9345123899730f37f19fed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*AsMmqo1VQpHFRQRPXDdHYQ.png"/></div></div></figure><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="98bb" class="ne li iu nt b gz ov ow l ox oy">y.head(3)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pc"><img src="../Images/c9974965f61fe4e8f19983959189272e.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*F-BC2-HNbTBfRMS7P3libA.png"/></div></figure><p id="09fc" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv"> 4。用sklearn建立模型</strong></p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="86f9" class="ne li iu nt b gz ov ow l ox oy"><strong class="nt iv">from </strong>sklearn <strong class="nt iv">import </strong>tree<br/>model=tree.DecisionTreeClassifier(criterion=<strong class="nt iv">"entropy"</strong>)<br/>model.fit(x,y)</span></pre><p id="d5bc" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">输出:<code class="fe nq nr ns nt b">DecisionTreeClassifier</code>(判据=“熵”)</p><p id="62b2" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv"> 5。模型分数</strong></p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="a212" class="ne li iu nt b gz ov ow l ox oy">model.score(x,y)</span></pre><p id="97ff" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">输出:1.0</p><p id="6a14" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">(由于我们取了一个非常小的数据集，所以分数是1。)</p><p id="0f4d" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv"> 6。模型预测</strong></p><p id="9170" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们预测一个47岁的人，身体质量指数29岁，是否会有糖尿病。数据集中有相同的数据。</p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="32bc" class="ne li iu nt b gz ov ow l ox oy">model.predict([[29,47]])</span></pre><p id="5d18" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">输出:<code class="fe nq nr ns nt b">array([‘no’], dtype=object)</code></p><p id="062c" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">预测为<em class="nb">否</em>，与数据集中相同。</p><p id="76ff" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">让我们预测一个47岁的人，身体质量指数45岁，是否会有糖尿病。该数据不在数据集中。</p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="9c0c" class="ne li iu nt b gz ov ow l ox oy">model.predict([[45,47]])</span></pre><p id="e999" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">输出:<code class="fe nq nr ns nt b">array([‘yes’], dtype=object)</code></p><p id="f68a" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">预测为<em class="nb">是</em>。</p><p id="4725" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">7.将模型可视化</p><pre class="kk kl km kn gu or nt os ot aw ou bi"><span id="509f" class="ne li iu nt b gz ov ow l ox oy">tree.plot_tree(model)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pd"><img src="../Images/c52869ec2727fc599cc8f921b5941985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYO2AjOAUY0AXMWCU3g66Q.png"/></div></div></figure></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><h1 id="1dca" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">GitHub链接</h1><p id="efea" class="pw-post-body-paragraph lz ma iu mb b mc md jv me mf mg jy mh mi mj mk ml mm mn mo mp mq mr ms mt mu in bi translated">本文使用的代码和数据集可以在我的<a class="ae kz" href="https://github.com/IndhumathyChelliah/DecisionTree" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>中获得。</p><h1 id="514b" class="lh li iu bd lj lk pe lm ln lo pf lq lr ka pg kb lt kd ph ke lv kg pi kh lx ly bi translated">我关于机器学习的其他博客</h1><div class="pj pk gq gs pl pm"><a href="https://towardsdatascience.com/line-of-best-fit-in-linear-regression-13658266fbc8" rel="noopener follow" target="_blank"><div class="pn ab fp"><div class="po ab pp cl cj pq"><h2 class="bd iv gz z fq pr fs ft ps fv fx it bi translated">线性回归中的最佳拟合线</h2><div class="pt l"><h3 class="bd b gz z fq pr fs ft ps fv fx dk translated">相关系数、决定系数、模型系数</h3></div><div class="pu l"><p class="bd b dl z fq pr fs ft ps fv fx dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa kt pm"/></div></div></a></div><div class="pj pk gq gs pl pm"><a href="https://towardsdatascience.com/logistic-regression-in-python-2f965c355b93" rel="noopener follow" target="_blank"><div class="pn ab fp"><div class="po ab pp cl cj pq"><h2 class="bd iv gz z fq pr fs ft ps fv fx it bi translated">Python中的逻辑回归</h2><div class="pt l"><h3 class="bd b gz z fq pr fs ft ps fv fx dk translated">详细的逻辑回归</h3></div><div class="pu l"><p class="bd b dl z fq pr fs ft ps fv fx dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="qb l px py pz pv qa kt pm"/></div></div></a></div><div class="pj pk gq gs pl pm"><a href="https://towardsdatascience.com/an-introduction-to-support-vector-machine-3f353241303b" rel="noopener follow" target="_blank"><div class="pn ab fp"><div class="po ab pp cl cj pq"><h2 class="bd iv gz z fq pr fs ft ps fv fx it bi translated">支持向量机简介</h2><div class="pt l"><h3 class="bd b gz z fq pr fs ft ps fv fx dk translated">如何在分类问题中使用SVM？</h3></div><div class="pu l"><p class="bd b dl z fq pr fs ft ps fv fx dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="qc l px py pz pv qa kt pm"/></div></div></a></div><div class="pj pk gq gs pl pm"><a href="https://towardsdatascience.com/an-introduction-to-k-nearest-neighbours-algorithm-3ddc99883acd" rel="noopener follow" target="_blank"><div class="pn ab fp"><div class="po ab pp cl cj pq"><h2 class="bd iv gz z fq pr fs ft ps fv fx it bi translated">K-最近邻算法简介</h2><div class="pt l"><h3 class="bd b gz z fq pr fs ft ps fv fx dk translated">什么是KNN？</h3></div><div class="pu l"><p class="bd b dl z fq pr fs ft ps fv fx dk translated">towardsdatascience.com</p></div></div><div class="pv l"><div class="qd l px py pz pv qa kt pm"/></div></div></a></div><div class="pj pk gq gs pl pm"><a href="https://pub.towardsai.net/naive-bayes-classifier-in-machine-learning-b0201684607c" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fp"><div class="po ab pp cl cj pq"><h2 class="bd iv gz z fq pr fs ft ps fv fx it bi translated">机器学习中的朴素贝叶斯分类器</h2><div class="pt l"><h3 class="bd b gz z fq pr fs ft ps fv fx dk translated">使用sklearn的数学解释和python实现</h3></div><div class="pu l"><p class="bd b dl z fq pr fs ft ps fv fx dk translated">pub.towardsai.net</p></div></div><div class="pv l"><div class="qe l px py pz pv qa kt pm"/></div></div></a></div></div><div class="ab cl la lb hy lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="in io ip iq ir"><p id="b5f8" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">我希望这篇文章对你有所帮助。</p><p id="3355" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><em class="nb">请关注此空间，获取更多关于Python和数据科学的文章。如果你喜欢多看我的教程，就关注我的</em> <a class="ae kz" href="https://medium.com/@IndhumathyChelliah" rel="noopener"> <strong class="mb iv"> <em class="nb">中</em></strong></a><a class="ae kz" href="https://www.linkedin.com/in/indhumathy-chelliah/" rel="noopener ugc nofollow" target="_blank"><strong class="mb iv"><em class="nb">LinkedIn</em></strong></a><strong class="mb iv"><em class="nb"/></strong><a class="ae kz" href="https://twitter.com/IndhuChelliah" rel="noopener ugc nofollow" target="_blank"><strong class="mb iv"><em class="nb">推特</em> </strong> </a> <strong class="mb iv"> <em class="nb">。</em>T51】</strong></p><p id="f032" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated"><strong class="mb iv"> <em class="nb">点击此处成为中等会员:</em> </strong></p><p id="17a4" class="pw-post-body-paragraph lz ma iu mb b mc mv jv me mf mw jy mh mi mx mk ml mm my mo mp mq mz ms mt mu in bi translated">【https://indhumathychelliah.medium.com/membership】T5<a class="ae kz" href="https://indhumathychelliah.medium.com/membership" rel="noopener">T6</a></p></div></div>    
</body>
</html>