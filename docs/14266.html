<html>
<head>
<title>Semantic Search With HuggingFace and Elasticsearch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用HuggingFace和Elasticsearch进行语义搜索</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/implementing-nearest-neighbour-search-with-elasticsearch-c59a8d33dd9d?source=collection_archive---------1-----------------------#2022-11-23">https://betterprogramming.pub/implementing-nearest-neighbour-search-with-elasticsearch-c59a8d33dd9d?source=collection_archive---------1-----------------------#2022-11-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2805" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们使用最近邻搜索对数据集中的段落进行排序</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ca7af4cf21f724eb1f4d4789fe730836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2-zK3Qn5SXEOST4A"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·温克勒在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="80bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">密集嵌入是机器学习中的游戏改变者，尤其是在搜索引擎和推荐系统中。密集嵌入目前被应用于自组织信息检索、产品搜索、推荐引擎等。许多公司目前正在他们的工作流程中采用某种形式的基于嵌入的搜索。</p><p id="c97f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些关于当前顶级公司如何集成密集嵌入的文章有<a class="ae kv" href="https://sigir-ecom.github.io/ecom22Papers/paper_8392.pdf" rel="noopener ugc nofollow" target="_blank"> Instacart </a>、<a class="ae kv" href="https://doordash.news/company/powering-search-recommendations-at-doordash/" rel="noopener ugc nofollow" target="_blank"> DoorDash </a>、<a class="ae kv" href="https://www.etsy.com/codeascraft/deep-learning-for-search-ranking-at-etsy?utm_source=pocket_reader" rel="noopener ugc nofollow" target="_blank"> Etsy </a>、<a class="ae kv" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank"> Google </a>和<a class="ae kv" href="https://medium.com/airbnb-engineering/improving-deep-learning-for-ranking-stays-at-airbnb-959097638bde" rel="noopener"> Airbnb </a>。</p><p id="b3a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Google的Talk to books很好地演示了如何使用密集向量进行搜索。这种形式的搜索通常被称为语义搜索。该演示使用编码器模型从存储在索引中的文档(在此上下文中是书籍)生成嵌入，并在搜索时与查询向量进行比较，以检索与给定查询最相似的文档。语义搜索是对BM25等传统关键词搜索算法的重大升级，因为它可以检索与给定查询相关的文档，但不一定包含与查询完全相同的单词。</p><blockquote class="ls lt lu"><p id="498b" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky ir">边注</strong>:演示中使用的这个模型(<a class="ae kv" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank">通用语句编码器</a>))在深度学习谱系中有些陈旧，有一些模型可以产生更好的嵌入，其中一些可以在这里找到<a class="ae kv" href="https://huggingface.co/models?pipeline_tag=sentence-similarity&amp;sort=downloads" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><h1 id="1ffb" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">什么是密集嵌入？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/edae136fdba96dbdd3ddac515897fa77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r96tb0gQwA0VIBwgXqKRdg.png"/></div></div></figure><p id="ec84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">密集嵌入是数据(文本、用户、产品等)的数字表示。)使用高维向量。密集向量具有不同的长度，并被期望对关于原始数据的信息进行编码，以便使用像<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>这样的向量相似度算法来容易地找到相似的数据点。下面是一个简单的实现:</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="9299" class="mx ma iq mt b be my mz l na nb">from sklearn.metrics.pairwise import cosine_similarity<br/>from numpy import random<br/><br/>array_vec_1 = random.rand(1,10)<br/>array_vec_2 = random.rand(1,10)<br/>print(cosine_similarity(array_vec_1, array_vec_2))</span></pre><blockquote class="ls lt lu"><p id="f991" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky ir">边注</strong>:两个向量的余弦相似度越高，越相似。余弦相似性也被用作训练神经网络的损失函数。参见py torch<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html" rel="noopener ugc nofollow" target="_blank">cosinembeddingloss</a></p></blockquote><p id="0084" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了生成高质量的信息丰富的嵌入，您需要已经在数百万成对示例上训练过的机器学习模型，并且有一些训练技术(例如，使用硬否定的对比学习)已经被应用于生成高质量的嵌入。对于一般的语义搜索和句子表示，在<a class="ae kv" href="https://huggingface.co/models?pipeline_tag=sentence-similarity&amp;sort=downloads" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>上有大量公开可用的预训练或微调模型🤗以及一些商用API如<a class="ae kv" href="https://docs.cohere.ai/docs/embeddings" rel="noopener ugc nofollow" target="_blank"> cohere嵌入</a>和<a class="ae kv" href="https://beta.openai.com/docs/guides/embeddings" rel="noopener ugc nofollow" target="_blank"> OpenAI嵌入</a>用于编码文本。</p><h1 id="4c25" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">索引和搜索</h1><p id="a768" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在对我们的文档进行编码(生成文档嵌入)之后，我们现在必须考虑索引向量和搜索密集索引。</p><p id="aea3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">向量搜索通常使用聚类算法(如最近邻搜索)来完成，由于各种原因，它可能是计算密集型的，并且实施起来具有挑战性，其中一些原因是:</p><p id="fe7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(I .)一些编码器产生具有大尺寸的表示向量。大的嵌入导致大的嵌入表，当执行向量操作时具有高的存储器成本，并且可能增加搜索等待时间。</p><p id="ec00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(二。)用新的向量更新密集索引可能要求很高，因为您可能需要为新的向量更新索引簇。</p><p id="76dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些开源库已经为快速矢量搜索建立起来，以解决上述问题，如Meta的<a class="ae kv" href="https://github.com/facebookresearch/faiss" rel="noopener ugc nofollow" target="_blank"> Faiss </a>，Spotify的<a class="ae kv" href="https://github.com/spotify/annoy" rel="noopener ugc nofollow" target="_blank">angry</a>，谷歌的<a class="ae kv" href="https://github.com/google-research/google-research/tree/master/scann" rel="noopener ugc nofollow" target="_blank"> ScaNN </a>。在<a class="ae kv" href="https://www.elastic.co/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0" rel="noopener ugc nofollow" target="_blank">8.0</a>+版本中，Elasticsearch宣布他们流行的开源搜索引擎现在支持最近邻搜索。</p><blockquote class="ls lt lu"><p id="1731" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky ir">旁注</strong> : <a class="ae kv" href="https://news.ycombinator.com/item?id=29554986" rel="noopener ugc nofollow" target="_blank">参见关于使用密集向量实现语义搜索的缺点和解决方案的讨论</a>。此外，<a class="ae kv" href="http://ann-benchmarks.com/" rel="noopener ugc nofollow" target="_blank">关于不同的近似最近邻算法搜索算法和库，参见基准</a>。</p></blockquote><h1 id="2256" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">使用弹性搜索的近似搜索</h1><p id="5973" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">让我们进入有趣的部分！</p><p id="9789" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将对由880万篇文章组成的<a class="ae kv" href="https://microsoft.github.io/msmarco/Datasets.html" rel="noopener ugc nofollow" target="_blank">MARCO女士</a>文章排名集合进行编码和索引。目标是根据段落与给定查询的相关性对它们进行排序。下载并解压缩该集合。</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="462d" class="mx ma iq mt b be my mz l na nb">wget https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/msmarco.zip<br/>unzip msmarco.zip</span></pre><p id="ca67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用下面的代码在下载后预览数据:</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="c603" class="mx ma iq mt b be my mz l na nb">#inspect the data<br/>head -1 msmarco/corpus.jsonl<br/><br/>#output<br/>"""<br/>{<br/>  '_id': '0', <br/>  'title': '', <br/>  'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', <br/>  'metadata': {}<br/>}<br/>"""<br/></span></pre><p id="92f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们需要对数据集进行编码。对于本教程，我们将使用托管在HuggingFace上的<a class="ae kv" href="https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5" rel="noopener ugc nofollow" target="_blank">“句子-变形金刚/ms Marco-MiniLM-L6-cos-V5”</a>模型。该模型在MS Marco passage ranking集合上训练，为每个输入序列产生384长度的嵌入。我们需要定义一个编码器函数，它接受一段或一批文本，并为每个文本生成嵌入。</p><p id="167e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型为输入句子中的每个标记生成不同的嵌入集。我们使用平均池(也称为平均池)来聚集嵌入。或者，我们可以使用为[CLS]令牌生成的嵌入向量。</p><blockquote class="ls lt lu"><p id="deda" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">旁注:<a class="ae kv" href="https://blog.ml6.eu/the-art-of-pooling-embeddings-c56575114cf8" rel="noopener ugc nofollow" target="_blank">这里有一个关于合用的初级读本</a></p></blockquote><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="ff25" class="mx ma iq mt b be my mz l na nb">class MSMarcoEncoder:<br/>    def __init__(self, model_name: str, device : str='cpu'):<br/>        self.device = device<br/>        self.tokenizer = AutoTokenizer.from_pretrained(model_name)<br/>        self.model = AutoModel.from_pretrained(model_name)<br/>        self.model.to(self.device)<br/><br/>    def encode(self, text:str, max_length: int):<br/>        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)<br/>        inputs = inputs.to(self.device)<br/>        with torch.no_grad():<br/>            model_output = self.model(**inputs, return_dict=True)<br/>        # Perform pooling<br/>        embeddings = self.mean_pooling(model_output, inputs['attention_mask'])<br/>        # Normalize embeddings<br/>        embeddings = F.normalize(embeddings, p=2, dim=1)<br/>        return embeddings.detach().cpu().numpy()<br/><br/>    def mean_pooling(self, model_output, attention_mask):<br/>        token_embeddings = model_output[0] #First element of model_output contains all token embeddings<br/>        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br/>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)<br/><br/>if __name__ == "__main__":<br/>    encoder = Encoder('sentence-transformers/msmarco-MiniLM-L6-cos-v5')<br/>    embeddings = encoder.encode(batch_info['text'], 512)<br/>    </span></pre><p id="1d47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了编码器，我们需要在Elasticsearch中索引数据，为了有效地做到这一点，我们需要定义一个迭代器来批量循环数据。然而，我们需要设置并确保Elasticsearch正在运行。首先，我们按照这里的<a class="ae kv" href="https://www.elastic.co/downloads/elasticsearch" rel="noopener ugc nofollow" target="_blank">指令在本地下载并启动elasticsearch服务器</a>或者按照<a class="ae kv" href="https://www.elastic.co/downloads/elasticsearch" rel="noopener ugc nofollow" target="_blank">这个</a>启动一个elastic search容器。</p><p id="8d45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于本地设置，在下载elasticsearch之后，运行以下命令来启动一个具有一个节点的集群:</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="0d1d" class="mx ma iq mt b be my mz l na nb">./elasticsearch-8.5.1/bin/elasticsearch</span></pre><p id="fda7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">默认情况下，ElasticSearch 8.0及更高版本启用了安全性，因此请通过运行以下命令来验证您是否可以连接到正在运行的ElasticSearch集群:</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="96d0" class="mx ma iq mt b be my mz l na nb">curl --cacert config/certs/http_ca.crt -u elastic https://localhost:9200</span></pre><p id="6fc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们的elasticsearch服务器正在运行，我们需要创建一个索引来存储数据。为此，我们需要为数据集中的不同字段定义一个映射。然而，对于最近邻搜索，我们需要定义一个类型为<code class="fe nh ni nj mt b">dense_vector</code>的字段，它将包含每个文档的嵌入内容。</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="59e0" class="mx ma iq mt b be my mz l na nb">from elasticsearch import Elasticsearch<br/><br/>es_client = Elasticsearch( "https://localhost:9200", <br/>                  http_auth=("username", "password"),<br/>                  verify_certs=False)<br/>config = {<br/>    "mappings": {<br/>        "properties": {<br/>            "title": {"type": "text"},<br/>            "text": {"type": "text"},<br/>            "embeddings": {<br/>                    "type": "dense_vector",<br/>                    "dims": 384,<br/>                    "index": false<br/>                }<br/>            }<br/>    },<br/>    "settings": {<br/>        "number_of_shards": 2,<br/>        "number_of_replicas": 1<br/>    }<br/>}<br/><br/>es_client.indices.create(<br/>    index="msmarco-demo",<br/>    settings=config["settings"],<br/>    mappings=config["mappings"],<br/>)<br/><br/>#check if the index has been created successfully<br/>print(es.indices.exists(index=["msmarco-demo"]))<br/>#True</span></pre><p id="245c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段使用elasticsearch的批量索引API来批量索引文档。下面的代码在<code class="fe nh ni nj mt b">cpu</code>上运行时间更长，在<code class="fe nh ni nj mt b">gpu or tpu</code>上运行速度更快。这里我们使用一个<a class="ae kv" href="https://gist.github.com/ToluClassics/bc4bbd8b7930ee35e237984c2c9998a1#file-knn_elasticsearch-py-L10-98" rel="noopener ugc nofollow" target="_blank">集合迭代器类</a>来遍历数据集。对于每一批，我们生成嵌入并索引它们。</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="94e7" class="mx ma iq mt b be my mz l na nb">collection_path = 'path/to/corpus.jsonl'<br/>collection_iterator = JsonlCollectionIterator(collection_path, fields=['title','text'])<br/>encoder = Encoder('sentence-transformers/msmarco-MiniLM-L6-cos-v5')<br/>index_name = "msmarco-demo"<br/><br/>for batch_info in collection_iterator(batch_size=256, shard_id=0, shard_num=1):<br/>    embeddings = encoder.encode(batch_info['text'], 512)<br/>    batch_info["dense_vectors"] = embeddings<br/><br/>    actions = []<br/>    for i in range(len(batch_info['id'])):<br/>        action = {"index": {"_index": index_name, "_id": batch_info['id'][i]}}<br/>        doc = {<br/>                "title": batch_info['title'][i],<br/>                "text": batch_info['text'][i],<br/>                "embeddings": batch_info['dense_vectors'][i].tolist()<br/>            }<br/>        actions.append(action)<br/>        actions.append(doc)<br/>    <br/>    es_client.bulk(index=index_name, operations=actions)<br/><br/>result = es_client.count(index=index_name)<br/><br/>#print the total number of documents in the index<br/>print(result.body['count'])<br/>#8841823<br/><br/>#output one document<br/>print(es_client.get(index=["msmarco-demo"], id="0", request_timeout=60))<br/><br/>'''<br/>{'_index': 'msmarco-demo', '_id': '0', '_version': 2, '_seq_no': 27, '_primary_term': 1, 'found': True, <br/>'_source': {'title': '', <br/>'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', <br/>'embeddings': [-0.032267116010189056, 0.05750396102666855,...]}}<br/>'''</span></pre><p id="8489" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在索引已经完成，我们可以搜索我们的索引。Elasticsearch提供了一个<a class="ae kv" href="https://elasticsearch-py.readthedocs.io/en/latest/api.html#elasticsearch.Elasticsearch.knn_search" rel="noopener ugc nofollow" target="_blank"> python包装器来执行KNN搜索</a>，这就是我们在下面代码片段中定义的搜索代码函数中使用的。为了进行搜索，我们需要使用参数<code class="fe nh ni nj mt b">k</code>和<code class="fe nh ni nj mt b">query_vector</code>中的查询嵌入来定义搜索集群的数量。</p><pre class="kg kh ki kj gt ms mt mu bn mv mw bi"><span id="a89f" class="mx ma iq mt b be my mz l na nb">def search(query: str, es_client: Elasticsearch, model: str, index: str, top_k: int = 10):<br/><br/>    encoder = Encoder(model)<br/>    query_vector = encoder.encode(query, max_length=64)<br/>    query_dict = {<br/>        "field": "embeddings",<br/>        "query_vector": query_vector[0].tolist(),<br/>        "k": 10,<br/>        "num_candidates": top_k<br/>    }<br/>    res = es_client.knn_search(index=index, knn=query_dict, source=["title", "text", "id"])<br/><br/>    for hit in res["hits"]["hits"]:<br/>        print(hit)<br/>        print(f"Document ID: {hit['_id']}")<br/>        print(f"Document Title: {hit['_source']['title']}")<br/>        print(f"Document Text: {hit['_source']['text']}")<br/>        print("=======================================================\n")<br/><br/><br/>if __name__ == "__main__":<br/><br/>  search(query="What is the capital of France?", <br/>         es_client=es_client, <br/>         model="sentence-transformers/msmarco-MiniLM-L6-cos-v5", <br/>         index=index_name)<br/>#output<br/>"""<br/>{'_index': 'msmarco-demo', '_id': '82390', '_score': 0.81541693, '_source': {'text': "In terms of total household wealth, France is the wealthiest nation in Europe and fourth in the world. It also possesses the world's second-largest exclusive economic zone (EEZ), covering 11,035,000 square kilometres (4,261,000 sq mi).", 'title': ''}}<br/>Document ID: 82390<br/>Document Title: <br/>Document Text: In terms of total household wealth, France is the wealthiest nation in Europe and fourth in the world. It also possesses the world's second-largest exclusive economic zone (EEZ), covering 11,035,000 square kilometres (4,261,000 sq mi).<br/>=====================================================================<br/><br/>{'_index': 'msmarco-demo', '_id': '162291', '_score': 0.80739325, '_source': {'text': 'Paris in France lies on the Seine River. The docking location is Port de Grenelle/Quai de Grenelle. As one of the largest cities in Europe, finding a property that suit your budget is not a problem. Choose between low cost guest rooms to luxury 4 and 5 star hotels and apartments to rent.', 'title': ''}}<br/>Document ID: 162291<br/>Document Title: <br/>Document Text: Paris in France lies on the Seine River. The docking location is Port de Grenelle/Quai de Grenelle. As one of the largest cities in Europe, finding a property that suit your budget is not a problem. Choose between low cost guest rooms to luxury 4 and 5 star hotels and apartments to rent.<br/>=====================================================================<br/>"""</span></pre><p id="4422" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">向量搜索现在是机器学习中的一个时髦领域，有很多其他很棒的库提供向量搜索功能，如<a class="ae kv" href="https://www.pinecone.io/" rel="noopener ugc nofollow" target="_blank">松果</a>、<a class="ae kv" href="https://jina.ai/" rel="noopener ugc nofollow" target="_blank">纪娜AI </a>、<a class="ae kv" href="https://weaviate.io/" rel="noopener ugc nofollow" target="_blank"> Weaviate </a>、<a class="ae kv" href="https://qdrant.tech/" rel="noopener ugc nofollow" target="_blank"> Qdrant </a>等。</p><p id="ecd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。用Github Copilot开发这个很有趣。要离线索引和搜索，请不要犹豫，来看看我们令人敬畏的图书馆<a class="ae kv" href="https://github.com/castorini/pyserini" rel="noopener ugc nofollow" target="_blank"> Pyserini </a>。Pyserini主要设计用于在多级排名架构中提供有效的、可重复的、易于使用的第一级检索。</p><p id="a4ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个教程的完整代码可以在<a class="ae kv" href="https://gist.github.com/ToluClassics/bc4bbd8b7930ee35e237984c2c9998a1" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="3731" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><ol class=""><li id="3dc6" class="nk nl iq ky b kz nc lc nd lf nm lj nn ln no lr np nq nr ns bi translated"><a class="ae kv" href="https://towardsdatascience.com/how-to-index-elasticsearch-documents-with-the-bulk-api-in-python-b5bb01ed3824" rel="noopener" target="_blank">https://towards data science . com/how-to-index-elastic search-documents-with-the-bulk-API-in-python-b5 bb 01 ed 3824</a></li><li id="cc9c" class="nk nl iq ky b kz nt lc nu lf nv lj nw ln nx lr np nq nr ns bi translated"><a class="ae kv" href="https://www.elastic.co/" rel="noopener ugc nofollow" target="_blank">https://www.elastic.co/</a></li></ol></div></div>    
</body>
</html>