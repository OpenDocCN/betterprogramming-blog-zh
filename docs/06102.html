<html>
<head>
<title>Running Create ML Style Transfer Models in an iOS Camera Application</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在iOS相机应用程序中运行创建ML风格传输模型</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/train-and-run-a-create-ml-style-transfer-model-in-an-ios-camera-application-84aab3b85458?source=collection_archive---------7-----------------------#2020-09-02">https://betterprogramming.pub/train-and-run-a-create-ml-style-transfer-model-in-an-ios-camera-application-84aab3b85458?source=collection_archive---------7-----------------------#2020-09-02</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="92fc" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">建立一个艺术相机，看看它如何在A13仿生芯片的神经引擎上执行</h2></div><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/3179a39610bb320a52ca2bf5d840aae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1uWUnykKC6s9g5Op"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kw" href="https://unsplash.com/@zvessels55?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">扎克船只</a>拍摄</p></figure><p id="2ade" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">风格转移是一个非常流行的深度学习任务，它让你通过应用另一个图像的视觉风格来改变一个图像的构图。</p><p id="247b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">从构建艺术图片编辑器到通过<a class="ae kw" href="https://9to5google.com/2019/03/19/google-stadia-style-transfer-ml/" rel="noopener ugc nofollow" target="_blank">最先进的主题</a>给你的游戏设计赋予新的外观，你可以用神经风格转移模型构建许多令人惊叹的东西。它也可以是方便的或数据扩充。</p><p id="3007" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><a class="ae kw" href="https://developer.apple.com/videos/play/wwdc2020/10642/#:~:text=Style%20Transfer%20is%20a%20new,style%20and%20content%20image%20together." rel="noopener ugc nofollow" target="_blank">在2020年WWDC奥运会上，Create ML </a>(苹果的模型构建框架)因包含了风格转移模型而获得了巨大的推动。虽然Xcode 12更新附带了这个工具，但是你需要macOS Big Sur(在撰写本文时还在测试阶段)来训练风格转换模型。</p><h1 id="1b0a" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">初探Create ML的风格转移</h1><p id="4658" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">Create ML现在释放了直接从MacBook训练风格转移模型的潜力。您可以训练图像和视频风格的传输卷积神经网络，后者仅使用有限的卷积滤波器集，使其为实时图像处理而优化。</p><p id="3a4c" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">要开始，你需要三样东西:</p><ul class=""><li id="21c8" class="mq mr ir kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">样式图像(也称为样式参考图像)。通常，您可以使用名画或抽象艺术图像来让您的模型学习和传授风格。在我们的例子中，我们将在第一个模型中使用铅笔素描图像(查看下面的截图)。</li><li id="cf98" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">在训练过程中帮助可视化模型质量的验证图像。</li><li id="bad2" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">作为我们训练数据的内容图像数据集。为了获得最佳结果，最好使用与运行推理时使用的图像目录相似的图像目录。</li></ul><p id="825a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在本文中，我将使用<a class="ae kw" href="https://www.kaggle.com/jessicali9530/celeba-dataset" rel="noopener ugc nofollow" target="_blank">这个名人图像数据集</a>作为我们的内容图像。</p><p id="2bec" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这里有一个在训练模型之前，我的创建ML样式传递设置选项卡的外观。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj ne"><img src="../Images/2555130680a95702736b12868c957df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CZv5HvelAReQ9CTDCmJ7g.png"/></div></div></figure><p id="5258" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">下面的验证图像显示了在每个迭代间隔应用的实时样式转换。以下是它的一瞥:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj ne"><img src="../Images/71f46fc796088f5f20ac4b7e75d8e969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3jjpOQiIVMMEz653SIR5Q.png"/></div></div></figure><p id="0bf9" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">值得注意的是，样式损失和内容损失图是理解样式和内容图像之间平衡的指标。典型地，风格损失应该随着时间减少，这表明神经网络正在学习采用风格图像的艺术特征。</p><p id="d365" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">虽然默认的模型参数工作得很好，但是Create ML允许我们为特定的用例定制它们。</p><p id="3cf8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">“低风格强度”参数仅调整带有风格图像的背景部分，从而保持主要主体完整。同时，将“样式强度”参数设置为高会在图像边缘赋予更多样式纹理。</p><p id="3d53" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">类似地，粗略的样式密度使用样式图像的高级细节(这样的模型被训练得更快)，而精细的密度让模型学习微小的细节。</p><p id="f797" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">创建ML风格转移模型训练，默认迭代次数设置为500，这对于大多数用例来说是理想的。迭代是完成一个历元所需的批次数量。一个历元等于整个数据集的一个训练周期。例如，如果训练数据集由500个图像组成，批大小为50，则表明10次迭代将完成一个时期(注意:创建ML模型训练不会告诉您批大小)。</p><p id="3964" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">今年，Create ML还引入了一个名为模型快照的新功能。这使我们能够在培训期间捕获中间核心ML模型，并将其导出到您的应用中。然而，从快照中使用的模型没有针对大小进行优化，并且比训练完成时生成的模型大得多(具体来说，我拍摄的快照的核心ML模型大小在5–6mb的范围内，而最终的模型大小为596 KB)。</p><p id="2872" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">下面的gif展示了一个这样的例子，其中我比较了不同迭代的模型快照结果:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj nf"><img src="../Images/31fd4552d4acfb8ff7f3b0ea49c3cd9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PyPIjKVJTV-piFo4TAdMug.gif"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">这个女孩是一名演员。那个人就是我。</p></figure><p id="f027" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">请注意，在其中一张图片上，样式并不是由完整的图片组成的。这是因为使用的造型图像尺寸较小。因此，网络无法学习足够的样式信息，导致合成的图像质量低于标准。</p><p id="fb1c" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">理想情况下，样式图像的最小尺寸为512 px将确保良好的效果。</p><h1 id="d51f" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">我们的目标</h1><p id="5ca4" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">在接下来的小节中，我们将构建一个iOS应用程序，实时运行风格转换模型。下面是我们下一步行动的概览:</p><ul class=""><li id="8b11" class="mq mr ir kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">分析三种视频风格传输神经网络模型的结果。其中一个使用默认参数训练，其他使用设置为高和低的风格强度参数。</li><li id="a3ad" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">在我们的iOS应用程序中使用AVFoundation实现自定义摄像机。</li><li id="ddb1" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">在实时摄像机上运行生成的核心ML模型。我们将使用视觉请求在屏幕上快速运行、推断和绘制风格化的摄像机画面。</li><li id="9fe6" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">查看CPU、GPU和神经引擎的结果。</li></ul><p id="b824" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">找到一个能给出好的艺术效果的风格图像是很棘手的。幸运的是，<a class="ae kw" href="https://www.google.com/search?hl=en-IN&amp;tbs=simg:CAQSpAIJkkL3sol2FMEamAILELCMpwgaYgpgCAMSKK8D-haFDPcWiRGhHpAXpxyhHIoRlCngKLgnhSOGJ-Ah3yjbJIYjnjcaMEWBtZsgkthHw07P5xre1gkdsfh4WFLK3PKZCL6k-vXjH_14Ga7h8HhFXsrDVBN2ZZyAEDAsQjq7-CBoKCggIARIEmiMCUwwLEJ3twQkakAEKHQoLdmlzdWFsIGFydHPapYj2AwoKCC9tLzBwOXh4Ch0KCm1vZGVybiBhcnTapYj2AwsKCS9tLzAxNXI2MQobCgl0dXJxdW9pc2XapYj2AwoKCC9tLzBmZ2toChgKBWxpbGFj2qWI9gMLCgkvbS8wNGZmY2oKGQoHcGF0dGVybtqliPYDCgoIL20vMGh3a3kM&amp;sxsrf=ALeKk00kkDblF7mP6fxtp3wyd8r-4SVquQ:1597937500373&amp;q=texture+graphic+design&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwiC2dSIjarrAhVTzjgGHdnmDg8Qwg4oAHoECAkQKQ&amp;biw=1440&amp;bih=820#imgrc=GeL5TbsFDvBUWM" rel="noopener ugc nofollow" target="_blank">我通过简单的谷歌搜索找到了一张这样的图片</a>。</p></div><div class="ab cl nh ni hv nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ik il im in io"><h1 id="157c" class="lt lu ir bd lv lw no ly lz ma np mc md jx nq jy mf ka nr kb mh kd ns ke mj mk bi translated">分析不同优势的风格转移模型</h1><p id="c179" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">我已经用相同的数据集训练了三个模型。以下是结果:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj nt"><img src="../Images/a1c67f63c8182493364d40b40d029b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*G7sbKaxtDvgyCQm5QALpnw.gif"/></div></div></figure><p id="4afc" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">正如您在上面看到的，低强度模型几乎不会影响给定样式图像的内容图像，但是高强度模型会通过更多的样式效果来细化边缘。</p><p id="341a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这样，我们的模型(大小大约为半MB)就可以加载到我们的应用程序中了。</p><p id="0636" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">Create ML还可以让我们预览视频结果，但它非常慢。幸运的是，我们将很快在我们的演示应用程序中实时看到它们。</p></div><div class="ab cl nh ni hv nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ik il im in io"><h1 id="28d8" class="lt lu ir bd lv lw no ly lz ma np mc md jx nq jy mf ka nr kb mh kd ns ke mj mk bi translated">AVFoundation基础</h1><p id="6c7b" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">AVFoundation是一个高度可定制的苹果媒体内容框架。您可以绘制自定义叠加，微调相机设置，使用深度输出进行照片分割，以及分析帧。</p><p id="f2ce" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">我们将主要关注分析帧，特别是使用样式转换对它们进行转换，并在图像视图中显示它们，以构建一个实时摄像机馈送(您还可以使用Metal进行进一步优化，但为了简单起见，我们将在本教程中跳过这一步)。</p><p id="2822" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在非常基本的层面上，构建自定义摄像机涉及以下组件:</p><ul class=""><li id="3e32" class="mq mr ir kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated"><code class="fe nu nv nw nx b">AVCaptureSession</code> —管理摄像机的整个会话。它的功能包括访问iOS输入设备并将数据传递给输出设备。AVCaptureSession还允许我们为不同的捕获会话定义<code class="fe nu nv nw nx b"><a class="ae kw" href="https://developer.apple.com/documentation/avfoundation/avcapturesession/preset" rel="noopener ugc nofollow" target="_blank">Preset</a></code>类型。</li><li id="b71f" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><code class="fe nu nv nw nx b">AVCaptureDevice</code> —让我们选择前置或后置摄像头。我们可以选择默认设置，或者使用<code class="fe nu nv nw nx b">AVCaptureDevice.DiscoverySession</code>来过滤和选择硬件特定的功能，如TrueDepth或广角摄像头。</li><li id="0355" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><code class="fe nu nv nw nx b">AVCaptureDeviceInput</code> —从捕获设备提供媒体源，并将其发送到捕获会话。</li><li id="7e56" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><code class="fe nu nv nw nx b">AVCaptureOutput</code> —向捕获会话提供输出媒体的抽象类。它也让我们处理相机的方向。我们可以设置多个输出(如摄像头和麦克风)。例如，如果你想捕捉照片和电影，添加<code class="fe nu nv nw nx b">AVCaptureMovieFileOutput</code>和<code class="fe nu nv nw nx b">AVCapturePhotoOutput</code>。在我们的例子中，我们将使用<code class="fe nu nv nw nx b">AVCaptureVideoDataOutput</code>，因为它提供视频帧进行处理。</li><li id="38cf" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><code class="fe nu nv nw nx b">AVCaptureVideoDataOutputSampleBufferDelegate</code>是一个协议，我们可以用它来访问<code class="fe nu nv nw nx b">didOutput</code>委托方法中的每个帧缓冲区。为了开始接收帧，我们需要调用<code class="fe nu nv nw nx b">AVCaptureVideoDataOutput</code>上的<code class="fe nu nv nw nx b">setSampleBufferDelegate</code>方法</li><li id="909e" class="mq mr ir kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><code class="fe nu nv nw nx b">AVCaptureVideoPreviewLayer</code> —基本上是一个<code class="fe nu nv nw nx b">CALayer</code>，它直观地显示来自捕获会话输出的实时摄像机馈送。我们可以用叠加和动画来改变图层。为了示例缓冲区委托方法能够工作，设置这一点很重要。</li></ul><h1 id="f179" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">设置我们的定制摄像机</h1><p id="f171" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">首先，在Xcode的项目的<code class="fe nu nv nw nx b">info.plist</code>文件中添加<code class="fe nu nv nw nx b">NSCameraUsageDescription</code>摄像机权限。</p><p id="36f3" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">现在，是时候在<code class="fe nu nv nw nx b">ViewController.swift</code>中创建一个<code class="fe nu nv nw nx b">AVCaptureSession</code>了:</p><pre class="kh ki kj kk gu ny nx nz oa aw ob bi"><span id="f86b" class="oc lu ir nx b gz od oe l of og">let captureSession = AVCaptureSession()<br/>captureSession.sessionPreset = AVCaptureSession.Preset.medium</span></pre><p id="cc2b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">接下来，我们将从出现在<code class="fe nu nv nw nx b">AVCaptureDevic</code>实例中的可用摄像机类型列表中过滤并选择广角摄像机，并将其添加到<code class="fe nu nv nw nx b">AVCaptureInput</code>中，后者又在<code class="fe nu nv nw nx b">AVCaptureSession</code>上设置:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="oh oi l"/></div></figure><p id="dfa7" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">现在我们的输入已经设置好了，让我们将视频输出添加到捕获会话中:</p><pre class="kh ki kj kk gu ny nx nz oa aw ob bi"><span id="9efa" class="oc lu ir nx b gz od oe l of og">let videoOutput = AVCaptureVideoDataOutput()</span><span id="d315" class="oc lu ir nx b gz oj oe l of og">videoOutput.alwaysDiscardsLateVideoFrames = true</span><span id="7dfa" class="oc lu ir nx b gz oj oe l of og">videoOutput.<strong class="nx is">setSampleBufferDelegate</strong>(self, queue: DispatchQueue(label: "videoQueue"))</span><span id="e234" class="oc lu ir nx b gz oj oe l of og">if captureSession.canAddOutput(videoOutput){<br/> <strong class="nx is">captureSession.addOutput(videoOutput)</strong><br/>}</span></pre><p id="0a64" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe nu nv nw nx b">alwaysDiscardsLateVideoFrames</code>属性确保晚到达的帧被丢弃，从而确保有更少的延迟。</p><p id="6fda" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">最后，添加下面这段代码来防止旋转的相机进给:</p><pre class="kh ki kj kk gu ny nx nz oa aw ob bi"><span id="4de3" class="oc lu ir nx b gz od oe l of og">guard let connection = videoOutput.connection(with: .video) <br/>else { return }</span><span id="68ca" class="oc lu ir nx b gz oj oe l of og">guard connection.isVideoOrientationSupported else { return }<br/>connection.videoOrientation = .portrait</span></pre><blockquote class="ok ol om"><p id="33e9" class="kx ky ng kz b la lb js lc ld le jv lf on lh li lj oo ll lm ln op lp lq lr ls ik bi translated">注意:为了确保所有方向，您需要根据设备的当前方向设置<code class="fe nu nv nw nx b">videoOrientation</code>。代码可以在本教程的末尾找到。</p></blockquote><p id="ef7f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">最后，我们可以添加我们的预览层，并开始相机会话:</p><pre class="kh ki kj kk gu ny nx nz oa aw ob bi"><span id="3931" class="oc lu ir nx b gz od oe l of og">let previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)</span><span id="9ae4" class="oc lu ir nx b gz oj oe l of og">view.layer.addSublayer(previewLayer)</span><span id="cae6" class="oc lu ir nx b gz oj oe l of og">captureSession.startRunning()</span></pre><p id="5fb3" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">下面是我们刚刚创建的<code class="fe nu nv nw nx b">configureSession()</code>方法:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj oq"><img src="../Images/a550192d26285fc71266a0e9b9d0d653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzFdTTH6QOMKHNh_aI4oLg.png"/></div></div></figure></div><div class="ab cl nh ni hv nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ik il im in io"><h1 id="9811" class="lt lu ir bd lv lw no ly lz ma np mc md jx nq jy mf ka nr kb mh kd ns ke mj mk bi translated">使用视觉实时传递核心ML模型的跑步风格</h1><p id="ff6c" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">现在，到机器学习部分。我们将使用视觉框架来为我们的风格转移模型处理输入图像预处理。</p><p id="1563" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">通过使我们的ViewController符合<code class="fe nu nv nw nx b">AVCaptureVideoDataOutputSampleBufferDelegate</code>协议，下面的方法可以访问每个帧:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="oh oi l"/></div></figure><p id="9280" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">从上面的示例缓冲区实例中，我们将检索一个<code class="fe nu nv nw nx b">CVPixelBuffer</code>实例，并将其传递给Vision请求:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="oh oi l"/></div></figure><p id="e831" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe nu nv nw nx b">VNCoreMLModel</code>作为一个容器，在里面我们用下面的方式实例化了我们的核心ML模型:</p><pre class="kh ki kj kk gu ny nx nz oa aw ob bi"><span id="b53f" class="oc lu ir nx b gz od oe l of og">StyleBlue.init(configuration: config).model</span></pre><p id="f78b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe nu nv nw nx b">config</code>是类型<code class="fe nu nv nw nx b">MLModelConfiguration</code>的一个实例。它用于定义<code class="fe nu nv nw nx b">computeUnits</code>属性，该属性允许我们设置<code class="fe nu nv nw nx b">cpuOnly</code>、<code class="fe nu nv nw nx b">cpuAndGpu</code>或<code class="fe nu nv nw nx b">all</code>(神经引擎)在所需的设备硬件上运行。</p><pre class="kh ki kj kk gu ny nx nz oa aw ob bi"><span id="6102" class="oc lu ir nx b gz od oe l of og">let config = MLModelConfiguration()</span><span id="a74b" class="oc lu ir nx b gz oj oe l of og">switch currentModelConfig {<br/>case 1:<br/>config.computeUnits = .cpuOnly<br/>case 2:<br/>config.computeUnits = .cpuAndGPU<br/>default:<br/>config.computeUnits = .all</span><span id="8d47" class="oc lu ir nx b gz oj oe l of og">}</span></pre><blockquote class="ok ol om"><p id="9ee2" class="kx ky ng kz b la lb js lc ld le jv lf on lh li lj oo ll lm ln op lp lq lr ls ik bi translated">注意:我们已经设置了一个<code class="fe nu nv nw nx b">UISegmentedControl</code> UI控件，让我们在上面的每个模型配置之间切换。</p></blockquote><p id="8322" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe nu nv nw nx b">VNCoreMLModel</code>在<code class="fe nu nv nw nx b">VNCoreMLRequest</code>请求中传递，该请求返回<code class="fe nu nv nw nx b">VNPixelBufferObservation</code>类型的观察值。</p><p id="4fd9" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe nu nv nw nx b">VNPixelBufferObservation</code>是<code class="fe nu nv nw nx b">VNObservation</code>的子类，返回<code class="fe nu nv nw nx b">CVPixelBuffer</code>的图像输出。</p><p id="6502" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">通过使用下面的扩展，我们将<code class="fe nu nv nw nx b">CVPixelBuffer</code>转换成UIImage并在屏幕上绘制它。</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="oh oi l"/></div></figure><p id="ed88" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">唷！我们已经创建了我们的实时风格转移iOS应用程序。</p><p id="c83f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">以下是应用程序在iPhone SE上运行时的结果:</p><div class="kh ki kj kk gu ab cb"><figure class="or kl os ot ou ov ow paragraph-image"><img src="../Images/e9fd0363e4d1d3e310ba4a9175943185.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/1*ymlSnvaZBZPaPwu2OinMBg.gif"/></figure><figure class="or kl os ot ou ov ow paragraph-image"><img src="../Images/f969a7fd3bca3b551a2b4d22db8d7d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/1*3IMFyv4H_rEHFTAV4fAZRw.gif"/></figure><figure class="or kl os ot ou ov ow paragraph-image"><img src="../Images/a4a7a5a8d006334f5447257d3544b9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/1*tUGBzcqgCiJJUGV7hEmBWw.gif"/><p class="ks kt gk gi gj ku kv bd b be z dk ox di oy oz translated">在每个图像中，顶部的片段选择器允许您在两个模型之间进行选择。StarryBlue使用默认的样式强度进行训练，而选择“强”选项卡将运行第二个模型，该模型使用设置为“高”的样式强度进行训练</p></figure></div><p id="494f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">请注意，当在神经引擎上运行时，风格转换预测是如何近乎实时地发生的。</p><blockquote class="ok ol om"><p id="e516" class="kx ky ng kz b la lb js lc ld le jv lf on lh li lj oo ll lm ln op lp lq lr ls ik bi translated">由于gif大小和质量的限制，我还<a class="ae kw" href="https://youtu.be/rrZsRJW2R9I" rel="noopener ugc nofollow" target="_blank"> <strong class="kz is">制作了一个视频</strong> </a>，展示了CPU、GPU和神经引擎上的实时风格转换演示。比上面的gif流畅了很多。</p></blockquote><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="pa oi l"/></div></figure><p id="6cd8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">你可以在GitHub库中找到上述应用程序的完整源代码，以及核心的ML风格传输模型<a class="ae kw" href="https://github.com/anupamchugh/iOS14-Resources/tree/master/CreateMLVideoStyleTransfer" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="65b1" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">iOS 14中的Core ML引入了<a class="ae kw" href="https://developer.apple.com/documentation/coreml/core_ml_api/encrypting_a_model_in_your_app" rel="noopener ugc nofollow" target="_blank">模型加密</a>，所以理论上我可以保护这个模型。但是本着学习的精神，我选择免费提供上面创建的模型。</p></div><div class="ab cl nh ni hv nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ik il im in io"><h1 id="6940" class="lt lu ir bd lv lw no ly lz ma np mc md jx nq jy mf ka nr kb mh kd ns ke mj mk bi translated">结论</h1><p id="c2ae" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">机器学习的未来显然是无代码的，有<a class="ae kw" href="https://makeml.app/" rel="noopener ugc nofollow" target="_blank"> MakeML </a>等平台，苹果的Create ML引领潮流，为快速训练移动就绪的机器学习模型提供易于使用的工具和平台。</p><p id="4bbc" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">Create ML今年还引入了对人类活动分类的模型训练支持。但我相信风格转移会很快被iOS开发者采用。如果你想创建一个包含多种风格图像的单一模型，使用<a class="ae kw" href="https://heartbeat.comet.ml/style-transfer-on-ios-using-convolutional-neural-networks-616fd748ece4" rel="noopener ugc nofollow" target="_blank">图瑞创建</a>。</p><p id="f167" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">你现在可以以绝对零的成本构建基于人工智能的、类似Prisma的应用程序(除非你决定将你的应用程序上传到App Store！).</p><p id="b5b9" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">风格转移也可以用在ARKit对象上，给它们一个完全不同的外观。我们将在下一个教程中讨论这个问题。敬请关注。</p><p id="39f6" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这一次到此为止。感谢阅读。</p></div></div>    
</body>
</html>