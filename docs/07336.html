<html>
<head>
<title>Swipeless Tinder Using iOS 14 Vision Hand Pose Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用iOS 14视觉手姿态估计的无刷打火机</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/swipeless-tinder-using-ios-14-vision-hand-pose-estimation-64e5f00ce45c?source=collection_archive---------7-----------------------#2021-01-05">https://betterprogramming.pub/swipeless-tinder-using-ios-14-vision-hand-pose-estimation-64e5f00ce45c?source=collection_archive---------7-----------------------#2021-01-05</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="f5eb" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">让我们利用计算机视觉的力量来检测iOS中的手势</h2></div><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/6368910e0291e529693342ab8fc6dc0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hUWPESyhqfpGHj0E"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/@everhooder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">奥尔尼克</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="a062" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">iOS 14的推出为苹果的计算机视觉框架带来了一系列增强和有趣的新功能。</p><p id="4ece" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">视觉框架于2017年发布，旨在让移动应用程序开发者轻松利用复杂的计算机视觉算法。具体来说，该框架包含了一系列预先训练的深度学习模型，同时还充当了一个包装器，可以快速运行您自己的定制核心ML模型。</p><p id="6208" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在iOS 13中引入文本识别和VisionKit以提振OCR之后，苹果在iOS 14的视觉框架中将重点转向了运动和动作分类。</p><p id="ba7a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">主要是，视觉框架现在可以让你做<a class="ae kw" href="https://heartbeat.comet.ml/new-in-ios-14-vision-contour-detection-68fd5849816e" rel="noopener ugc nofollow" target="_blank">轮廓检测</a>，光流请求，并包括一堆新的离线视频处理工具。但更重要的是，我们现在可以估计手和身体的姿势——这无疑为增强现实和计算机视觉的新可能性打开了大门。</p><p id="a921" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在本文中，我们将重点关注手部姿态估计，以构建一个iOS应用程序，让您执行无触摸的手指手势。</p><p id="fd31" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">如果你一直在关注我的文章，我已经演示了如何使用ML Kit的人脸检测API构建一个无触摸滑动iOS应用程序。我觉得prototype集成到Tinder、Bumble等约会应用程序中很酷。但同时，由于眨眼和转动，它可能会导致眼睛疲劳和头痛。</p><p id="6945" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">因此，我们将简单地通过使用手势来扩展这一用例，而不是向左或向右滑动——因为在2020年，懒惰和用手机练习社交距离是可以的。在我们深入研究之前，我们先来看看如何在iOS 14中创建一个视觉手姿势请求。</p><h1 id="01b3" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">视觉手部姿态估计</h1><p id="d85a" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">新的<code class="fe mq mr ms mt b">VNDetectHumanHandPoseRequest</code>是一个基于图像的视觉请求，检测人手姿势。在类型:<code class="fe mq mr ms mt b">VNHumanHandPoseObservation</code>的实例中，它返回每手牌上的21个标志点。我们可以设置在视觉处理过程中每一帧要检测的<code class="fe mq mr ms mt b">maximumHandCount</code>。</p><p id="b1d2" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">要获得每个手指的点数组，我们只需通过以下方式调用实例上的枚举:</p><pre class="kh ki kj kk gu mu mt mv mw aw mx bi"><span id="1642" class="my lu ir mt b gz mz na l nb nc">try observation.recognizedPoints(.thumb)<br/>try observation.recognizedPoints(.indexFinger)<br/>try observation.recognizedPoints(.middleFinger)<br/>try observation.recognizedPoints(.ringFinger)<br/>try observation.recognizedPoints(.littleFinger)</span></pre><p id="8be8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">还有一个手腕标志，位于手腕的中心，不属于上述任何一组。相反，它属于<code class="fe mq mr ms mt b">all</code>组，可以通过以下方式检索:</p><pre class="kh ki kj kk gu mu mt mv mw aw mx bi"><span id="4236" class="my lu ir mt b gz mz na l nb nc">let wristPoints = try observation.recognizedPoints(.all)</span></pre><p id="7d71" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">一旦我们得到了上面的点数组，我们可以用下面的方法提取各个点:</p><pre class="kh ki kj kk gu mu mt mv mw aw mx bi"><span id="bd2d" class="my lu ir mt b gz mz na l nb nc">guard </span><span id="6150" class="my lu ir mt b gz nd na l nb nc">let thumbTipPoint = thumbPoints[.thumbTip],<br/>let indexTipPoint = indexFingerPoints[.indexTip],<br/>let middleTipPoint = middleFingerPoints[.middleTip],<br/>let ringTipPoint = ringFingerPoints[.ringTip],<br/>let littleTipPoint = littleFingerPoints[.littleTip],<br/>let wristPoint = wristPoints[.wrist]</span><span id="52f0" class="my lu ir mt b gz nd na l nb nc">else {return}</span></pre><p id="f1f7" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe mq mr ms mt b">thumbIP</code>、<code class="fe mq mr ms mt b">thumbMP</code>、<code class="fe mq mr ms mt b">thumbCMC</code>是您可以从拇指的点组中检索的其他单个点(其他手指依此类推)。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj ne"><img src="../Images/bdc0b3e0eff50592bffab611594f8642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KVNpm_7WWk7OFQDkUknsBw.png"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">来源:苹果视频</p></figure><p id="1b0f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">每个单独的点对象包含在<code class="fe mq mr ms mt b">AVFoundation</code>坐标系中的位置以及它们的<code class="fe mq mr ms mt b">confidence</code>阈值。</p><p id="011a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">随后，我们可以找到点之间的距离或角度，以创建某些手势处理器。例如，在<a class="ae kw" href="https://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision" rel="noopener ugc nofollow" target="_blank">苹果的演示应用</a>中，他们通过计算拇指和食指尖端之间的距离创建了一个捏手势。</p><h1 id="3413" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">入门指南</h1><p id="0155" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">既然我们已经完成了视觉手势请求的基础知识，让我们深入到实现中。</p><p id="18ee" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">启动Xcode并创建一个新的UIKit应用程序。确保您已经选择了iOS 14作为部署目标，并在<code class="fe mq mr ms mt b">Info.plist</code>中设置了<code class="fe mq mr ms mt b">NSCameraUsageDescription</code>字符串。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj nf"><img src="../Images/9b2d1c2452c98b8426621c5632a29a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LQUO0b4dw4pUM2t8wymog.png"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">作者图片</p></figure><p id="8556" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">因为我们已经介绍了如何用动画创建Tinder风格的卡片，这里是那个类的<a class="ae kw" href="https://gist.github.com/anupamchugh/6a7f8941dc097d2e9c467cf791d94c91" rel="noopener ugc nofollow" target="_blank">最终代码。</a></p><p id="e55a" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">类似地，下面是存放Tinder卡的类的代码。</p><h1 id="e322" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">使用AVFoundation设置我们的摄像机</h1><p id="5ef1" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">接下来，让我们使用苹果的<code class="fe mq mr ms mt b">AVFoundation</code>框架创建我们自己的定制相机。</p><p id="fc59" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">以下是ViewController.swift文件的代码:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ng nh l"/></div></figure><p id="d4fb" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">上面的代码中发生了很多事情。我们来分解一下。</p><ul class=""><li id="4717" class="ni nj ir kz b la lb ld le lg nk lk nl lo nm ls nn no np nq bi translated"><code class="fe mq mr ms mt b">CameraView</code>是一个在屏幕上显示摄像机内容的自定义UIView类。我们很快就会谈到它。</li><li id="3638" class="ni nj ir kz b la nr ld ns lg nt lk nu lo nv ls nn no np nq bi translated"><code class="fe mq mr ms mt b">setupAVSession()</code>是我们设置前置摄像头并将其作为输入添加到<code class="fe mq mr ms mt b">AVCaptureSession</code>的地方。</li><li id="c3d6" class="ni nj ir kz b la nr ld ns lg nt lk nu lo nv ls nn no np nq bi translated">随后，我们调用了<code class="fe mq mr ms mt b">AVCaptureVideoDataOutput</code>上的<code class="fe mq mr ms mt b">setSampleBufferDelegate</code>。</li></ul><p id="a923" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe mq mr ms mt b">ViewController</code>类符合<code class="fe mq mr ms mt b">HandSwiperDelegate</code>协议:</p><pre class="kh ki kj kk gu mu mt mv mw aw mx bi"><span id="4148" class="my lu ir mt b gz mz na l nb nc">protocol HandSwiperDelegate {<br/>  func thumbsDown()<br/>  func thumbsUp()<br/>}</span></pre><p id="185b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">我们将在检测到手势时触发相应的方法。现在，让我们看看如何在捕获的帧上运行视觉请求。</p><h1 id="e00a" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">在捕获的帧上运行视觉手部姿态请求</h1><p id="4521" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">在下面的代码中，我们创建了上面的<code class="fe mq mr ms mt b">ViewController</code>的扩展，它符合<code class="fe mq mr ms mt b">AVCaptureVideoDataOutputSampleBufferDelegate</code>:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ng nh l"/></div></figure><p id="6178" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">值得注意的是，<code class="fe mq mr ms mt b">VNObservation</code>返回的点属于视觉坐标系。我们需要将它们转换成UIKit坐标，以便最终在屏幕上绘制它们。</p><p id="0c72" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">因此，我们通过以下方式将其转换到AVFoundation坐标系中:</p><pre class="kh ki kj kk gu mu mt mv mw aw mx bi"><span id="f416" class="my lu ir mt b gz mz na l nb nc">wrist = CGPoint(x: wristPoint.location.x, y: 1 - wristPoint.location.y)</span></pre><p id="433f" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">随后，我们将在<code class="fe mq mr ms mt b">processPoints</code>函数中传递这些点。为了简单起见，我们只使用两个标志——拇指尖和手腕——来检测手势。</p><p id="16b3" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">下面是<code class="fe mq mr ms mt b">processPoints</code>函数的代码:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ng nh l"/></div></figure><p id="7559" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">以下代码行将<code class="fe mq mr ms mt b">AVFoundation</code>坐标系转换为UIKit坐标系:</p><pre class="kh ki kj kk gu mu mt mv mw aw mx bi"><span id="b96b" class="my lu ir mt b gz mz na l nb nc">previewLayer.layerPointConverted(fromCaptureDevicePoint: point!)</span></pre><p id="dab9" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">最后，基于两点之间的绝对阈值距离，我们在牌堆上触发相应的向左滑动或向右滑动动作。</p><p id="af9b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated"><code class="fe mq mr ms mt b">cameraView.showPoints(pointsConverted)</code>在<code class="fe mq mr ms mt b">CameraView</code>子层上的两点之间画一条线。</p><p id="2187" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">下面是<code class="fe mq mr ms mt b">CameraView</code>类的完整代码:</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ng nh l"/></div></figure><h1 id="5aeb" class="lt lu ir bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">最终输出</h1><p id="7308" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">下面给出了实际应用程序的输出:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj nw"><img src="../Images/169ac551ce4e6c5eb41d4824c67138fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/1*cyz-LOhw_Kvl16jtxyaMWQ.gif"/></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">作者截屏</p></figure></div><div class="ab cl nx ny hv nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ik il im in io"><h1 id="1fb2" class="lt lu ir bd lv lw oe ly lz ma of mc md jx og jy mf ka oh kb mh kd oi ke mj mk bi translated">结论</h1><p id="74a1" class="pw-post-body-paragraph kx ky ir kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ik bi translated">从基于手势的自拍点击到绘制签名，再到找到人们在视频中做出的不同手势，您可以通过许多方式来利用Vision的新手部姿势估计请求。</p><p id="d769" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">人们也可以将这种视觉请求与身体姿势请求链接起来，以构建复杂的手势。</p><p id="12a4" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">上述项目的完整源代码可以在<a class="ae kw" href="https://github.com/anupamchugh/iOS14-Resources/tree/master/iOS14VisionHandPoseSwipe" rel="noopener ugc nofollow" target="_blank"> GitHub资源库</a>中找到。</p><p id="dcb8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这一次到此为止。感谢阅读！</p></div></div>    
</body>
</html>