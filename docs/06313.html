<html>
<head>
<title>It Took Two Days and Seven Engineers to Move Data Between Two S3 Buckets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在两个S3桶之间移动数据需要两天时间和七名工程师</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/it-took-2-days-and-7-engineers-to-move-data-between-s3-buckets-d79c55b16d0?source=collection_archive---------1-----------------------#2020-09-21">https://betterprogramming.pub/it-took-2-days-and-7-engineers-to-move-data-between-s3-buckets-d79c55b16d0?source=collection_archive---------1-----------------------#2020-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2653" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在S3存储桶之间传输大数据的最佳选择</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d63cdca72383a3b57c614f33b3281964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rugi0yyZsOJvWvUd7RgAmw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由来自<a class="ae ky" href="https://www.pexels.com/photo/photo-of-person-typing-on-computer-keyboard-735911/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae ky" href="https://www.pexels.com/@soumil-kumar-4325?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">苏米尔·库马尔</a>拍摄</p></figure><p id="53f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个工程师团队试图将25TB的数据从一个S3存储桶快速转移到另一个[1]。他们的要求是移动大量的小日志文件(<em class="lv">在MB </em>的范围内)，最好是在接下来的两个小时内。事实证明，他们需要两个完整的工作日和一个由七名工程师组成的团队来完成这项任务。很容易称它们效率极低——从商业角度来看，这个问题似乎很简单。但事实是，在没有准备的情况下，执行如此大的数据传输不是在两个小时内可以轻松完成的事情。</p><p id="42c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从他们的错误中吸取教训，看看我们如何更快更有效地完成这项工作。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="2b13" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">哪里出了问题？</h1><p id="067b" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">从最初的Reddit帖子中，除了他们希望快速传输25 TB的数据(主要由小日志文件组成)之外，我们没有获得很多背景信息。我们不知道他们必须将这些数据传入和传出哪些AWS区域。关于他们的方法，我们所知道的是，他们简单地进行了研究并得出结论，所有选项都太耗时，因此他们决定通过使用AWS CLI、<strong class="lb iu"> </strong>运行并行上传来迁移数据，如下所示:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="0d69" class="nf me it nb b gy ng nh l ni nj">aws s3 cp s3://source-bucket-name/ \<br/>s3://destination-bucket-name/ --recursive \<br/>--exclude "*" --include "2020-10*" \<br/>--include "2020-09*" --include "2020-08*" \<br/>--include "2020-07*" --include "2020-06*"</span></pre><p id="a09f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方式，他们利用多线程将数据传输分成多个操作[2]。每个<code class="fe nk nl nm nb b">--include</code>块为以特定前缀开始的文件生成一个新的上传线程。<code class="fe nk nl nm nb b">--exclude "*"</code>块确保我们在开始仅<code class="fe nk nl nm nb b">include</code>具有特定前缀的文件之前排除所有文件。在上面显示的命令中，我们只包括以每个上传线程的特定年份和月份前缀开始的日志文件。</p><p id="0d74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该命令将由一名工程师执行和监控，而另一名工程师(<em class="lv">或同一名工程师，但在另一个终端会话</em>中)可以运行其他月份的传输:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="5faa" class="nf me it nb b gy ng nh l ni nj">aws s3 cp s3://source-bucket-name/ \<br/>s3://destination-bucket-name/ --recursive \<br/>--exclude "*" --include "2020-05*" \<br/>--include "2020-04*" --include "2020-03*" \<br/>--include "2020-02*" --include "2020-01*"</span></pre><p id="1dc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这是AWS [2]推荐的在S3桶之间移动大量数据的选项之一，但它让我想起了一个map-reduce <strong class="lb iu"> </strong>问题:如何统计图书馆的图书数量。在几个人(工人/流程)之间分配工作，在他们之间平均分配工作，这样每个人只清点特定书架上的书籍，每个工人将结果报告给协调人(主人)。</p><p id="c42b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法可行，但是会产生大量的开销(他们需要七名工程师和两天的时间来协调)。一定有更好的办法！</p><p id="beec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意:即使我们使用了</em> <code class="fe nk nl nm nb b"><em class="lv">aws s3 cp</em></code> <em class="lv">，我们也可以使用</em> <code class="fe nk nl nm nb b"><em class="lv">aws s3 mv</em></code> <em class="lv">来确保数据不仅被复制到目的地，还被从源桶中删除。</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="3659" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">设置从存储桶A到存储桶B的复制</h1><p id="4888" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">理想情况下，我们不想单独迁移单个文件。我们倾向于只配置将数据从存储桶A传输到存储桶b，有一个选项可以让我们做到这一点:复制。</p><p id="b37c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">复制允许我们将存储桶A配置为始终与存储桶B同步，并自动确保所有文件都被复制。</p><p id="9d28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想将数据从生产存储桶复制到某个开发存储桶，复制就特别有用。通过这种方式，我们可以确保我们的开发环境拥有生产数据的精确副本，从而实现可靠的开发设置。</p><p id="78c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> AWS允许我们使用</strong> [3]:</p><ul class=""><li id="734a" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">CRR(跨区域复制)</li><li id="e0f5" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">SRR(相同区域复制)</li></ul><p id="e710" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个选项允许我们在跨地区(CRR)或同一地区(SRR)的存储桶之间移动数据。</p><p id="587e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，只有当两个S3存储桶都启用了<em class="lv">版本控制</em>时，复制才有效。</p><p id="094f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了实现这一点，我们转到管理控制台，在我们的源bucket中，我们选择<em class="lv">管理→复制→添加规则</em>。然后，我们按照下面屏幕截图中所示的三个步骤，实现从bucket A到bucket B的版本控制和复制:</p><div class="kj kk kl km gt ab cb"><figure class="ob kn oc od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/2f861cd1c1f532f2cef9199c04e2aaec.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*7AKt3mwuIMf_jDGFim4OhA.png"/></div></figure><figure class="ob kn oh od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/85daa2639120f67879723ff80820b2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Mhrc9MttUWjVoHImHxtwQw.png"/></div></figure><figure class="ob kn oi od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5d6e146a6e5a1ca1149eb0c5ec92f315.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6lKf8aC266Gw3gRMS9AYUw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk oj di ok ol translated">设置复制-按作者排列的图像</p></figure></div><p id="4ec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们应该会看到一个屏幕，确认复制已经建立:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/d6686e247c5131abba9331b78480d3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Py000hyfEPMD9FgrBZiKg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">复制成功-按作者排序的图像</p></figure><p id="d821" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完事了吗？不完全是。</p><p id="b22c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，复制听起来很棒，因为只要我们配置一次，就没有其他事情要做:AWS自动将对象从bucket A复制到bucket B。但是有一个警告:在我们设置好之后，复制只对我们将来上传的文件有效，它不会复制现有的对象！</p><p id="526c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不过，有一个技巧:改变S3存储桶的存储类别(或者，改变加密状态)就足够了。这可能涉及将存储类别从<code class="fe nk nl nm nb b">Standard</code>更改为<code class="fe nk nl nm nb b">Intelligent</code> <em class="lv"> </em>分层，但要点是:必须从一个类别更改为另一个类别。试图从<code class="fe nk nl nm nb b">Standard</code>更改为<code class="fe nk nl nm nb b">Standard</code>不会修改对象。通过更改存储类别，我们可以确保所有文件都:</p><ul class=""><li id="e196" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">从存储桶A移回到存储桶A(但使用新的存储类别)。</li><li id="6521" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">自动复制到存储桶b。</li></ul><p id="c129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过下面的命令[2]来实现这一点:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="dd67" class="nf me it nb b gy ng nh l ni nj">aws s3 cp s3://source-bucket-name s3://source-bucket-name --recursive --storage-class <!-- -->INTELLIGENT_TIERING</span></pre><p id="dfde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以从控制台执行同样的操作:</p><div class="kj kk kl km gt ab cb"><figure class="ob kn on od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/70d74552841d446f343e702bfb20be33.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*BuM7lxeDtCIKEi1GDeJ3-w.png"/></div></figure><figure class="ob kn oo od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7d485ac75c3f5a71927c6e0a9f50f638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*1WVuPe8ba2WeiNX7MmgK8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk op di oq ol translated">更改存储类别—按作者分类的图像</p></figure></div><p id="ea62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成这些更改后，所有数据会自动从一个存储桶复制到另一个存储桶:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/170e5de6eba0f6b0c3cb99333b59e750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzth8xAHCYULA-6hiannPA.png"/></div></div></figure><p id="6df2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以变回之前的存储类。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="5a67" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">S3批处理操作</h1><p id="3aa4" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们引入的第一种方法(AWS CLI)的缺点是，我们需要做大量的工作(我们自己的“map-reduce”)并进行许多API调用，这会产生较大的成本。第二种方法(复制)的缺点是这个过程是异步的，这意味着所有对象最终都会被复制。根据AWS:</p><blockquote class="os ot ou"><p id="9dde" class="kz la lv lb b lc ld ju le lf lg jx lh ov lj lk ll ow ln lo lp ox lr ls lt lu im bi translated">“大多数对象在15分钟内完成复制，但是<a class="ae ky" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-troubleshoot.html" rel="noopener ugc nofollow" target="_blank">有时复制可能需要几个小时或更长时间</a>。”[4]</p></blockquote><p id="8889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">潜在的延迟可能是这些工程师没有选择这个选项的原因，他们希望在两个小时内完成数据传输。</p><p id="8c3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，服务“<em class="lv"> S3批处理操作”</em>似乎是一个有吸引力的替代方案。它承诺在单个API请求中快速处理大量S3对象[2]。</p><h2 id="74b3" class="nf me it bd mf oy oz dn mj pa pb dp mn li pc pd mp lm pe pf mr lq pg ph mt pi bi translated">S3批量操作:过程</h2><p id="8766" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">将数据从存储桶A移动到存储桶B的整个过程需要以下步骤:</p><ul class=""><li id="2100" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">设置库存报告(<em class="lv">它可以存储在同一个桶中，我们希望将数据复制到</em> → <em class="lv">桶B </em>)以生成需要从桶A复制到桶B的所有对象的列表。</li></ul><ol class=""><li id="5a45" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu pj nt nu nv bi translated">为S3批处理操作创建IAM角色，为作业提供从两个存储桶(<em class="lv">或三个存储桶，如果您配置了将库存报告存储到第三个存储桶</em>)读取和写入数据的权限。</li><li id="1431" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu pj nt nu nv bi translated">在AWS管理控制台(<em class="lv">或AWS CLI </em>)中，创建一个带有<code class="fe nk nl nm nb b">PUT</code>复制操作的S3批处理作业，根据库存作业的输出进行实际的数据传输。</li><li id="6892" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu pj nt nu nv bi translated">运行作业并查看完成报告，以验证所有对象都已成功传输。</li></ol><p id="9ca7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个过程需要几个小时。</p><p id="691f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请注意，在S3批处理操作中创建作业的一个重要先决条件是准备好库存报告(步骤1)。</em></p><h2 id="6503" class="nf me it bd mf oy oz dn mj pa pb dp mn li pc pd mp lm pe pf mr lq pg ph mt pi bi translated">S3批量操作在行动:实施</h2><p id="9ef9" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们首先为我们的存储桶A创建一个S3库存报告(<em class="lv">选择您的存储桶→管理→库存</em>)，它将(<em class="lv">当完成</em>时)列出我们的S3存储桶中的所有对象:</p><div class="kj kk kl km gt ab cb"><figure class="ob kn pk od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/566a5fc74d7f3cef30460c3525b56b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*WeSTgeFSnAkhmcMlba7VDQ.png"/></div></figure><figure class="ob kn pl od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/2f6a286944dc3cd9a3904e4e71411716.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*4PSyaYqhhcb5QchreVybyQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk pm di pn ol translated">配置库存报告—按作者分类的图像</p></figure></div><p id="b917" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以自定义报告:选择目标存储区，选择是每天还是每周创建此报告，添加可选字段以包括额外的元数据，如对象大小、上次修改日期或对象是否加密。我们应该选择CSV，因为这是唯一可用于S3批处理操作的格式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/51795f9a82dd497f67452068f47148e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MW3PU0moS4iQjrXi1jbWKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在S3配置库存报告-按作者分类的图像</p></figure><p id="d206" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单击“保存”后，配置就完成了。但是，AWS通知我们可能需要48小时才能提交第一份报告！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/ccaf1a89568516c57b8a3bd875e7b834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXQOFJzPK380WgJon7awDg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">库存报告应在48小时内交付—图片由作者提供</p></figure><p id="575e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我试图通过AWS CLI列出文件，将结果保存到一个CSV文件中，并上传到我的S3存储桶，来伪造它并自己生成库存报告:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="d95f" class="nf me it nb b gy ng nh l ni nj"># create the file manually<br/>aws s3 ls s3://ecommerce-marketplace --recursive &gt; manifest.csv</span><span id="b9bd" class="nf me it nb b gy pq nh l ni nj"># upload to S3<br/>aws s3 cp manifest.csv s3://e-commerce-marketplace/manifest.csv</span></pre><p id="d524" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是这个服务似乎要求库存报告采用特定的格式，这导致我的“S3批处理操作”尝试失败:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/2464d882030bce2d3e4f3bcca6cae2ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPSY8cvZa7lSPtkTu06kGg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">试图“伪造”库存报告—作者图片</p></figure><p id="f596" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，没有库存报告就意味着没有S3批处理作业！</p><p id="57b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们已经有了这个库存，我们可以继续如下创建一个S3批处理操作作业:</p><ul class=""><li id="6cdf" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">创建新工作:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/2752660d4779b4e0d78f2881ee0817d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CsSHvOOYU6Eubt0vPvnHgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建工作-按作者创建图像</p></figure><ul class=""><li id="ac17" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">指定CSV库存报告的路径和S3目的地(<em class="lv"> bucket B </em>)，然后选择我们要执行的操作类型(<em class="lv"> Copy </em>):</li></ul><div class="kj kk kl km gt ab cb"><figure class="ob kn pt od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/b9a27a2b138f9dceb925a079abf0f8f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*BRUlNCyTwjbpolKOX9t7ZA.png"/></div></figure><figure class="ob kn pu od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0bbee141d4377d546a6b68df41a8cbc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*r0MW-PiOMQcekXwOE8SxCA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk pv di pw ol translated">配置作业-按作者分类的图像</p></figure></div><p id="978b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一步是配置完成报告和IAM角色，以授予作业访问我们的S3资源的权限:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/4e52ec39dd6786618edad6523e9495cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WBydxxdIRoIzZqcdCdvmw.png"/></div></div></figure><div class="kj kk kl km gt ab cb"><figure class="ob kn py od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/a992a936a75ac5726313d3f17372d681.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*uIxY8IYOx1uVlU6EgUVHug.png"/></div></figure><figure class="ob kn pz od oe of og paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e50ef06629332c01d39d732706344afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*aNm3-NlrxhRe8m1Fy8gJ_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk qa di qb ol translated">完成S3批处理操作作业以在S3存储桶之间移动大数据所需的所有后续步骤—图片由作者提供</p></figure></div></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="6afc" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">替代选项</h1><p id="fd51" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">除了上述方法，AWS还提供了在S3存储桶之间传输大量数据的其他方法:</p><ul class=""><li id="c319" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">AWS SDK:这需要您编写一个定制的应用程序(例如，用Java)来完成这个简单的<code class="fe nk nl nm nb b">COPY</code>操作。</li><li id="f07a" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">在亚马逊EMR上运行Hadoop集群，执行<code class="fe nk nl nm nb b"><a class="ae ky" href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html" rel="noopener ugc nofollow" target="_blank">S3DistCp</a></code> <a class="ae ky" href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html" rel="noopener ugc nofollow" target="_blank">操作</a>，将大数据从S3复制到新的目的地。这涉及到运行并行复制命令，将数据从bucket A下载到Hadoop集群，并将文件并行写入bucket B。</li></ul><p id="f17b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你问我，我会说这些选项似乎是<strong class="lb iu">过度工程</strong>的一个例子，但是在某些情况下它是有意义的，特别是如果你有一些意想不到的需求，比如必须在几个小时内移动数兆字节的数据。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="c8fd" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论</h1><p id="9a8a" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在本文中，我们讨论了在S3存储桶之间移动大量数据的几种选择:AWS CLI复制命令、复制和S3批处理操作。我们可以得出这样的结论:如果不为此类迁移计划更多的时间(<em class="lv">例如，计划一个足够大的缓冲区以等待清单报告生成，或者等待复制来为我们处理这个过程</em>)，移动大量数据的过程会非常复杂。它需要定制的、通常过度设计的解决方案，例如:</p><ul class=""><li id="f29e" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">用AWS SDK编写定制应用程序。</li><li id="7e16" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">在Hadoop集群上执行S3DistCp。</li><li id="2277" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">甚至尝试通过将AWS CLI复制过程分成单独的会话并将工作分配给七名工程师来执行自己的map-reduce工作。</li></ul><p id="0c7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不应该要求单个工程师在两个小时内完成如此大的数据传输。此外，提前规划可以首先消除对这种大数据传输的需要。</p><p id="5069" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，我希望企业主和经理们能够认识到，工程中有许多事情不会在一夜之间发生(当然不会在两个小时内)。一切都需要计划、准备、收集和与涉众讨论需求、基础设施设置和大量测试。这是为业务问题提供高质量IT解决方案的唯一途径。</p><p id="7db3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我从这些工程师的经历中学到了很多，我很感激他们分享了他们的故事。我希望它对你也有用。感谢您的阅读！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="d030" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated"><strong class="ak">参考文献</strong></h1><p id="ac69" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">[1] Reddit帖子:<a class="ae ky" href="https://www.reddit.com/r/aws/comments/irkshm/moving_25tb_data_from_one_s3_bucket_to_another/" rel="noopener ugc nofollow" target="_blank">https://www . Reddit . com/r/AWS/comments/irkshm/moving _ 25tb _ data _ from _ one _ S3 _ bucket _ to _ another/</a></p><p id="f319" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] AWS知识中心:<a class="ae ky" href="https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-transfer-between-buckets/" rel="noopener ugc nofollow" target="_blank">https://AWS . Amazon . com/premium support/Knowledge-Center/S3-large-transfer-between-buckets/</a></p><p id="1ff9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]亚马逊S3 —复制:<a class="ae ky" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/Amazon S3/latest/dev/Replication . html</a></p><p id="80f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] S3 CRR复制时间:<a class="ae ky" href="https://aws.amazon.com/premiumsupport/knowledge-center/s3-crr-replication-time/" rel="noopener ugc nofollow" target="_blank">https://AWS . Amazon . com/premium support/knowledge-center/S3-crr-Replication-time/</a></p></div></div>    
</body>
</html>