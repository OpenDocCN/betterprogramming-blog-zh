<html>
<head>
<title>iOS 14 Vision Body Pose Detection: Count Squat Reps in a SwiftUI Workout App</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">iOS 14 Vision身体姿势检测:在SwiftUI锻炼应用程序中计算深蹲次数</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/ios-14-vision-body-pose-detection-count-squat-reps-in-a-workout-c88991f7cad4?source=collection_archive---------2-----------------------#2021-06-16">https://betterprogramming.pub/ios-14-vision-body-pose-detection-count-squat-reps-in-a-workout-c88991f7cad4?source=collection_archive---------2-----------------------#2021-06-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d38c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用苹果的视觉框架建立一个人工智能健身SwiftUI应用程序，以检测人体姿势</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/af098eb326565511b883bf89b351e30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FFOj-ByO6M4tJo-A"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Sergio Pedemonte 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><p id="8361" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将探索一个移动ML用例，并围绕苹果的<code class="fe ls lt lu lv b">VNDetectHumanBodyPoseRequest</code>构建一个实际应用。这种预训练模型在WWDC20期间推出，是实时检测身体点的强大工具。</p><p id="425f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在疫情期间，家庭健身行业一直在蓬勃发展，因此我们的应用程序将成为一种家庭锻炼镜子，可以计算深蹲重复次数，并提供关于我们姿势的有用线索。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/b162ec41bbcfe8de562173f39761249c.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/1*g6eEb4sIYHDjbYhvVakbiQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者创作的GIF</p></figure><p id="9f9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们深入研究代码之前，让我们花一些时间思考一下我们在这个任务中使用的模型。<code class="fe ls lt lu lv b">VNDetectHumanBodyPoseRequest</code>返回给定图片中19个不同位置的2D坐标。我们将在SwiftUI应用中实现这一请求，然后构建一个基于逻辑的方法来寻找我们身体位置的变化。您现在可能想知道为什么我们不使用Create ML Action分类器。我们可以捕捉一些蹲视频，并创建ML将训练一个定制的模型，以身体位置为特征。这将有助于判断我们是否在蹲着。问题是它需要固定长度的视频输入。例如，如果我们将捕获长度设置为3秒，我们将获得该时间窗口的预测，但不计算蹲的次数。所以我们宁愿在<code class="fe ls lt lu lv b">VNDetectHumanBodyPoseRequest</code>之上建立我们自己的小逻辑。</p><p id="bea1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">继续创建一个新的SwiftUI项目。第一步将是访问我们iPhone的前置摄像头。由于SwiftUI不支持直接摄像头访问，我们将使用带有<code class="fe ls lt lu lv b">AVCaptureSession</code>的UIKit视图控制器。我不会为这一步提供太多的细节，因为已经有无穷无尽的教程介绍如何在SwiftUI中捕捉相机输出。在我们的应用程序中，我们还想显示来自我们相机的实时预览，所以我们让<code class="fe ls lt lu lv b">UIViewController</code>管理一个带有类型为<code class="fe ls lt lu lv b">AVCaptureVideoPreviewLayer</code>的根层的<code class="fe ls lt lu lv b">UIView</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="d758" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在视图控制器的<code class="fe ls lt lu lv b">viewDidAppear</code>调用中，我们初始化<code class="fe ls lt lu lv b">AVCaptureSession</code>并将前置摄像头设置为输入。由于我们的模型处理单个图像，我们需要从视频输出中获取样本帧。这是通过<code class="fe ls lt lu lv b">SampleBufferDelegate</code>完成的，这也是我们到SwiftUI的交接点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="26a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将<code class="fe ls lt lu lv b">CameraViewController</code>包装在一个<code class="fe ls lt lu lv b">UIViewControllerRepresentable</code>中，并分配一个类型为<code class="fe ls lt lu lv b">PoseEstimator</code>的变量作为我们接收样本帧的委托。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="18e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们来看看<code class="fe ls lt lu lv b">PoseEstimator</code>的细节。它实现<code class="fe ls lt lu lv b">captureOutput</code>以符合<code class="fe ls lt lu lv b">AVCaptureVideoDataOutputSampleBufferDelegate</code>。代码非常简单。由于我们将接收一系列图像，我们需要创建一个<code class="fe ls lt lu lv b">VNSequenceRequestHandler</code>并让它对每张图像执行一个<code class="fe ls lt lu lv b">VNDetectHumanBodyPoseRequest</code>操作。</p><p id="c6d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果由<code class="fe ls lt lu lv b">detectedBodyPose</code>函数接收，该函数获取第一个观察值并将其分配给<code class="fe ls lt lu lv b">bodyParts</code>变量。该字典属于<code class="fe ls lt lu lv b">[VNHumanBodyPoseObservation.JointName : VNRecognizedPoint]</code>类型，包含19个预定义身体点的置信分数和位置。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="183e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很好。只需几行代码，我们现在就可以使用前置摄像头图像进行实时推断，并拥有一个发布器，可以不断发送识别出的身体点的新预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/25094ebf4b7689fc32d5b77fd440a36f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LZhQ1yeACJqy1UtgMFJ4dw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://developer.apple.com/documentation/vision/vnrecognizedpointkey/body_landmarks" rel="noopener ugc nofollow" target="_blank">苹果开发者文档</a></p></figure><p id="8d79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">应用程序的ML部分实现后，我们现在将使用已识别的身体部位来构建两个功能:</p><ol class=""><li id="83ff" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">从身体点画一个简笔画，并把它放在相机视图上。</li><li id="5f32" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">数蹲起次数，检查身体姿势。</li></ol><p id="6cd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于简笔画，我们首先需要一种在身体点之间画线的方法。这可以通过SwiftUI形状来实现。请记住，到目前为止，我们使用的是从Vision返回的归一化点。当绘制形状时，我们需要缩放和平移这些点以匹配相机视图。size变量是摄像机视图的帧大小，将从顶层视图传递下来。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="dd80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一个新的视图中，我们为所有的身体部位绘制木棒。最初，<code class="fe ls lt lu lv b">bodyParts</code>变量是一个空字典，所以我们需要检查它是否已经填充了推理结果。此时，我们还可以检查置信度得分，以避免绘制不准确的线条或定制线条的大小/颜色。但是我们会保持简单，用绿色画出每一条线。下面是右腿的一个例子:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="d45d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们将<code class="fe ls lt lu lv b">CameraViewWrapper</code>和<code class="fe ls lt lu lv b">StickFigure</code>都添加到<code class="fe ls lt lu lv b">ContentView</code>中。我们给<code class="fe ls lt lu lv b">ZStack</code>视频输出的帧比率(1920x1080)以保持正确的纵横比。在运行应用程序之前，我们需要将<code class="fe ls lt lu lv b">Privacy Camera Usage Description</code>添加到我们应用程序的<code class="fe ls lt lu lv b">.plist</code>中。然后把手机放在地上的某个地方，你会在你的全身自拍上面看到一个绿色的简笔画。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/74900287489e5bee13d46a6e584bb117.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/1*Y70RhktL_RJmEeJPcC6XqQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者创作的GIF</p></figure><p id="03d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是时候添加计算重复次数和检查我们姿势的逻辑了。为了计算深蹲的次数，我们需要知道我们是在低位还是高位。通过查看上面的片段，我们可以看到，在下蹲运动过程中，以下值会发生变化:</p><ul class=""><li id="3ba4" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mp mg mh mi bi translated">上身相对于小腿的高度</li><li id="8f1e" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">髋关节角度</li><li id="2921" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mp mg mh mi bi translated">膝关节角度</li></ul><p id="858f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，我们都可以比较不同<code class="fe ls lt lu lv b">CGPoints</code>的高度，并使用三角学计算角度。在我们的例子中，我们比较了臀部的高度和膝盖的高度。如果臀部在膝盖下面，我们假设在下蹲位置。对于上下蹲姿势，我们不能比较身高，所以我们用膝盖角度来代替。假设膝盖角度超过150度，说明腿是伸展的，暗示处于上位。将以下函数添加到<code class="fe ls lt lu lv b">PoseEstimator</code>类中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="e4b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让应用程序检查我们在下蹲过程中的姿势。我不是健身专家，但我曾经被告知，我应该确保我的膝盖比脚踝宽。这实际上很容易用给定身体部位的坐标来实现。我们可以比较这些点的<em class="mq"> x </em>坐标，就像我们比较臀部和膝盖的高度一样。但是手机可能会倾斜，所以我们将使用第三种技术，同时考虑<em class="mq"> x </em>和<em class="mq"> y </em>坐标。在对<code class="fe ls lt lu lv b">CGPoint</code>的扩展中，我们定义了一个使用勾股定理计算两点间距离的函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="3b4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们添加一个新变量<code class="fe ls lt lu lv b">isGoodPosture</code>，比较膝关节和踝关节之间的距离。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="0eae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，每当我们获得对<code class="fe ls lt lu lv b">bodyParts</code>的新预测时，都需要调用<code class="fe ls lt lu lv b">countSquats</code>函数。我们为此使用Combine，因为它允许我们删除publisher的初始值，并避免在位置的强制展开期间出现问题。将以下内容添加到<code class="fe ls lt lu lv b">PoseEstimator</code>类中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="aeeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们快完成了。最后一步是在摄像机视图下添加一个<code class="fe ls lt lu lv b">HStack</code>。每当姿势不好的时候就会弹出一个红色的警戒三角。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="edeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">继续运行应用程序。我们现在已经实现了我们的小AI健身教练。这绝对不是一个生产就绪的应用程序，但我希望你了解如何在SwiftUI中分析身体运动。3D姿态估计模型将提供更大的分析潜力，但是可能不会实时运行。</p><p id="6cc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我知道你还能想出哪些用例。感谢阅读。</p><p id="1098" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码可从以下网址获得:</p><div class="mr ms gp gr mt mu"><a href="https://github.com/philippgehrke/SquatCounter/tree/main" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">菲利普·盖尔克/蹲起计数器</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">在GitHub上创建一个帐户，为philippgehrke/SquatCounter的开发做出贡献。</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni kp mu"/></div></div></a></div></div></div>    
</body>
</html>