<html>
<head>
<title>How to Integrate Great Expectations with Databricks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将《远大前程》与Databricks整合</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-integrate-great-expectations-with-databricks-e17740e2a97a?source=collection_archive---------3-----------------------#2022-07-07">https://betterprogramming.pub/how-to-integrate-great-expectations-with-databricks-e17740e2a97a?source=collection_archive---------3-----------------------#2022-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="986b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">获得更好的数据质量指标，实现一个巨大的改变</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/6e267f109f46f495526a67a3ea3c2d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*byfB7vk8ROM9CS4k75Hryg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">远大前程和数据砖块是绝配！形象成就了作者。</p></figure><p id="42c1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数据工程团队面临的一个常见挑战是如何最好地衡量数据质量。糟糕的数据质量会导致错误的见解和潜在的糟糕商业决策。集成的数据质量框架减少了团队在评估数据质量问题时的工作量。</p><p id="e7d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://greatexpectations.io/" rel="noopener ugc nofollow" target="_blank">远大前程</a> (GE)是一个很棒的数据质量python库。它集成了Apache Spark和几十个预配置的数据预期。<a class="ae lq" href="https://databricks.com/" rel="noopener ugc nofollow" target="_blank"> Databricks </a>是基于Spark构建的顶级数据平台。因此，您会期望它们无缝集成，但事实并非如此。</p><p id="7e81" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，在本文中，我将介绍一个简单的变化，您可以对一个ge类进行修改，从而在GE和Databricks之间实现更加集成的解决方案。</p><p id="54b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文所有代码均可在回购<strong class="kw iu"> </strong> <a class="ae lq" href="https://github.com/sjrusso8/databricks-great-expectations-extended" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">这里</strong> </a>。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="8318" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated"><strong class="ak">问题</strong></h2><p id="7321" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">我希望有一种简单的方法来集成GE和Databricks，而不需要在PySpark和配置文件之间切换。我发现在托管环境中使用GE很有挑战性。通用电气确实提供了“如何在数据块中使用远大期望”的分步指南如果您一步一步地遵循指南，您最终会得到堆积如山的配置设置。</p><p id="67e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">具体来说，我想要一个数据质量框架，它能很好地与data bricks<a class="ae lq" href="https://databricks.com/glossary/medallion-architecture" rel="noopener ugc nofollow" target="_blank">Medallion Architecture</a>相适应，并触及以下领域:</p><ul class=""><li id="ae9e" class="mx my it kw b kx ky la lb ld mz lh na ll nb lp nc nd ne nf bi translated">最小的开销，并且它“只适用于”数据块</li><li id="6ed4" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp nc nd ne nf bi translated">能够根据其他PySpark代码编写数据质量测试</li><li id="f081" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp nc nd ne nf bi translated">如果基础数据以意外方式更改，则引发错误</li><li id="7ce9" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp nc nd ne nf bi translated">将结果作为文件保存到存储位置</li></ul><p id="1316" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最终状态将是类似于以下的架构模式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nl"><img src="../Images/1402fc467f70f917ea8b8b0eba73f114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yTeWvCnz5ONFkvKWOC1TvA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">灵感来自数据砖<a class="ae lq" href="https://databricks.com/glossary/medallion-architecture" rel="noopener ugc nofollow" target="_blank">圆形建筑</a>的示意图</p></figure><p id="fe0d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">随着数据通过每一级，数据质量逐渐提高。在这个过程中，每次数据被验证时，结果都被保存为JSON，坏数据在加载到下一级之前被隔离。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="aa00" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">解决方案:扩展<em class="nq"> SparkDFDataset </em>类</h2><p id="bc1d" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">GE的一个基本数据集类是<strong class="kw iu"> SparkDFDataset </strong>。SparkDFDataset继承了PySpark DataFrame，并将所有期望都实现为方法。</p><blockquote class="nr ns nt"><p id="e561" class="ku kv mw kw b kx ky ju kz la lb jx lc nu le lf lg nv li lj lk nw lm ln lo lp im bi translated">通过扩展<strong class="kw iu"> SparkDFDataset </strong>类，您可以添加新方法来增强与数据块的集成。</p></blockquote><p id="c28e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的代码演示了我如何添加一些方法来集成GE和Databricks。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="2f47" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">方法<code class="fe nz oa ob oc b">get_notebook_metdata()</code>使用Databricks <code class="fe nz oa ob oc b">dbutils</code>对象收集关于笔记本的所有元数据，并返回一个python字典。</p><p id="622f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最重要的添加是<code class="fe nz oa ob oc b">.validate_and_save()</code>方法，它完成了以下工作:</p><ul class=""><li id="69eb" class="mx my it kw b kx ky la lb ld mz lh na ll nb lp nc nd ne nf bi translated">将Databricks元数据作为阿格引文添加进来</li><li id="93dc" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp nc nd ne nf bi translated">运行<code class="fe nz oa ob oc b">.validate()</code>生成“验证对象”</li><li id="6015" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp nc nd ne nf bi translated">将“验证对象”作为JSON保存到存储位置</li><li id="de91" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp nc nd ne nf bi translated">断言没有预期失败，并在笔记本上打印出一条消息</li></ul><p id="2b92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些是对主类的小调整，但是它们有助于使GE和Databricks的使用更加顺畅。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="2773" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated"><strong class="ak">使用<em class="nq"> ExtendedSparkDFDataset </em>类</strong></h2><p id="c05f" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">为了与通用电气教程保持一致，下面的例子使用了一个相似的纽约出租车数据集。该数据集被预加载到Databricks工作空间中的位置'<em class="mw"> /databricks-datasets/ </em>'下。下面提供的代码将使用<strong class="kw iu"> ExtendedSparkDFDataset </strong>作为“验证”步骤的一部分。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3a7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当在笔记本中运行<code class="fe nz oa ob oc b">validate_and_save()</code>方法时，会提供一条打印消息，显示所评估的期望值数量以及JSON文件在存储器中的写入位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi od"><img src="../Images/fbae8abf095e30e984c17127f071c9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uzsr2VmjeO9lUZfFF2_BNA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">笔记本中validate_and_save()方法的示例</p></figure><p id="f869" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为所有的验证结果都是JSON，所以可以使用Databricks查询结果。以下是查询JSON文件的输出结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oe"><img src="../Images/503ae36ae3216d9b35bb7ddc239d26d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7VRv2zwwUcwknsEZoaoMw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">cluster_id是运行笔记本或作业的Spark集群id。</p></figure><p id="a2d6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相反，如果期望在<code class="fe nz oa ob oc b">validate_and_save(),</code>期间没有通过，则输出消息将类似于带有<code class="fe nz oa ob oc b">AssertionError</code>的以下内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c9d592ca147b21c5b1e859d9ac8fbc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*sELOKhZ3GbLc5qrn7c4OIw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">有错误的笔记本中的validate_and_save()方法示例</p></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="9fdf" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">插图的编号</h2><p id="608a" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">尽管这种方法消除了一些启动和运行GE的配置开销，但是您也失去了一些特性。以下是扩展<code class="fe nz oa ob oc b">SparkDFDataset</code>的一些利弊:</p><h2 id="b413" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated"><strong class="ak">优点</strong></h2><ol class=""><li id="cc8c" class="mx my it kw b kx mr la ms ld og lh oh ll oi lp oj nd ne nf bi translated">更简洁的管道代码，配置设置更少</li><li id="2d49" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp oj nd ne nf bi translated">预期与PySpark一致</li><li id="3a5b" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp oj nd ne nf bi translated">断言预期的成功将抛出一个错误并停止处理</li><li id="a175" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp oj nd ne nf bi translated">结果保存为JSON，可以用本地数据块函数进行查询</li></ol><h2 id="d21a" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">骗局</h2><ol class=""><li id="a362" class="mx my it kw b kx mr la ms ld og lh oh ll oi lp oj nd ne nf bi translated">失去轻松呈现GE数据文档的能力</li><li id="1278" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp oj nd ne nf bi translated">不能使用GE文档中描述的“期望套件”或“检查点”</li><li id="a601" class="mx my it kw b kx ng la nh ld ni lh nj ll nk lp oj nd ne nf bi translated">您必须在不使用profiler类的情况下编写每个期望，以快速生成一个基本的期望套件</li></ol><p id="379f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，Databricks有一个新的<a class="ae lq" href="https://databricks.com/product/delta-live-tables" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> Delta Live Tables </strong> </a>特性，带有内置的质量控制。问题是，您必须专门为Delta Live表开发管道。我计划将来一定要深入DLT！</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="0dda" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">把它包起来！</h2><p id="a77d" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">在本文中，我展示了如何扩展一个基本的GE数据集，以实现与Databricks的额外集成。我在我的数据工程中使用了一个类似的定制类，这样我的团队可以随着时间的推移准确地跟踪数据质量。我相信这种方法比设置GE在托管环境中工作更直接有效。</p><p id="35ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样，这篇文章的所有代码都可以在回购<a class="ae lq" href="https://github.com/sjrusso8/databricks-great-expectations-extended" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="40ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">查看我的另一篇关于如何隔离坏数据的文章，现在您已经识别了它:</p><div class="ok ol gp gr om on"><a href="https://medium.com/@sjrusso/3-methods-for-dealing-with-bad-data-quality-b2e8c72fafc3" rel="noopener follow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd iu gy z fp os fr fs ot fu fw is bi translated">处理不良数据质量的3种方法</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">设计为在坏数据毒害您的平台之前处理它！</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">medium.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ko on"/></div></div></a></div></div></div>    
</body>
</html>