<html>
<head>
<title>How To Use Scrapy To Build a Dataset for Your Data Science Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Scrapy为您的数据科学项目构建数据集</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-use-scrapy-to-build-a-dataset-for-your-data-science-project-8f04af3548c6?source=collection_archive---------6-----------------------#2021-09-06">https://betterprogramming.pub/how-to-use-scrapy-to-build-a-dataset-for-your-data-science-project-8f04af3548c6?source=collection_archive---------6-----------------------#2021-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ea03" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Scrapy蜘蛛从Python网站中提取数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/be00eb03e9abf158bad07edb3039f817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tmgu8WfvEY-iZN0q"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">若昂·路易斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1f69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学项目的第一步是数据收集。现实世界的数据可以在无数网站上获得，但有些网站没有API，即使有API，免费版本也会限制请求的数量，除非你升级。这就是网络抓取派上用场的时候了。网络抓取是使用机器人从网站提取公共数据的过程。</p><p id="7204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了抓取网站和建立我们自己的数据集，我们将使用Scrapy，这是Python中最强大和最快的web抓取框架。这也是Scrapy优于美汤、硒等其他刮痧工具的原因。</p><p id="bb20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本指南中，我们将构建一个人口数据集，这将为我们提供一个如何使用Scrapy创建数据集的好主意。本指南将涵盖以下主题:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2d2a" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu">Table of Contents<br/></strong>1. <a class="ae ky" href="#1615" rel="noopener ugc nofollow">Setup</a><br/> - <a class="ae ky" href="#ad9b" rel="noopener ugc nofollow">Installing Anaconda</a><br/> - <a class="ae ky" href="#0618" rel="noopener ugc nofollow">Installing Scrapy</a><br/> - <a class="ae ky" href="#6f9d" rel="noopener ugc nofollow">Open conda environment within an IDE and add Python interpreter</a><br/>2. <a class="ae ky" href="#deef" rel="noopener ugc nofollow">Creating a Project and Spider</a><br/> - <a class="ae ky" href="#c932" rel="noopener ugc nofollow">The Template</a><br/>3. <a class="ae ky" href="#f42a" rel="noopener ugc nofollow">Building The Spider</a><br/> - <a class="ae ky" href="#c524" rel="noopener ugc nofollow">Finding elements with Scrapy</a><br/> - <a class="ae ky" href="#52da" rel="noopener ugc nofollow">Locating the rows</a><br/> - <a class="ae ky" href="#547e" rel="noopener ugc nofollow">Locating the country names and population</a><br/> - <a class="ae ky" href="#ff49" rel="noopener ugc nofollow">Returning Elements</a><br/> - <a class="ae ky" href="#a035" rel="noopener ugc nofollow">Run the spider and export data to CSV or JSON File</a></span></pre><h1 id="1615" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">设置</h1><h2 id="ad9b" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">安装Anaconda</h2><p id="60ea" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">根据Scrapy的<a class="ae ky" href="https://docs.scrapy.org/en/latest/intro/install.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>，建议通过Anaconda或者Miniconda安装Scrapy，所以去这个<a class="ae ky" href="https://www.anaconda.com/" rel="noopener ugc nofollow" target="_blank">链接</a>下载Anaconda然后安装。</p><p id="7c20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后打开Anaconda并创建一个新环境。为此，请按照下列步骤操作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/f6960f181ff76ed890b01c923d9d6e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1gcng9AmRXOSWHWE3pOgPw.gif"/></div></div></figure><p id="63e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，选择Python版本并点击绿色的“创建”按钮。然后应该安装虚拟环境。</p><h2 id="0618" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">安装刮刀</h2><p id="e3c2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">现在是时候安装Scrapy了。首先，打开一个终端(单击您环境中的play按钮，然后单击“打开终端”)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/ba51db219bb1c0f0a9684c0d6110f7e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*B_bB0_8vYE6x7KoGnt4bRQ.gif"/></div></div></figure><p id="bab5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">终端将激活虚拟环境“my_scrapy_venv”。从现在开始，每次我们运行一个命令，确保这个环境出现在括号中。</p><p id="14fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要安装Scrapy，请在终端上运行以下命令。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7152" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) conda install -c conda-forge scrapy</span></pre><h2 id="6f9d" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">在IDE中打开conda环境，并添加Python解释器</h2><p id="d654" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">如果您使用的是IDE或文本编辑器，您必须执行一些额外的步骤来设置我们创建的这个新的虚拟环境。在我的例子中，我使用Pycharm，所以我必须遵循下面的步骤(对于大多数文本编辑器来说应该是类似的)</p><p id="222c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，在终端上运行以下命令:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1514" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) which Python # mac<br/>(my_scrapy_venv) where Python # windows</span></pre><p id="3c6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出将是Python解释器所在的路径。我的如下:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9e3f" class="ma mb it lw b gy mc md l me mf">/opt/anaconda3/<strong class="lw iu">envs</strong>/my_scrapy_venv/bin/python</span></pre><p id="12e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">复制这个路径，然后打开Pycharm。然后点击“创建新项目”下面的“打开”按钮将弹出一个窗口，然后找到“envs”文件夹并打开它。这个文件夹包含我们在Anaconda中创建的虚拟环境。</p><p id="4583" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这之后，您应该会看到Pycharm中的“envs”文件夹。现在转到Windows上的设置(文件-&gt;设置)或Mac上的首选项(Pycharm -&gt;首选项)。然后转到“Project: envs”中的“Project Interpreter ”,按照以下步骤选择Python解释器:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1e31ef78b927f07bcb037ae8d0d80012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9-sOvT277Dh7Tlk1HLgwgw.gif"/></div></div></figure><p id="d03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，我们粘贴了之前获得的Python解释器的路径。然后我们点击“Ok”按钮来添加解释器。</p><p id="1f88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，确保你安装了<code class="fe no np nq lw b">protego</code>来开始使用Scrapy。为此，请在终端中运行命令:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b91b" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) <!-- -->conda install -c conda-forge protego</span></pre><h1 id="deef" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">创建项目和蜘蛛</h1><p id="5795" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">要开始使用Scrapy，我们需要创建一个项目。让我们创建一个名为<code class="fe no np nq lw b">spider_tutorial</code>的项目。为此，我们使用如下所示的<code class="fe no np nq lw b">startproject</code>命令:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a032" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) scrapy startproject <!-- -->spider_tutorial</span></pre><p id="d41c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们必须创造一只蜘蛛。不像美汤或硒，我们不是从Scrapy上的空白脚本开始，而是使用模板。在这种情况下，我们将使用<code class="fe no np nq lw b">scrapy.Spider</code>模板。</p><p id="e47f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们创建一个蜘蛛之前，我们必须将目录更改为项目的文件夹。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="af66" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) cd <!-- -->spider_tutorial</span></pre><p id="e0c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了创建一个蜘蛛，我们使用<code class="fe no np nq lw b">genspider</code>命令。我们还需要为蜘蛛添加一个独特的名称和我们希望抓取的网站的链接。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d70d" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) scrapy genspider worldometers www.worldometers.info/world-population/population-by-country</span></pre><p id="06a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意这个链接没有HTTP协议，因为Scrapy是自动添加的。此外，删除链接末尾的任何附加斜杠(/)。</p><p id="f763" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这之后，一个名为<code class="fe no np nq lw b">worldometers</code>的新蜘蛛应该被创建在<code class="fe no np nq lw b">spider_tutorial</code>文件夹内的蜘蛛文件夹中。打开脚本，你会看到用Scrapy刮一个网站的基本模板。</p><p id="85df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目录树应该如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/fd5d69bd96a70988ae95fc93dff8d0ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*5gP_1rCWPI2X_TSq8NhWjg.png"/></div></figure><p id="fd6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在一切都准备好了，开始刮刮。</p><h2 id="c932" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">模板</h2><p id="09b3" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><code class="fe no np nq lw b">worldometers</code>蜘蛛包含如下所示的模板:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="0505" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我对模板做了一些小的修改。首先，<code class="fe no np nq lw b">allowed_domains </code>变量应该只包含网站的根，而start_urls包含我们想要抓取的确切链接。此外，手动将“s”添加到HTTP协议中，以防“s”出现在网站的链接中。</p><p id="352c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在解析方法中，我们将使用响应对象来查找元素，然后提取我们想要的数据。让我们在下一节看看如何做到这一点。</p><h1 id="f42a" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">建造蜘蛛</h1><p id="bb9f" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在构建蜘蛛之前，先来看看我们要刮的<a class="ae ky" href="https://www.worldometers.info/world-population/population-by-country/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/295de712f812afe26c57def0f1600341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZBqjsqMfIkxPUtOFhQwVg.png"/></div></div></figure><p id="1955" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们要提取的两个字段是国家和人口列。然后，我们将这个数据集导出到一个CSV或JSON文件。</p><h2 id="c524" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">用Scrapy寻找元素</h2><p id="929a" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">Scrapy允许我们用XPath和CSS选择器找到元素。在本指南中，我们将使用XPath。为了构建XPath，我们需要标记名、属性名和属性值。所有这些都是HTML元素，所以如果你不熟悉HTML，可以看看这个<a class="ae ky" href="https://medium.com/geekculture/web-scraping-cheat-sheet-2021-python-for-web-scraping-cad1540ce21c#2773" rel="noopener"> HTML网页抓取指南</a>。</p><p id="4c29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看XPath语法:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a483994a7b30a82d0a8a4c04a12282f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/0*cIdY9IED2Am0Q3yC.png"/></div></figure><h2 id="52da" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">定位行</h2><p id="0bdd" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">当抓取表中的数据时，通常首先定位行。</p><p id="6557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要获取行后面的HTML元素，请转到页面并检查任何一行。您应该看到一个<code class="fe no np nq lw b">tr</code>标记(<code class="fe no np nq lw b">tr</code>代表表格行)。这是我们将要用来构建XPath的元素。</p><p id="6cd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种特殊情况下，标记名应该足以构建它的XPath。XPath将会是<code class="fe no np nq lw b">//tr</code>。要测试这个XPath，在页面上按CTRL+F，然后编写<code class="fe no np nq lw b">//tr</code>，应该选择表示第一行的HTML元素，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/c63449f4f58ec8e5b4389c6c79acc150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iUY9s3ZPFH59tlbuqrmaCg.gif"/></div></div></figure><p id="e481" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有多行，因此将使用此XPath选择多个元素。它们都将存储在一个列表中。为了获得国家名称和人口，我们必须遍历这个列表。</p><p id="8ea0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们构建的脚本应该类似于下面的代码片段:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="547e" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">查找国家名称和人口</h2><p id="eba3" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">为了定位国家名称和人口，我们需要检查表中的任何国家和人口值。为此，请按照以下步骤操作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/693a0f374a597d4e758a78801b74ace3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*CuXx0N_5ZxiYE52DBJzHzA.gif"/></div></div></figure><p id="6518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，为了获得国家名称和人口中的文本值，我们必须分别遵循序列<code class="fe no np nq lw b">tr, td, a</code>和<code class="fe no np nq lw b">tr, td[3]</code>(<code class="fe no np nq lw b">[3]</code>表示应该选择索引为3的<code class="fe no np nq lw b">td</code>节点)</p><p id="0071" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，我们将在for循环中使用<code class="fe no np nq lw b">row</code>变量作为上下文。也就是说，我们不需要再次编写<code class="fe no np nq lw b">//tr</code> XPath，而只需要使用特殊字符<code class="fe no np nq lw b">.</code>来表示应该使用当前上下文(行)。</p><p id="aa39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">for循环中的代码如下:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9643" class="ma mb it lw b gy mc md l me mf">for row in rows:<br/>    # Locating country names and population<br/>    countries = row.xpath(<strong class="lw iu">'./td/a/text()'</strong>).get()<br/>    population = row.xpath(<strong class="lw iu">'./td[3]/text()'</strong>).get()</span></pre><p id="783e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，我们使用文本属性<code class="fe no np nq lw b">text()</code>和<code class="fe no np nq lw b">.get()</code>来获取文本值。</p><p id="1608" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要验证XPath是否正常工作，请按照我们定位行的相同步骤:复制XPath，转到页面，检查它，按CTRL+F并粘贴XPath。</p><h2 id="ff49" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">返回元素</h2><p id="dff2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">最后一步是返回所有提取的数据。为此，我们使用yield关键字。这与return关键字的工作方式类似，但它不会破坏其局部变量的状态<strong class="lb iu">。</strong></p><p id="38db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们产生在<code class="fe no np nq lw b">for</code>循环中提取的数据。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c418" class="ma mb it lw b gy mc md l me mf">   # Return data extracted<br/>   yield {<br/>          'countries': countries,<br/>          'population': population,<br/>    </span></pre><h2 id="a035" class="ma mb it bd mh mx my dn ml mz na dp mp li nb nc mr lm nd ne mt lq nf ng mv nh bi translated">运行spider并将数据导出到CSV或JSON文件</h2><p id="6791" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在运行spider之前，打开终端并确保用anaconda创建的虚拟环境(在我的例子中是激活的<code class="fe no np nq lw b">my_scrapy_venv)</code>)。此外，检查您是否在项目的文件夹中，该文件夹包含一个名为<code class="fe no np nq lw b">scrapy.cfg</code>的文件(在本指南中，我们将该项目命名为<code class="fe no np nq lw b">spider_tutorial</code>)。</p><p id="4e30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要运行蜘蛛程序，我们使用如下所示的<code class="fe no np nq lw b">crawl</code>命令:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7b11" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) scrapy crawl worldometers</span></pre><p id="5b47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想要运行spider并将其导出到CSV或JSON文件，请运行以下命令:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1c02" class="ma mb it lw b gy mc md l me mf">(my_scrapy_venv) scrapy crawl worldometers -o name_of_file.csv<br/>(my_scrapy_venv) scrapy crawl worldometers -o name_of_file.json</span></pre><p id="d885" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！之后，转到Pycharm，在项目的文件夹中应该可以找到创建的CSV或JSON文件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/28ffcf4014a8563321dd800c0de76ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ppqem-qvBnYUCWP5XsWkTw.gif"/></div></div></figure><p id="c560" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">恭喜你！您成功构建了人口数据集。下面是我们构建的蜘蛛的完整代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="f6f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/geekculture/4-web-scraping-projects-that-will-help-automate-your-life-6c6d43aefeb5" rel="noopener">在这里</a>你可以找到四个网页抓取项目来练习这项新技能。</p><p id="dd4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="3e90" class="mg mb it bd mh mi og mk ml mm oh mo mp jz oi ka mr kc oj kd mt kf ok kg mv mw bi translated">想联系作者？</h1><p id="f53c" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><a class="ae ky" href="https://frankandrade.ck.page/bd063ff2d3" rel="noopener ugc nofollow" target="_blank">与3k以上的人一起加入我的电子邮件列表，获取我在所有教程中使用的Python for Data Science备忘单(免费PDF) </a></p></div></div>    
</body>
</html>