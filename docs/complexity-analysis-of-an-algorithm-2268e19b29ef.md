# 算法的复杂性分析

> 原文：<https://betterprogramming.pub/complexity-analysis-of-an-algorithm-2268e19b29ef>

## 关于如何理解您正在编写的算法的详细指南，加上一些大 O 符号

![](img/1dc70106d115350d85bb9488f92d8b68.png)

[H . Shaw](https://unsplash.com/@hikeshaw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/complex?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

今天，我们将讨论计算机科学中最重要但也是最令人恐惧的话题之一——算法复杂性，尤其是时间复杂性。因为时间就是金钱！

*注意:在本文中，我们交替使用算法和程序这两个词。*

# 什么是复杂性分析？

它是一个工具，允许我们解释当输入越来越大时，算法将如何运行。

现在，有人可能会问，为什么我们需要计算一个程序有多复杂？只要给它足够快的硬件和更多的资源，程序应该是罚款，对不对？

嗯，也许在理想世界里，每个程序员都希望自己生活在其中。在现实世界中，我们需要关注复杂性，因为硬件和资源都是要花钱的。

例如，如果一个程序花一秒钟为一个用户完成工作，想象一下如果有一百万个用户会发生什么。随着用户数量的增加(即输入数量的增加)，程序占用的资源越来越多(即复杂性的增加)。

# 我们不能使用 Profilers 吗？

Profilers 是可以测量程序运行速度的程序，以毫秒或秒为单位。因此，有人会认为我们可以只比较两个程序执行所需的秒数。目标实现。对吗？实际上，不完全是。

让我们举一个例子:一个程序需要一秒钟来为一个用户完成它的工作。对于不同的编程语言，这一秒钟可能会有所不同。你可能会有一个用低级语言编写的非常糟糕的程序，比如汇编，它可能需要和用高级语言编写的非常好的程序一样长的时间来执行，比如 Swift 或 Java。我们可以得出结论，时间不是衡量一个好的、有效的计划的方式。我们又回到起点了。我们如何计算算法复杂性？

# 算法复杂性

算法复杂性是设计用来在思想层面比较两种算法的，而忽略了底层细节，如运行的硬件、编程语言等。

让我们举一个 JavaScript 的例子:

```
var number = list[0];
```

你能数出这一行代码中有多少条指令吗？

答案是两个。为什么？：

*   第一个指令是从“列表”中查找 0ᵗʰ索引处的对象
*   第二条指令是将该对象分配到一个新的变量“数字”中。

所以，对于上面的一行程序，我们可以说 *f* ( *n* ) *= 2* 。无论如何，这个程序中的指令数将是 2。

*如果你不熟悉数学记数法 f(n)* [*这里阅读更多*](https://www.mathsisfun.com/sets/function.html) *。*

让我们举一个更复杂的例子:

```
var number = list[0];
for (var i=0; i<n; ++i) {
   if (list[i] >= number) {
      number = list[i];
   }
}
```

你能列出上面代码中的所有指令吗？我会等的。试试吧。

它们是:

*   从列表中查找 0ᵗʰ对象。
*   把上面的物体分配给数字。
*   创建值为 0 的变量“I”。
*   比较` i
*   Get an ith object from the list.
*   Compare it with number.
*   Again, get an ith object from the list.
*   Assign it to the number.
*   For the next loop iteration, compare `i
*   Increment `i` (at ++i).

The last six instructions are only performed if the loop executes at least once when the *n > 0* 。

在这种情况下，指令的数量不是一个确定的数字，它随输入 n 而变化。

*   如果 *n* =0，将执行前 4 + 2 (=6)条指令。因为循环将检查“i < n”并立即退出。继续，手动运行并验证它。
*   如果 *n* =1，将执行所有 4 + 6 (=10)条指令。其中 6 是循环执行的指令数。
*   如果 *n* =2，将执行所有 4 + 6*2 (=16)条指令。因为循环执行了两次。

现在，我们可以把上面的函数写成 4 + 6 *n* ，其中 *n* 是提供给程序的输入，因为循环执行了 n 次。

**最终答案:*f*(*n*)*= 4+6n*。**

上面的复杂性计算听起来可能足够好，但它不是计算复杂性的最佳方法。想象一下为一个非常非常大的程序计算指令。为了简化，我们可以利用渐近行为。

# 渐近行为

用简单的数学语言来说，渐近行为是一种限制行为的方法。没明白吗？别担心。只是跟着走。

## 举例说明

假设我们对函数 *f* ( *n* )的性质感兴趣，因为 *n* 变得非常大。如果 *f(n) = n* *+ 3n，*那么随着 *n* 变得非常大，项 *3n* 与 *n* 相比变得无关紧要。函数 *f(n)* 被称为“渐近等价于 *n* ”。在维基百科上阅读[更多这方面的内容。](https://en.wikipedia.org/wiki/Asymptotic_analysis)

所以，我们可以简单地把 *f(n)=4+6n* 写成 *f(n)=6n* 因为随着 *n* 变大，常数 4 变得无关紧要。

现在，我们正在从 *6n* 投放 *6* 。为什么？因为不同的编程语言对同一代码块执行不同数量的指令是有道理的。此外，我们只对最大的增长项(到无穷大)感兴趣，它是 *n* 。于是，我们的 *f(n)=6n* 就变成了刚好 *f(n)=n* 。(在数学中这是不允许的，但是在计算机科学中，由于上面给出的原因，这是允许的。)

这告诉我们的是放弃所有的装饰常数。这使得检测程序的渐近行为变得非常容易，我们不必计算指令，这是一种解脱。

## 实践

让我们看一些例子，找到渐近行为，

*   *f(n) = 5n + 12* 给出 *f(n) = n* 。因为我们必须同时放弃 12 和 5。
*   *f(n) = 2* 给出 *f(n) = 1* 。我们去掉了乘数 2 * 1，但是我们仍然需要在这里放一个 1 来表示这个函数有一个非零值。当一个程序有 *f(n)=1* 的复杂度时，它被认为是最好的。
*   *f(n) = n + 10000n + 56675* 给出 *f(n) = n* 。即使前面的因子 if *n* 很大，我们仍然可以找到一个值为 *n* ，这使得 *n* 大于等式中的所有其他项。

***经验法则:*** *对于一个不执行循环或递归的程序，复杂度将为 f(n)=1。如果我们回到上面的一行程序，f(n)将永远是 1，因为即使列表有一百万个对象，指令的数量也不会改变。*

*如果一个程序有多个顺序循环，其中最慢的一个决定了渐近行为。两个嵌套循环后跟一个单独的循环在渐近上与单独的嵌套循环相同，因为嵌套循环在简单循环中占主导地位。*

# “奇特的”θ(theta)符号

## 这是什么？

到目前为止，我们已经使用数学 *f(n)* 来表示程序的渐近行为。现在我们将转向计算机科学符号，即θ符号。

渐近地，我们会说我们的程序是θ*(f(n))，*也是我们的程序有 *f(n)* =1 变成θ(1)， *f(n)=n* 变成θ(*n*)等等。

## **θ(n)读作“n 的θ*”***

比如我们可以说 *f(n)* = *2n* 是一个函数是*θ(n)*。我们也可以把*2n∈θ(n)*写成“两个 *n* 是 *n.* 的θ”

等等，那是什么意思？不要迷惑。我们要说的是，如果我们已经计算了一个程序需要的指令的数量，这些指令是 *2n* ，那么我们算法的渐近行为由 *n* 描述，这是我们通过丢弃常数找到的。我们已经经历过了。这里没什么新鲜的。

如果我们应用上面的符号，下面是一些真正的数学陈述:

*   *n⁶+3n∈θ(n⁶)*
*   *2ⁿ+12∈θ(2ⁿ)*
*   *3ⁿ+2ⁿ∈θ(3ⁿ)*
*   *nⁿ+n∈θ(nⁿ)*

我们放在θ(*这里是*)里面的东西，叫做时间复杂度，或者只是我们程序的复杂度。所以，一个带有θ(*n*)的程序是复杂的 *n* 。带有θ(*n*)的程序是复杂的 *n* 。

我们对θ(1)、θ(n)、θ(*n*)和θ(log(*n*))也有专门的称呼，因为它们经常出现。它们如下:

*   θ(1)=恒定时间复杂度。
*   θ(*n*)=线性复杂度。
*   θ(*n*)=二次复杂度。
*   θ(log(*n*)=对数复杂度。

经验法则:θ较大的程序比θ较小的程序运行得慢。

# 大 O 符号

我打赌你已经听过很多次了。可能你在面试的时候被问到过这个问题。大 O 是最受欢迎的术语，因为它更糟糕。不，不，这不是更差，而是更差的时间复杂度符号。所以，它说的是一个程序，它的表现有多差。因此，所述程序将**永远不会**超过某个界限。简而言之，Big-O 符号只是一个上限！

## 这是什么？

我们如何找到这个上限？更重要的是，当我们已经如上所述计算出精确的复杂度时，为什么我们还需要找到一个上界呢？嗯，有一个很好的理由这么做。

有时候，计算一个程序的θ(*n*)复杂度真的很难，在这种情况下，我们必须让程序变得比原来更糟，以便简化复杂度计算。

## 举例说明

例如，在一个[选择排序](https://en.wikipedia.org/wiki/Selection_sort)程序中，外循环执行 *n 次*次，但是内循环执行 *n-1* 次，然后 *n-2* 次等等，直到外循环的最后一次迭代，在此期间内循环将只运行一次。

因此，我们必须计算出内循环的总和 *n + (n-1) + (n-2) + … + 2 + 1* 。外环复杂度计算很容易，它将是“ *n* ”。在这种情况下，为了使复杂性计算更容易，我们使程序变得更糟——我们使内部循环变得更糟。怎么会？不是让它运行 *n* 然后 *n-1* 等等，而是让它运行 *n* 次。

现在很容易将复杂度计算为θ(*n*)，因为有两个循环正好运行了 *n* 次。还有，由于我们的程序不比 *n* 差，可以说我们原来的算法是 O( *n* )。所以，我们的程序可以比 O( *n* )更好但不会更差。就说这么多。

## **O( *n* )读作 *n* 平方的大 oh】**

如果一个原始的(即未改变的)程序的复杂度是θ(*n*)，那么我们可以说它实际上是 O( *n* )，因为大 O 只是一个更差的程序复杂度。但是，如果一个原始程序的 Big-O 复杂度是 O( *n* )，那么这意味着该程序的更差复杂度是 O( *n* )，该程序可能已经被改变，以便导出 O( *n* )。所以这个程序可能是θ(*n*)或者更好，可能是θ(*n*)或者θ(1)。

综上，如果我们的程序是θ(*n*)，我们还是可以说是 O( *n* )。但是反过来并不总是正确的。概而言之，任何为θ(*a*)的程序都是 O( *b* )其中 *b* 比 *a* 差。

请注意，我们对程序的修改并不需要给我们一个实际上有意义或等同于我们原来程序的程序。对于给定的 n，它只需要执行比原始指令更多的指令。我们使用它只是为了计算指令，而不是实际解决我们的问题。

## 实践

我们来看几个例子。找出以下哪些是正确的:

1.  一个θ(*n*)算法是 O( *n* )
2.  一个θ(*n*)算法是 O( *n* )
3.  一个θ(*n*)算法是 O( *n* )
4.  一个θ(*n*)算法是 O(1)
5.  一个 O(1)算法是θ(1)]
6.  一个 O( *n* )算法是θ(1)

## **解决方案**

1.  **Aθ(*n*)算法为 O( *n* )** :真。我们肯定可以在不改变原程序的情况下实现 O( *n* )。
2.  **Aθ(*n*)算法为 O( *n* )** :由于 *n* 比 *n* 差，这个是真的。
3.  **Aθ(*n*)算法为 O( *n* )** : As *n* 比 *n* 差，这个是真的。
4.  **Aθ(*n*)算法为 O(1)** :此为假。因为 O(1)应该比θ(*n*)差。反而其实更好。
5.  **一个 O(1)算法是θ(1):**真。因为这两种复杂性是相同的。
6.  **A O( *n* )算法是θ(1)**:A O(*n*)程序可以是θ(*n*)也可以是θ(1)。如果是θ(*n*)那么这将为假，否则如果是θ(1)这将为真。所以，这是真/假取决于程序。但一般来说，这是错误的。

我之前说过，O 是复杂度的上限。O 界有两种类型，紧界和上界。如果一个程序是θ(*n*)和 O( *n* )，那么 O( *n* )就是一个紧界，因为它和θ(*n*)相同。如果一个程序是θ(1)且 O(1)或 O( *n* )那么 O(1)称为紧界，O( *n* )则不是紧界，只是上界(也许我们可以称之为弱界，我不知道)。

如果我们找到的界不是紧界，那么我们可以把 is 写成 o( *n* )，读作“小 o 的 *n，*”来说明我们知道我们的界不是紧的。

如果我们能找到紧边界就更好了，因为这些能给我们更多关于程序行为的信息。我想这就是为什么“O”比“O”用得少的原因。但是，这并不总是容易做到的。

## 总结一下，数学上我们可以写，(这是我怎么记得的)

θ≤O≤O(读作θ小于/等于 Big-O，而 Big-O 小于/等于 Small-o)

***经验法则:*** *算出一个算法的 O 复杂度比θ复杂度更容易。*

我希望你已经和θ，O，O 讲和了，因为在结束符号部分之前，我们将再引入两个符号。它们很容易理解，我保证！

# 大欧米伽符号

## 这是什么？

我们在上面看到了 Big-O 符号，它告诉我们程序的极限，在这里我们的程序不会比一个特定的界限慢。如果我们反其道而行之，修改我们的程序使之更好，并计算复杂性，我们使用符号ω，读作“big-omega”这给了我们复杂性，我们知道我们的程序不会更好。当我们想要证明一个程序运行缓慢或者是一个坏程序时，这是特别有用的。

比如我们说算法是ω(*n*)，那么算法不可能比 *n* 更好。可能是θ(*n*)或者θ(*n*⁴)或者更糟。所以，ω给了我们一个算法复杂度的下界。

类似于小 o，如果我们知道我们的界不是紧的，我们可以写ω。比如一个θ(*n*)算法是ω(*n*)和ω( *n* ⁴).

总而言之，从数学上讲，ω≤ω≤θ(读作小ω比大ω小/相等，大ω比θ小/相等)

## 实践

让我们练习一下。对于以下θ复杂性，写下一个紧和非紧 O 界，以及一个你选择的紧和非紧ω界，前提是它们存在。

1.  Θ(1)
2.  θ(√*n*)(*n*的平方根)
3.  θ(*n*)

## 解决方法

1.  严格的界限将是 O(1)和ω(1)。非紧界 O 将是 o( *n* )。这些函数不能小于 1，所以不存在非紧ω界。
2.  紧边界将是 O(√ *n* )和ω(√*n*)。当 *n* 大于√ *n* 时，非紧 O 界限将为 o( *n* )。当 1 小于√n 时，非紧ω界为ω(1)。
3.  紧边界将是 O( *n* )和ω(*n*)。非紧 o 绑定将是 o( *n* ⁴)或 o( *n* ⁵)或 o( *n* ⁶).)非紧ω界限将是ω( *n* )或ω( *n* )或ω(√ *n* )或ω(1)。

如果你不记得所有的符号和它们的用法，也不要担心。最重要的符号是 O 和θ。

***经验法则:*** *所有的符号 O、O、ω、ω和θ有时都是有用的，O 是最常用的，因为它比θ更容易确定，也比ω更实用。*

# θ、O 和ω符号概述

ω≤ω≤θ≤O≤O(这个应该不需要解释)。

## 一些幽默

*(感谢*[*【Mitch Mithunpaul】*](https://www.youtube.com/channel/UCAiE4uwGJqERSzr174WbJZA)*，以下是来自他的 YouTube 评论)*

大 O 就像一个老大哥说:无论你做什么，你永远不会比我好。

欧米茄就像你的小弟弟，他说，做你想做的任何坏事，但如果你跌倒，我会在那里接住你。

西塔就像你的妹妹，根据恶作剧的利害关系——被吼还是糖果，她不停地转换立场。

# 对数

如果你知道对数是什么，请随意跳过这一节。这只是简单的介绍。

## 定义

对数是一种应用于数字的运算，它使数字变小，很像数字的平方根。老实说，这个定义并不适合我——我总是需要一个合乎逻辑的例子来更好地理解。

## 例子

2ˣ = 1024.如果我们想解出这个 x 的方程，我们需要找到一个数，当它被应用于 2 时，结果是 1024。现在，我们可以手工做这个，解出 x，我们会得到 10 乘以 2 的⁰=1024.对数帮助我们用一种新的符号来表示这一点。在这种情况下，10 是 1024 的以 2 为底的对数，我们可以把它写成 log₂(1024，读作 1024 的以 2 为底的对数。

## 概括来说:

对于 2ˣ = N，那么 log₂(N) = x

## 使用以 2 为底的对数

因为我们用 2 作为基数，所以这些对数叫做以 2 为基数的对数。我们将只使用基数为 2 的对数，因为它们比任何其他类型的对数都更常见。这是因为我们通常只有两个不同的实体:0 和 1。

[*如果你想练习对数*](http://tutorial.math.lamar.edu/Classes/Alg/LogFunctions.aspx) *。*

## 实践

指出你在下面每种情况下找到的对数(以 2 为底):

1.  2ˣ = 64
2.  (2²)ˣ = 64
3.  4ˣ = 4
4.  2ˣ = 1
5.  2ˣ + 2ˣ = 32
6.  (2ˣ) * (2ˣ) = 64

## 解决方法

1.  2ˣ = 64 :这个很简单。就像我们之前看到的例子一样。这里 x 会是 6。因此，log(64)=6。
2.  **( )ˣ = 64** :我们可以把这个简化为 2 个ˣ.如果我们解出 x，我们会得到 3，那么它就变得和上面第一个问题一样了。因此，log(64)=6。
3.  **4ˣ = 4** :记住，我们必须在等式的左边使用 2，因此我们将它重写为 2 ˣ，这与第二个问题相同。解出 x 后，我们会发现 1。因此，log(4)=2。
4.  **2ˣ = 1** : log(1)=0。因为 x 需要为 0 才能让左边变成 1。
5.  **2ˣ + 2ˣ = 32** :我们可以把这个函数简化为 2*2ˣ = 2 *2ˣ = 2 ⁺ˣ，求解 x 后，我们会发现 4。因此，2 ⁺⁴=32，因此 log(32)=5。
6.  **(2ˣ) * (2ˣ) = 64** :我们可以把这个改写成 2ˣ⁺ˣ = 2 ˣ.这和第二个问题是一样的。因此，log(64)=6。

# 递归复杂性

我们大多数人都知道什么是递归函数——我们在大学的阶乘程序中做过。

## 定义

递归函数是调用自身的函数。

## 例子

正如我之前提到的，我们都见过的最常见的例子是阶乘。阶乘程序可以递归编写——显然是一个非常短的程序。

## 分析复杂性

我们来分析一下这个程序的复杂度。它没有任何循环，但复杂度不是常数。如果我们只是简单地回到基础(计算指令)，我们会清楚地发现程序执行了 n 次。所以函数是θ(*n*)，线性复杂度。

# 对数复杂度

## 二分搜索法的例子

正如我在对数部分提到的，对数使 n 变得更小。一个著名的问题是[二分搜索法](https://en.wikipedia.org/wiki/Binary_search_algorithm)。二分搜索法通过将数组切成两半来查找数组中的元素，并继续这样做，直到找到该元素。

二分搜索法最简单的实现是递归算法。因此，对于每个调用，数组中的元素数量将减半:

0ᵗʰ迭代: *n*
1ˢᵗ迭代: *n* / 2
2ⁿᵈ迭代: *n* / 4
3ʳᵈ迭代:*n*/8
…
*I*ᵗʰ迭代:*n*/2*ⁱ*
…
最后一次迭代:1

注意，在 I-迭代中，我们的数组有 *n* / 2 *ⁱ* 个元素。如果我们希望找到哪个迭代将是最后一次迭代，1 = *n* / 2 *ⁱ* 必须重写为 2 *ⁱ* = *n* 。

现在，使用对数，i = log( *n* )。这告诉我们，执行二分搜索法所需的指令数量是 log( *n* )，其中 n 是输入数组中元素的数量。

这允许我们比较二分搜索法算法和线性搜索算法。线性搜索算法复杂度为 O( *n* )。我们可以清楚地看到，log( *n* )比 n 小得多，所以，我们可以得出结论，二分搜索法算法肯定比线性搜索算法好。

恭喜你学会了这个看似很难的题目。等你熬过来了就好办了！

如果你已经走到这一步，我衷心感谢你。我知道这是一篇非常长的文章，但有专门关于算法复杂性的书籍——我在这里试图做的是总结我从 Dionysis Zindros 的文章中所学到的东西，正如本文结尾所提到的。

# 一些附加材料

*   哈佛 CS50 —渐近符号
    [https://youtu.be/iOq5kSKqeR4](https://youtu.be/iOq5kSKqeR4)
*   大 O 记法(通用快速教程)
    [https://youtu.be/V6mKVRU1evU](https://youtu.be/V6mKVRU1evU)
*   大 O 符号(还有ω和θ)—最佳数学解释
    [https://youtu.be/ei-A_wy5Yxw](https://youtu.be/ei-A_wy5Yxw)

非常感谢 dionysizndros。这篇文章是[的总结，一篇非常详细的文章可以在这里访问](http://discrete.gr/complexity)。我花了五个小时看完整篇文章，但这是非常值得的！