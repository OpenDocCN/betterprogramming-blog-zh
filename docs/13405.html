<html>
<head>
<title>How To Build a YOLOv5 Object Detection App on iOS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在iOS上搭建YOLOv5物体检测App</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-build-a-yolov5-object-detection-app-on-ios-39c8c77dfe58?source=collection_archive---------5-----------------------#2022-08-24">https://betterprogramming.pub/how-to-build-a-yolov5-object-detection-app-on-ios-39c8c77dfe58?source=collection_archive---------5-----------------------#2022-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f5e1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我用YOLOv5和Core ML搭建了一个iOS物体检测app。以下是你也可以建立一个的方法！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7586f445b32f3d37f51b672b96990bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuR_9TzVTmVQraheVbV-Sw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卡罗琳娜·格拉博斯卡在<a class="ae kv" href="https://www.pexels.com/photo/folding-chair-and-a-potted-plant-in-a-white-interior-7193647/" rel="noopener ugc nofollow" target="_blank">的照片</a></p></figure><p id="bbe5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本教程将教你如何使用<a class="ae kv" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> YOLOv5型号</a>构建一个实时视频对象检测iOS应用程序！</p><p id="2d4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该教程基于苹果公司提供的<a class="ae kv" href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture" rel="noopener ugc nofollow" target="_blank">这个</a>物体检测示例，但该教程将特别展示如何在iOS上使用自定义模型。</p><p id="1e11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了增加刺激，下面是最终版本的样子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/88107d6ea9c6896aaf85a0c87288618b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2t-dmMjHWscRNmN-EzaRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在iOS上运行YOLOv5模型的最终应用</p></figure><h1 id="0c43" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">YOLOv5、CoreML和Vision</h1><p id="8eae" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">下面你可以找到我们构建示例应用程序所需的关键构件的摘要。</p><h2 id="8333" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">YOLOv5</h2><p id="6757" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> YOLOv5 </a>是使用<a class="ae kv" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>构建的一系列对象检测模型。这些模型能够从单个图像中检测对象，其中模型输出包括边界框的预测、边界框分类和预测的置信度。</p><p id="f603" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预训练的YOLOv5模型已经使用<a class="ae kv" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO数据集</a>进行了训练，该数据集包括80种不同的日常对象类别，但使用自定义对象类别训练模型非常简单。可以参考<a class="ae kv" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data" rel="noopener ugc nofollow" target="_blank">官方指南</a>获取说明。</p><p id="4a55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">YOLOv5 GitHub库提供了各种实用工具，比如将预训练的PyTorch模型导出为各种其他格式。此类格式包括<a class="ae kv" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>、<a class="ae kv" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a>、<a class="ae kv" href="https://developer.apple.com/documentation/coreml" rel="noopener ugc nofollow" target="_blank"> Core ML </a>。使用核心ML是本教程的重点。</p><h2 id="39b2" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">核心ML</h2><p id="9770" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">苹果的核心ML支持在iOS设备上使用机器学习(ML)模型。Core ML提供了各种现成的API和模型，但它也支持构建完全自定义的模型。</p><h2 id="0e57" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">视力</h2><p id="6120" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">苹果的Vision框架旨在支持iOS上的各种标准计算机视觉任务。我还允许使用自定义的核心ML模型。</p><h1 id="badc" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">让我们开始—设置一个Xcode项目</h1><p id="b40d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">首先打开Xcode并选择“创建新的Xcode项目”</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/07022032d448b731d2c59b09a3fe8c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bv09L3HofFUFU5REjokVbA.png"/></div></div></figure><p id="4869" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于模板类型，选择“App”</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/5fb7866dd571a52e2bcb44b0a0c00573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gDa3w6_1BeIb5S3DHmL2w.png"/></div></div></figure><p id="810d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请确保将“组织标识符”编辑为例如您的Github用户名，并为项目选择一个名称(产品名称)。对于界面，选择“故事板”，对于语言，选择“Swift”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/c8b46e26ba3c6f9d3c4d6ed776c00e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7SHTUOOhTFFOVdOQR8g7g.png"/></div></div></figure><p id="265d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完成上述步骤后，项目结构将如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/49d6247955ad03a3362c20cb018ff2a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRdgowK_3u_uJncmyfdkQA.png"/></div></div></figure><h1 id="7cde" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">守则</h1><p id="c7aa" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">完整的示例源代码可以在<a class="ae kv" href="https://github.com/hietalajulius/ios-demo-app/tree/main/ObjectDetection-CoreML" rel="noopener ugc nofollow" target="_blank">这里</a>找到，但我们将在以下部分中浏览重要部分。</p><h2 id="b1d2" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">ViewController</h2><p id="bf00" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">应用程序的主要逻辑将在<code class="fe ng nh ni nj b"><strong class="ky ir">ViewController</strong></code>类中创建。下面的代码显示了类的成员。</p><p id="594f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所创建的主要项目是视频捕获会话和用于显示输出视频流、检测和推断时间的可视层。此外，这里对<code class="fe ng nh ni nj b"><strong class="ky ir">Vision</strong></code>预测请求数组进行初始化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6db1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最重要的是<code class="fe ng nh ni nj b">previewView</code>需要连接到<code class="fe ng nh ni nj b">Main.storyboard</code>文件中的主<code class="fe ng nh ni nj b">View</code>，以便在屏幕上显示不同的可视层。这可以通过选择视图，按下控制，并将其拖到<code class="fe ng nh ni nj b">ViewController</code>的一行中来完成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/bcd06119278f023008da5665e65aa5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Hsl9NUWFfPK1ptg4qKfRw.png"/></div></div></figure><h2 id="d2fd" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">viewDidLoad</h2><p id="5574" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">初始化类成员后，您应该设置<code class="fe ng nh ni nj b">viewDidLoad</code>方法，该方法在视图控制器将其视图层次加载到内存后调用。方法分为初始化</p><ul class=""><li id="8e13" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated">视频捕获</li><li id="01e8" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">视频预览输出</li><li id="29ff" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">可视层</li><li id="3777" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">视觉预测。</li></ul><p id="e230" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它在代码中的布局如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="f6f7" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">设置视频输入和输出</h2><p id="24fa" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们从设置视频捕获和输出开始，逐步介绍不同的元素。以下是实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="8803" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该逻辑的要点(并非双关语)是找到一个设备(背面摄像头)并将其添加到<code class="fe ng nh ni nj b">AvCaptureSession</code>中。<code class="fe ng nh ni nj b">setupOutput</code>方法还将输出添加到进程中。总而言之，<code class="fe ng nh ni nj b">AvCaptureSession</code>负责将输入(相机)连接到输出(屏幕)。</p><h2 id="4aa9" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">设置用户界面层</h2><p id="8451" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">视频可视屏幕层设置如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="e47f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">显示预览视频(<code class="fe ng nh ni nj b">previewLayer</code>，以<code class="fe ng nh ni nj b">AvCaptureSession</code>为自变量)、检测(<code class="fe ng nh ni nj b">detectionLayer</code>)和显示当前推断时间(<code class="fe ng nh ni nj b">inferenceTimeLayer</code>)有不同的层次。</p><h2 id="bca3" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">建立愿景</h2><p id="7ff8" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">最后，视觉设置通过以下方式实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="887e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在下一节中重新讨论在项目中包含实际的检测模型(上述要点中的第2行和第7行)，但是这里要注意的最重要的部分是任务定义，即<code class="fe ng nh ni nj b">Vision</code>应该在第8行执行的请求，并为结果定义回调(<code class="fe ng nh ni nj b">drawResults</code>)。</p><p id="2b2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了将所有内容联系在一起，我们还定义了一个方法(<code class="fe ng nh ni nj b">captureOutput</code>)，每当一个新的帧从摄像机到达时都会调用该方法，该方法的任务是使用新图像向Vision发送一个新的对象识别请求:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="1524" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">导出检测模型</h1><p id="e1a5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如前所述，在运行应用程序之前，我们需要创建YOLOv5 Core ML模型。为此，您应该用以下代码克隆这个存储库:</p><pre class="kg kh ki kj gt ob nj oc od aw oe bi"><span id="07f4" class="mq lu iq nj b gy of og l oh oi">git clone <a class="ae kv" href="https://github.com/hietalajulius/yolov5" rel="noopener ugc nofollow" target="_blank">https://github.com/hietalajulius/yolov5</a></span></pre><p id="cd03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要创建导出的模型，您需要python 3.7或更高版本，并从存储库的根目录安装依赖项，包括:</p><pre class="kg kh ki kj gt ob nj oc od aw oe bi"><span id="cd5a" class="mq lu iq nj b gy of og l oh oi">pip install -r requirements.txt -r requirements-export.txt</span></pre><p id="55a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">成功安装依赖项后，可以使用以下命令运行实际导出:</p><pre class="kg kh ki kj gt ob nj oc od aw oe bi"><span id="d4e4" class="mq lu iq nj b gy of og l oh oi">python export-nms.py --include coreml --weights yolov5n.pt</span></pre><p id="0410" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该脚本是原始YOLOv5 <code class="fe ng nh ni nj b">export.py</code>脚本的修改版本，在模型末尾包含<a class="ae kv" href="https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/#:~:text=Non%20Maximum%20Suppression%20(NMS)%20is,arrive%20at%20the%20desired%20results." rel="noopener ugc nofollow" target="_blank">非最大抑制</a> (NMS)以支持使用视觉。本例中使用的砝码来自YOLOv5模型的nano变体。这个脚本负责下载训练好的模型，所以不需要手动下载。</p><p id="e746" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该脚本输出一个名为<code class="fe ng nh ni nj b">yolov5n.mlmodel</code>的核心ML文件，需要拖放到Xcode项目中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0e3b16b8c22b8f9ab10f11458baab362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*J7NU0IXvUqWWp9Kg1zk6iA.png"/></div></figure><p id="bbdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将会看到下面的屏幕提示，在这里记住将模型文件包含在构建的目标中是很重要的(“Add to targets”)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/e128d354c107fa3bfc76c9ed53ce5bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*irP-8kfHBkJmVjRhqkQahA.png"/></div></div></figure><h1 id="74e2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">运行应用程序</h1><p id="0aae" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">要运行完成的应用程序，请在Xcode上选择一个iOS模拟器或设备来运行该应用程序。应用程序将开始输出预测和当前推断时间:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/88107d6ea9c6896aaf85a0c87288618b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2t-dmMjHWscRNmN-EzaRw.png"/></div></div></figure><p id="8b52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您的阅读！欢迎反馈和改进意见！</p><pre class="kg kh ki kj gt ob nj oc od aw oe bi"><span id="05c2" class="mq lu iq nj b gy of og l oh oi"><strong class="nj ir">Want to Connect?</strong></span><span id="15ac" class="mq lu iq nj b gy ol og l oh oi">You can find me on <a class="ae kv" href="https://www.linkedin.com/in/julius-hietala-8967b8a2/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>, <a class="ae kv" href="https://twitter.com/hietalajulius?lang=en" rel="noopener ugc nofollow" target="_blank">Twitter</a>, and <a class="ae kv" href="https://github.com/hietalajulius" rel="noopener ugc nofollow" target="_blank">Github</a>. Also check my personal <a class="ae kv" href="https://www.hietalajulius.com/" rel="noopener ugc nofollow" target="_blank">website/blog</a> for topics covering ML and web/mobile apps!</span></pre></div></div>    
</body>
</html>