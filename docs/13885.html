<html>
<head>
<title>Automatic Subtitles Dubbing on YouTube Using Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用计算机视觉在YouTube上自动配音字幕</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/automatic-subtitles-dubbing-on-youtube-using-computer-vision-35ad776ffe18?source=collection_archive---------0-----------------------#2022-10-09">https://betterprogramming.pub/automatic-subtitles-dubbing-on-youtube-using-computer-vision-35ad776ffe18?source=collection_archive---------0-----------------------#2022-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fc4c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">帮助您访问更多内容的循序渐进的Python指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/366a2b352b4bcf8e510f6876dc4d90fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B-HCOzwn1G5gIrdqcnm-EQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://www.shutterstock.com/image-photo/voiceover-artist-actor-recording-voice-overs-2061833759" rel="noopener ugc nofollow" target="_blank">图片来自shutterstock </a></p></figure><p id="b2f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大约半年前，我看到了一个<a class="ae kv" href="https://www.youtube.com/watch?v=apCfQt_f_Jo" rel="noopener ugc nofollow" target="_blank">扩展</a>，标题和我的文章差不多。我对这个想法很好奇，想做一些类似的事情，除了使用计算机视觉，这正是我得到的结果:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><h1 id="1392" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">项目的总体架构</h1><p id="ec67" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">该项目基于三个服务，其中第一个服务负责检测字幕并将带有文本的图像转换为机器可读的文本格式，第二个服务用于翻译文本(目前仅从英语翻译为俄语)，最后一个服务负责可视化和文本配音。使用<a class="ae kv" href="https://github.com/zeromq/pyzmq" rel="noopener ugc nofollow" target="_blank"> ZeroMQ库</a>实现服务之间的通信。</p><h2 id="1cd6" class="mr lv iq bd lw ms mt dn ma mu mv dp me lf mw mx mg lj my mz mi ln na nb mk nc bi translated">数据收集</h2><p id="56ce" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">几乎任何ML任务的解决方案都是从数据收集开始的——这次也不例外，所以有必要收集一个数据集，其中包括带有字幕的YouTube视频的截图，并突出显示字幕本身的边界框。以下是数据集的基本要求:</p><p id="7b09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)字幕必须使用不同的语言。</p><p id="e7d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2)带有视频剪辑的图片应该具有不同的大小，这意味着视频剪辑可以是全屏的或者是屏幕的一部分。</p><p id="1bae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3)字幕必须在屏幕的不同区域。</p><p id="4e9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4)对字幕本身的要求:</p><ul class=""><li id="320f" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">可以是不同的尺寸</li><li id="92d3" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">字体类型:正常和按比例无衬线</li><li id="11f1" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">字体颜色:白色</li><li id="8988" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">背景颜色:黑色</li><li id="c0d0" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">背景的透明度:75%</li><li id="5f2f" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">窗口透明度:0</li></ul><p id="e613" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原始数据集可以在<a class="ae kv" href="https://www.kaggle.com/datasets/wadzim/youtube-subtitles" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到。</p><h2 id="472b" class="mr lv iq bd lw ms mt dn ma mu mv dp me lf mw mx mg lj my mz mi ln na nb mk nc bi translated">字幕检测</h2><p id="d129" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">在收集了数据集之后，需要训练模型来找到字幕，以便实时检测它们。我选了<a class="ae kv" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> yolov5 </a>。在对预训练模型进行50次训练后，我们得到了一些令人印象深刻的指标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/389b1e9ec16549751b159992b1a3fde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeJcN18JNtcKvGrSt46xug.png"/></div></div></figure><p id="3733" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是实时检测的工作原理:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><h2 id="8539" class="mr lv iq bd lw ms mt dn ma mu mv dp me lf mw mx mg lj my mz mi ln na nb mk nc bi translated">光学字符识别用图像的准备</h2><p id="e89b" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">对于大多数OCR库，我们将在下一节中介绍，最好是传递灰度图像，这就是为什么我做的第一件事是这样的:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="2a17" class="mr lv iq nt b gy nx ny l nz oa">kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))<br/>gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br/>_, thresh_img = cv2.threshold(gray, 200, 255,<br/>                          cv2.THRESH_BINARY)<br/>transformation_image = cv2.morphologyEx(thresh_img, cv2.MORPH_OPEN, <br/>                                        kernel, iterations=1)</span></pre><p id="2b8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它以某种形式出现:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4f171b3f8e135802cc89bd9c7b4f1d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*INnI52Hv7bVh42w2ckmICg.png"/></div></figure><p id="e425" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当图像准备好时，人们可能会认为可以将其发送到OCR库，但事实并非如此。问题是yolo每秒发出大约10帧，因此我们会得到许多描绘相同文本的图像，但为什么我们需要多次翻译和配音相同的文本呢？</p><p id="8aca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在OpenCV中，有一个很奇妙的函数叫做<code class="fe oc od oe nt b">matchTemplate()</code>。这个函数可以被看作是对象检测的一种非常简单的形式。使用模板匹配，我们可以使用包含我们想要检测的对象的“模板”来检测输入图像中的对象:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/9d27e0ce2d6c09efaa77e536bebf9689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-aFZTDnHMc2V0vcz6JoklA.png"/></div></div></figure><p id="fd6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了在原始图像中找到模板，我们在原始图像上从左到右和从上到下移动模板:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/ee27aafd7759122b6973c73e48e4744e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jtHhkK_5eP4fW1ce_GTgUw.png"/></div></div></figure><p id="c637" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数返回两幅图像彼此相似程度的概率(从0到1 ),作为其参数之一。使用这个数字，我们可以假设如果它大于0.75，那么两个图像具有相同的文本。你可以在这里阅读关于这个功能<a class="ae kv" href="https://pyimagesearch.com/2021/03/22/opencv-template-matching-cv2-matchtemplate/" rel="noopener ugc nofollow" target="_blank">的更多信息。</a></p><p id="94b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个问题是，下一帧中的文本被添加到前一帧中1-2个单词，即:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c0da9f641b702a5d3ad6049303a66229.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*yOtFohGBa7ety6i4eK0RSA.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/133a14e2fbea747debfab5ee756d9f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Upcdc3ADwJYBHqG6po4pBQ.png"/></div></figure><p id="077f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如你所见，第一张图片的文字与第二张图片的文字只有一个“是”我认为一次取出一个单词并把这一个单词传递给下一个步骤是不可取的，所以我建议以某种方式确定该行结束，并只把该行结束的图片传递给下一个步骤。我们将如何确定这一点将在下面进一步描述。</p><p id="6a2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe oc od oe nt b"><a class="ae kv" href="https://pyimagesearch.com/2021/01/19/opencv-bitwise-and-or-xor-and-not/" rel="noopener ugc nofollow" target="_blank">bitwise_and()</a></code> <a class="ae kv" href="https://pyimagesearch.com/2021/01/19/opencv-bitwise-and-or-xor-and-not/" rel="noopener ugc nofollow" target="_blank">函数</a>将帮助我们解决这个问题。这种按位运算合并两个图像，以便在输出图像中只保留两个图像的相同部分。在理想情况下，如果我们将这个函数应用于上面的两幅图像，我们将得到如下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4f171b3f8e135802cc89bd9c7b4f1d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*INnI52Hv7bVh42w2ckmICg.png"/></div></figure><p id="63d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们实际得到的是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/79ff05c6d4b0ba7fb65058451c6b9c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*rkLe4cDLM4ZjRP5VmJVLTQ.png"/></div></figure><p id="78d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实是，检测并不总是以相同的方式工作，即，经常发生的情况是，具有相同文本但在不同帧上的图像在宽度和/或高度方面会有2-3个像素的差异。在这种情况下，您可以找到每个字母的边界框，并在这些坐标处裁剪图像，这样我们就可以移除文本周围的黑色背景。</p><p id="5400" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果，我们会得到一个类似于理想情况的图像。剩下的就很简单了:我们将图像和第一个图像(最后没有“是”这个词)一起传递给<code class="fe oc od oe nt b">matchTemplate()</code>函数，做和上面几段描述的一样的事情。就是这样。图像已准备好进行文本识别:)</p><h2 id="d2fd" class="mr lv iq bd lw ms mt dn ma mu mv dp me lf mw mx mg lj my mz mi ln na nb mk nc bi translated">光学字符识别</h2><p id="17f9" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">在我看来，在一次成功的检测之后，我想在OCR方面取得同样的成功，但却是一团糟。作为文本识别的库，我查了以下:<a class="ae kv" href="https://github.com/madmaze/pytesseract" rel="noopener ugc nofollow" target="_blank">宇宙魔方</a>、<a class="ae kv" href="https://github.com/JaidedAI/EasyOCR" rel="noopener ugc nofollow" target="_blank"> EasyOCR </a>和<a class="ae kv" href="https://github.com/PaddlePaddle/PaddleOCR" rel="noopener ugc nofollow" target="_blank"> PaddleOCR </a>。现在我们必须从建议中选择一个。我决定根据三个标准来研究和检查每个库对我的数据的处理情况:CER、WER和算法运行时间。你可以在这里阅读前两个指标<a class="ae kv" href="https://towardsdatascience.com/evaluating-ocr-output-quality-with-character- error-rate-cer-and-word-error-rate-wer-853175297510" rel="noopener" target="_blank"/>。我从数据集中提取了50张图片，并将它们放在一个单独的<a class="ae kv" href="https://github.com/wb-08/SubVision/blob/main/additional_data/check.json" rel="noopener ugc nofollow" target="_blank"> JSON文件</a>中。我把数据经过每个解的推断，得到了以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/b4871229a345c36baab53abdd075d41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yoq5A5398HZIQcJl-2IVPA.png"/></div></div></figure><p id="01d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">老实说，我认为结果会更好，因为我已经准备了一个非常好的灰度图像:最小的背景噪声，直线文本，没有旋转或扭曲。</p><p id="cd81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们必须决定使用哪个库。使用PaddleOCR似乎几乎是显而易见的，但问题是我只有2Gb的显存，其中~1.5Gb是检测模型消耗的。在CPU上，这种解决方案运行的时间足够长，对于实时来说，它不是很合适。同样的情况也发生在EasyOCR上，只不过在CPU上，我无法启动它，但是当我有一个像样的GPU时，我会把它记在心里:)只剩下宇宙魔方了，那就是我要用的。</p><h2 id="1838" class="mr lv iq bd lw ms mt dn ma mu mv dp me lf mw mx mg lj my mz mi ln na nb mk nc bi translated">文字处理和翻译</h2><p id="bcb3" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">Tesseract经常会拼错一个单词中的1-2个字母，之后，这个单词可能会被错误地翻译。所以我决定使用<a class="ae kv" href="https://pypi.org/project/pyenchant/" rel="noopener ugc nofollow" target="_blank"> pyenchant </a>来检查单词的拼写，如果单词拼写错误，库将建议一个与当前单词相似的单词，并将结果文本传递给下一步——翻译。</p><p id="ad6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">翻译部分很简单——<a class="ae kv" href="https://github.com/ssut/py-googletrans" rel="noopener ugc nofollow" target="_blank">Google翻译API </a>。翻译在某些情况下不太正确，但很快，我没有注意到对请求数量的任何限制。</p><h2 id="b1f7" class="mr lv iq bd lw ms mt dn ma mu mv dp me lf mw mx mg lj my mz mi ln na nb mk nc bi translated">配音和文本可视化</h2><p id="5a6e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">为了配音，我用了不那么复杂的<a class="ae kv" href="https://github.com/nateshmbhat/pyttsx3" rel="noopener ugc nofollow" target="_blank"> pyttsx3 </a>。俄语听起来没那么好，但随着时间的推移你会习惯的。文本可视化是用<a class="ae kv" href="https://github.com/PySimpleGUI/PySimpleGUI" rel="noopener ugc nofollow" target="_blank"> PySimpleGUI </a>完成的。</p><h1 id="09e6" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论</h1><p id="ff86" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">该项目不太可能被人们用来解决问题，至少在目前的实施中，因为仍然存在问题。一个这样的问题是速度(落后于现实五秒钟)，但我有改进这个项目的想法:)</p><p id="6710" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">源代码可以在<a class="ae kv" href="https://github.com/wb-08/SubVision" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="d796" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">祝你今天开心！玩的开心！</p></div></div>    
</body>
</html>