<html>
<head>
<title>Kubeflow Pipelines With GPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带GPU的Kubeflow管道</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/kubeflow-pipelines-with-gpus-1af6a74ec2a?source=collection_archive---------4-----------------------#2019-07-01">https://betterprogramming.pub/kubeflow-pipelines-with-gpus-1af6a74ec2a?source=collection_archive---------4-----------------------#2019-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6787fe7711ed5c707ebd6dee9b6c41ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKov3tzuGKU4aIx77Nq9XQ.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com/search/photos/pipelines?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kc" href="https://unsplash.com/@isak_combrinck?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Isak Combrinck </a>拍摄的照片</p></figure><p id="23c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算密集型DL和ML工作负载，从银行欺诈检测到流媒体服务的视频推荐，都需要频繁的大规模训练和推理。<a class="ae kc" href="https://www.kubeflow.org/" rel="noopener ugc nofollow" target="_blank"> Kubeflow </a>是一个基于Kubernetes构建的端到端平台，用于机器学习和深度学习模型训练和部署。Kubeflow于2018年作为谷歌的开源项目推出，迅速成为各行各业最受欢迎的混合云ML工具包之一，贡献者群体不断增长。</p><p id="2557" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> Kubeflow寻求实现以下目标:</em></p><ul class=""><li id="3fe2" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">扩展底层Kubernetes基础设施，使ML/DL部署变得简单、可重复和可移植。</li><li id="a278" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">允许研究人员专注于共享笔记本电脑和数据的快速实验，而无需担心底层基础架构。</li><li id="e5b4" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">提供在本地或云中运行训练和推理的能力，只需对代码库进行最小的更改。</li></ul><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/211c67862a62a79520891ae41cc047aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQsFV3o1c3Amu26Z-IEd7w.png"/></div></div></figure><h2 id="aada" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">库伯弗洛管道公司</h2><p id="8317" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">典型的数据科学工作流通常包括数据验证、功能工程、模型训练和可扩展部署等阶段。容器使得部署更加容易，因为数据科学家和ML工程师可以很好地打包他们的代码并在计算环境中移植，而不用担心依赖性。Kubernetes支持可重复的批量作业创建、高效的节点跟踪和监控，以及直观的横向扩展方式。</p><p id="04fe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，我们如何才能使工作流模块化、可重复、可移植并与数据科学团队中的其他成员共享呢？进入<a class="ae kc" href="https://www.kubeflow.org/docs/pipelines/pipelines-overview/" rel="noopener ugc nofollow" target="_blank"> Kubeflow管道</a>。</p><p id="82f8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">容器构成了Kubeflow管道的基本构件。每个都包含一个与管道中的步骤对应的脚本——如预处理、训练和服务中的步骤——输入和输出路径被指定为命令行参数。用Python脚本或Jupyter笔记本编写的管道定义文件将各个组件粘合在一起，形成一个图形。具体来说，它定义了管道的参数和每个组件的输入、输出以及与其他组件的关系。</p><p id="092d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Pipelines UI是一个仪表板，支持上传已编译的管道，以便可视化和运行图形。可以通过改变管道参数来配置不同的运行，从而允许对训练试验进行深入分析，并对所服务的模型进行有效的版本控制。</p><h2 id="e027" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">GPU加速的Kubeflow流水线</h2><p id="ecd4" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">由于Kubeflow已经成为跨企业的此类需求的流行框架，NVIDIA与社区密切合作，将GPU技术集成到生态系统中。</p><p id="9d7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行GPU加速的Kubeflow管道并不难。查看<a class="ae kc" href="https://github.com/kubeflow/pipelines/tree/master/samples/nvidia-resnet" rel="noopener ugc nofollow" target="_blank">本教程</a>关于如何在CIFAR-10数据集上训练Keras中的Resnet-50模型。这应该可以帮助你开始。克隆一个副本，今天就开始实验吧！</p><p id="5adb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是一些集成到管道中的组件，使GPU的使用更加简单有效:</p><ul class=""><li id="4959" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated"><a class="ae kc" href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TensorFlow </strong> </a>集装箱来自(NGC) <strong class="kf ir"> : </strong>我们都在为发展争取合适的环境。获得所需的DL框架、库和GPU驱动程序可能是一项挑战，而且非常耗时。因此，在众多解决方案中，NGC提供了高度优化的专用容器，可以从你的个人电脑、DGX站或云端运行。</li><li id="6c76" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated"><a class="ae kc" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TensorRT </strong> </a>:在训练过程之后，为了减少延迟和增加吞吐量，优化模型进行推理是很常见的，尤其是在生产环境中。TensorRT带来了几项优化，如层&amp;张量融合、精确校准和内核自动调整。</li><li id="fe00" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated"><a class="ae kc" href="https://github.com/NVIDIA/tensorrt-inference-server" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TensorRT推理服务器</strong> </a> <strong class="kf ir"> : </strong>使用GPU大规模部署模型需要在处理单元之间灵活、均匀地分配工作负载。<strong class="kf ir"> </strong> TensorRT推理服务器是一款容器化的生产就绪型软件服务器，用于数据中心部署，支持多模型、多框架。通过对客户端请求进行高效的动态批处理，TensorRT推理服务器能够处理大量的传入请求，并智能地平衡GPU之间的负载。</li></ul><h2 id="b976" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">Kubeflow管道教程概述</h2><p id="03dd" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">现在让我们看看上面的技术是如何与我们的Kubeflow管道示例集成的。</p><h2 id="351b" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">预处理容器:</h2><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/93412eae298d5262c2df68771b69c00f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/0*EpHIvNpHMt3n4VQR"/></div></figure><ul class=""><li id="63a3" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">基于来自NGC的<a class="ae kc" href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow" rel="noopener ugc nofollow" target="_blank"> TensorFlow容器</a> (19.03-py3)构建，作为基础映像。</li><li id="27ea" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">entrypoint脚本加载Keras库附带的CIFAR-10数据集。</li><li id="402b" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">图像被旋转、缩放和裁剪，并保存在输出目录中。</li></ul><h2 id="eca0" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">培训容器:</h2><ul class=""><li id="cdbb" class="lc ld iq kf b kg mo kk mp ko mu ks mv kw mw la lh li lj lk bi translated">同样构建在<a class="ae kc" href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow" rel="noopener ugc nofollow" target="_blank"> TensorFlow容器</a> (19.03-py3)之上作为基础映像。</li><li id="5262" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">entrypoint脚本在预处理的数据上训练ResNet-50模型，该预处理的数据被装载在管道的指定体积参数中。</li><li id="50b1" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">然后，生成的Keras <strong class="kf ir"> h5 </strong>模型文件被转换为<strong class="kf ir"> SavedModel </strong>文件，并使用TensorFlow (TF-TRT)中的TensorRT集成进行优化，以利用张量核进行加速推理。</li></ul><h2 id="85e2" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">服务容器</h2><ul class=""><li id="2b4c" class="lc ld iq kf b kg mo kk mp ko mu ks mv kw mw la lh li lj lk bi translated">该容器启动一个Kubernetes服务，然后部署在<a class="ae kc" href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrtserver" rel="noopener ugc nofollow" target="_blank">NGC tensort推理服务器容器</a> (19.03-py3)内运行的tensort推理服务器，以服务于训练好的模型。</li></ul><h2 id="bd4a" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">Web UI容器</h2><ul class=""><li id="4b54" class="lc ld iq kf b kg mo kk mp ko mu ks mv kw mw la lh li lj lk bi translated">该容器还启动了一个Kubernetes服务，该服务部署了一个Web UI来对模型进行交互式评估。</li></ul><h2 id="8bfb" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">从这里你能去哪里？</h2><p id="98fc" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">本文研究了GPU加速的Kubeflow管道，以及它的采用如何极大地提高数据科学工作流的模块化和性能。现在，您可以在本地或云上快速轻松地在计算机视觉、NLP或推荐中构建和部署强大的模型。</p><h2 id="4bd3" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">参考</h2><p id="108a" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">[1] <a class="ae kc" href="https://www.kubeflow.org/docs/" rel="noopener ugc nofollow" target="_blank"> Kubeflow文档</a></p><p id="a761" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <a class="ae kc" href="https://github.com/kubeflow/pipelines/tree/master/samples/nvidia-resnet" rel="noopener ugc nofollow" target="_blank"> NVIDIA Resnet Kubeflow管道</a></p><p id="bb1b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <a class="ae kc" href="https://ngc.nvidia.com/catalog/landing" rel="noopener ugc nofollow" target="_blank"> NVIDIA GPU云</a></p><p id="a311" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] TensorRT <a class="ae kc" href="https://devblogs.nvidia.com/speed-up-inference-tensorrt/" rel="noopener ugc nofollow" target="_blank">【博客】</a> <a class="ae kc" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html" rel="noopener ugc nofollow" target="_blank">【文档】</a></p><p id="1363" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] TensorRT推理服务器<a class="ae kc" href="https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/" rel="noopener ugc nofollow" target="_blank">【博客】</a> <a class="ae kc" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/" rel="noopener ugc nofollow" target="_blank">【文档】</a></p><h2 id="321d" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated">作者</h2><p id="3aea" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">NVIDIA解决方案架构师Ananth Sankarasubramanian</p><p id="4b68" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NVIDIA解决方案架构师Khoa Ho</p></div></div>    
</body>
</html>