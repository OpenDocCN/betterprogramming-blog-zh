# 我们能消灭“人工智能”这个术语吗？

> 原文：<https://betterprogramming.pub/kill-artificial-intelligence-7bc02f85ea70>

## 我们正在加深数据科学的可信度危机

![](img/80529dee3ecd202c02bb1b32105b7c79.png)

```
1\. [Background](#48a6)
2\. [History of AI](#da38)
3\. [Limits of ML](#21c0)
4\. [Next Steps](#b751)
5\. [Killing AI](#b898)
6\. [Closing Thoughts](#b13b)
```

**TL；博士:**数据科学家有责任将术语*人工智能*赶出这个世界。我们需要表现出成熟，收回这个被误用的短语所蕴含的几乎无法实现的承诺。

# 背景

"那个*机器学习的发音真的很奇怪，*朋友."

这是我对一位数据科学家同事在一次关于我们如何发展业务的周中电话中使用的词语*人工智能*的回应。

他停顿了一下，意识到刻薄的打断不是有效的教学工具，我深吸一口气，解释道:“我们说*机器学习*，因为它更准确地描述了**通过向**提供训练示例来教授数学模型**以产生洞察力的过程。**

随着我对数据科学理解的加深，我越来越发现*人工智能*这个词闪烁着虚假的承诺:

*   该术语误导决策者在他们的组织达到足够的数据成熟度之前投资高级分析。毕竟*智能*应该可以应付一点数据复杂度吧？
*   这让很多年轻学生感到困惑，如果他们明白这是通往数据科学领域有前途职业的道路，他们可能会在他们的**统计课**上多花点心思。
*   它分散了人们对数据科学中久经考验的方法的注意力。

我准备好干掉人工智能了。

# 人工智能的历史

这门学科始于 1955 年，目标是创造能够模仿人类认知功能的机器。学习、解决问题、运用常识、在不确定的条件下操作——这些特质合在一起，构成了*普通智能的基础，也是*人工智能的长期目标。

自成立以来，人工智能研究经历了繁荣和萧条的周期，乐观情绪高涨，随之而来的是资金的崩溃。这些挫折如此引人注目，在这个领域如此普遍，以至于它们有了自己的新词: *AI winter。最引人注目的两个冬天分别发生在 70 年代中后期和 80 年代中期到 90 年代中期。未能恰当地管理宣传是这种不幸的反模式的常见原因。*

用战略人工智能实验室首席数据科学家 T. Scott Clendaniel 的话说:

> “我真的很担心我们将进入第三个人工智能冬天……因为我认为这个领域已经被过度宣传了，它不可能达到预期。”

最近，机器学习社区一直在回应许多关于人工智能的新闻调查。2020 年 5 月， [OpenAI 发布了他们用于自然语言处理的 GPT-3 模型](https://openai.com/blog/openai-api/)。最初训练模型的成本是[460 万美元，需要 355 GPU 年的计算能力](https://lambdalabs.com/blog/demystifying-gpt-3/)。截至目前，OpenAI 已经通过一个受控的 API 访问点发布了该模型，而不是向研究人员免费提供代码。

暂且把不切实际和难以接近的担忧放在一边——GPT 3 号已经创造了[一些令人印象深刻的壮举](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/)。然而，重要的是，这种被夸大的发展并没有让我们更接近人工智能。

如果推进 AGI 的研究类似于发射宇宙飞船探索火星，那么 GPT 3 号的开发或多或少类似于向一枚火箭投资 460 万美元，该火箭在不离开发射台的情况下产生了美丽的废气云。

# 机器学习的局限性

研究界的普遍共识是，AGI 不会通过深化机器学习技术来实现。

![](img/59f01b1f1e63739a7f69f5e87aa04888.png)

创造自然智能平均花费 125，000 美元和许多不眠之夜，但它最终非常令人满意。照片由 [Shirota Yuri](https://unsplash.com/@itshoobastank?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄。

[机器学习能力狭窄](https://www.youtube.com/watch?v=mSTCzNgDJy4&feature=youtu.be&t=171)。一个人工智能算法也许能够实现比人类更好的表现，但是只能在非常特殊的任务上，而且必须经过非常昂贵的训练。

人的能力是相对广泛的。我们似乎特别擅长*一次性学习*——根据很少的例子进行推理和分类。幼儿很快掌握匹配、分类、比较和排序的任务。婴儿天生渴望探索新奇事物，并对更广阔的世界做出结论。

这些能力是机器智能无法比拟的。事实上，机器学习可以打败围棋大师，但机器人却不能在分类积木方面打败一个蹒跚学步的孩子，这有点自相矛盾。

遵循改进的机器学习技术的道路似乎不太可能导致达到人类水平的常识推理或多功能问题解决。引用机器学习先驱斯图尔特·罗素的话:

> “我不认为深度学习会演变成 AGI。人工通用智能不会仅仅通过更大的深度学习网络和更多的数据来实现……**深度学习系统什么都不知道**，它们不能推理，也不能积累知识，它们不能将它们在一个上下文中所学的知识应用到另一个上下文中来解决问题，等等。这些只是人类一直在做的基本事情。”

换句话说，基于统计的解决方案在插值方面相当出色——也就是说，对落入他们已经看到的数据范围内的新例子得出结论。他们不太擅长推断——也就是说，利用他们所学的知识对更广阔的世界做出结论。

有可能“[AI 做过的最大的恶作剧是让世界相信它的存在](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/)”

# 后续步骤

我认为，对于机器学习研究人员来说，问问自己是否正在解决正确的挑战是极其重要的。

这里有一个关于 [StarGAN v2](https://arxiv.org/abs/1912.01865) 的 YouTube 视频，这是一个机器学习模型，它可以拍摄一张猫的照片，并用它来创建一堆受狗的照片启发的类似图像。同时，根据 IBM 的分析，数据质量问题每年花费美国组织 3.1 万亿美元[。](https://hbr.org/2016/09/bad-data-costs-the-u-s-3-trillion-per-year)

也许从我们的词典中删除人工智能将有助于揭穿人工智能工具可以解决实质性数据管理失败的概念。不幸的是，低质量的数据无处不在，它会损害业务功能，甚至阻碍最简单的高级分析工具的实施。

# 杀死人工智能

如果你是一名数据科学家或机器学习工程师，我希望你从这篇文章中获得的是一种责任感，不再助长围绕人工智能的炒作。除非你处于人工智能研究的绝对前沿，否则你对这个术语的使用应该保留到讨论[超级智能控制问题](http://globalprioritiesproject.org/2015/10/three-areas-of-research-on-the-superintelligence-control-problem/)和其他面向未来的考虑中。

如果你是一个非技术人员，你可以安全地用*非常非常先进的统计学来代替*人工智能*的几乎所有用途。* 对于当今市场上的任何工具来说，都是如此。围绕人工智能的问题仍有哲学讨论的空间，但这项技术还有很长的路要走。

如果你碰巧是一名商业领袖，尤其是一名头衔中带有人工智能的商业领袖，[下面是美国 GSA 人工智能实施总监 Anil Chaudhry](https://www.youtube.com/watch?v=knGTORm5s5I&feature=youtu.be&t=1245) 对他的角色的看法:

> “我向人们描述人工智能是增强智能，而不是人工智能。”

就连头衔中带有人工智能的领导人也对人工智能敬而远之。

总之，人工智能的未来愿景不会通过当代的方法来实现。围绕庞大、不切实际的模型(如 GPT-3)的炒作揭示了对机器智能现状的缺乏了解——或者说缺乏了解。

人工智能的过度使用不仅仅是异想天开的夸张——它对数据科学界造成了损害，并有可能使该领域陷入信任危机。

# 结束语

以下是我认为未来三到五年数据科学的三个趋势。

## **我们领域所用语言的管理**

很明显，我强烈地感觉到有必要将艾从词典中删除。

## **更加依赖以人为中心的设计来识别与机器学习不当相关的风险**

不是所有的错误都应该同等对待。

再次引用斯图尔特·罗素的话:

> “有些种类的错误相对便宜。然而[把一个人归类为大猩猩，就像谷歌发现的](https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/#35d674b7713d)一样，真的很昂贵，就像在**数十亿美元的诋毁你的商誉和全球声誉。**我敢肯定这是一种无害的错误，来自于**使用统一的损失函数**

从设计过程的一开始就认识到这个潜在错误的严重性，可能会让谷歌的 ML 工程团队省去很多麻烦。将以人为中心的设计纳入模型创建过程，不仅有助于选择最佳损失函数，还有助于识别潜在的偏差来源(例如，种族不平衡的训练数据)和其他风险。

## **回归基础，包括重新关注理解数据生成的端到端流程**

我很乐意看到数据科学家开发跨越数据生成管道的技能。

再次引用哈佛大学教授 Gary King 的话:

> “你必须知道数据产生的整个数据生成过程……我们总是关注整个证据链……毕竟，**我们在研究世界——我们不是在研究一些数据**。”

这就是为什么我认为让数据科学家熟悉端到端数据策略的[原则至关重要。](https://towardsdatascience.com/best-data-science-certification-4f221ac3dbe3)