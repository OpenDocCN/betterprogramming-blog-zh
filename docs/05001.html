<html>
<head>
<title>Lessons From Processing 300 Million Messages a Day</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每天处理3亿封邮件的经验</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/lessons-from-processing-300-million-messages-a-day-5ded130ea1b4?source=collection_archive---------0-----------------------#2020-05-29">https://betterprogramming.pub/lessons-from-processing-300-million-messages-a-day-5ded130ea1b4?source=collection_archive---------0-----------------------#2020-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d878" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">缩放AWS胶水时的常见问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4bb42659aa1548e7fc9a78347bbe791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMnhdL0IjHR1sEqLr2r81Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/s/photos/speed?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@chuttersnap?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> chuttersnap </a>拍摄</p></figure><p id="e306" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几个月前，我们面临一项挑战:处理来自数百个不同来源的海量数据。几个星期后，我们让我们的软件投入生产。这对我们来说是一个重要的时刻，但这也是一个反思的机会。</p><p id="49f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我意识到有些问题拖了我们的后腿。它们的范围从架构决策到AWS Glue的隐藏实现细节。浏览大量文档以找到一个句子或配置参数是很昂贵的，所以我想我会把一些最大的问题放在一篇文章中，并给出一些关于如何解决它们的建议。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3a93" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">什么是AWS胶水？</h1><p id="85bd" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">AWS Glue是亚马逊的大数据ETL产品。它在一个无服务器的包装器中提供了大量的计算能力和对底层Spark集群的访问。它使我们能够迅速解决我们的问题。然而，你会发现，这并不是一帆风顺的，我们遇到了胶水系统的许多怪癖。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4994" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">胶水不是开箱即用的S3优化产品</h1><p id="c82b" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">AWS Glue让您可以立即获得大量并行处理能力。您可以转换数据，并使用生成的AWS Glue脚本将数据推送到S3。从表面上看，这似乎实现了它所承诺的，但没有什么是免费的——我不是在说你可以很容易地花1000多美元在一份糟糕的工作上。</p><p id="4323" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会在失败的作业日志中看到以下错误:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c4ed" class="ne md it na b gy nf ng l nh ni">... Slow Down (Service: Amazon S3; Status Code: 503; Error Code: 503 Slow Down; ...</span></pre><p id="2606" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表示您的作业正试图以超过其允许限制的速率写入S3。调整您的工作以优化S3存储将是您必须克服的首要障碍之一。不仅仅是因为工作可能会失败。如果你不小心，S3成本将迅速成为你最大的开支。幸运的是，你可以采取一些措施。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="0ed1" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">合并/重新划分</h1><p id="35a7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">大多数S3的速度变慢是由于你试图写的文件的数量。写大文件比写许多小文件更有效。一种分割文件的机制叫做spark分区。单个数据帧可以被分解成多个分区，这很有用，因为它们<a class="ae ky" href="https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297" rel="noopener ugc nofollow" target="_blank">提高了节点上的并发处理</a>。</p><p id="72dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">唉，当你试图写到S3时，这些分区中的每一个都将被写成一个文件。如果您有许多节点在运行，那么您可能会编写比您需要的多得多的文件。那么，如何减少分区数量呢？你有两个选择。</p><p id="b81c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重新分区— </strong>重新分区是一种更改数据集中分区数量的机制。它可以用来增加或减少分区的数量。</p><p id="04ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Coalesce — </strong> Coalesce只能用于减少分区数量。在这个用例中，它非常适合。</p><p id="95b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">调用<code class="fe nj nk nl na b">coalesce(1)</code>并将输出减少到一个文件可能很有诱惑力。这对于大型数据集不太可能奏效，因为您指示Spark做的是将所有数据加载到一个节点中。你的节点只有这么多内存，如果你的数据集超过这个限制，你就卡住了。然而，还有一些其他的选择。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a348" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">分组文件:分区</h1><p id="f4ce" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如果你不能简单地将数据重组到单个节点上，你需要告诉Spark和AWS Glue它应该如何<a class="ae ky" href="https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html" rel="noopener ugc nofollow" target="_blank">读取你的文件</a>。如果您直接阅读S3的文章，下面的数据源代码就是一个例子:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7d8b" class="ne md it na b gy nf ng l nh ni">df = glueContext.create_dynamic_frame_from_options("s3", {'paths': ["s3://s3path/"], 'recurse':True, <strong class="na iu">'groupFiles': 'inPartition'</strong>, 'groupSize': '1048576'}, format="json")</span></pre><p id="b59d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果你是从一张胶粘桌子上阅读，一切都不会丢失。您需要将其作为表属性添加，而不是在作业中指定。table属性遵循相同的约定。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/ec1d46f1b3d7c7c0d74093ce05bfe091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMQe4iGAL6vdu5rNhnQ6GA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将此表属性添加到有问题的表中</p></figure><p id="afcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，您需要导航到AWS Glue控制台并选择您感兴趣的表。在该表中，您可以添加属性。将名为<code class="fe nj nk nl na b">groupFiles</code>和值为<code class="fe nj nk nl na b">inPartition</code>的属性添加到您的表中。</p><p id="d9cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>如果您的作业读取超过50，000个文件，则会自动启用此功能，但是AWS文档并不清楚这是否仅适用于您的作业直接从S3或通过胶表读取的情况。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="2edb" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">G.1X实例类型</h1><p id="955a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如果你还在挣扎，你应该意识到还有一个变量。输出到S3的文件大小受并行执行器数量的影响(假设您不执行<code class="fe nj nk nl na b">coalesce/repartition</code>命令)。一种选择是使用<a class="ae ky" href="https://docs.aws.amazon.com/glue/latest/dg/add-job.html" rel="noopener ugc nofollow" target="_blank"> G.1X实例类型</a>来减少执行程序的数量。您保留了相同数量的内核和更多的内存，但是限制了并行性。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3896" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">火花作业的启动时间很慢</h1><p id="7b63" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">当使用Glue时，我们必须处理的最昂贵的问题之一是缓慢的迭代。人们很容易认为花在ec2和S3读/写上的400美元是大部分成本，但事实并非如此。最初几周，最大的成本是你的时间。</p><p id="020d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark作业大约需要10分钟启动，这意味着在部署代码和发现代码是否损坏之间至少有10分钟的等待时间。更糟糕的是，如果您正在处理大量数据，您可能需要等待几个小时才能知道您的解决方案是否可行。虽然AWS已经开始着手解决Spark jobs的效率问题，但是有一个现有的工具可以帮助你加速开发者的迭代。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="19c0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">利用开发端点</h1><p id="bb32" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">开发端点是<a class="ae ky" href="https://www.datanami.com/2016/05/04/rise-data-science-notebooks/" rel="noopener ugc nofollow" target="_blank">笔记本</a>，托管在SageMaker中，已经配备了AWS胶水库。它们也需要十分钟才能启动，但是一旦它们开始运行，你就可以随心所欲地运行了。如果您要在工作中测试逻辑，笔记本是最好的环境。</p><p id="0d77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>开发端点可能会变得昂贵。你可以用大量的CPU和内存来提高它们的速度。除非你删除它们，否则它们是在浪费你的钱。确保在一天结束时关闭它们。在我的团队中，这导致了下面的松弛提醒:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/0f0b7595b01b19b48a66c3457ca459c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5MyxJG8qE3lVbDy1X7bKA.png"/></div></div></figure><p id="66fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Dev端点听起来很棒，对吧？他们有你需要的一切！当然，没有什么是完美的…</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ec7d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">开发端点不是类似工作的环境</h1><p id="5e38" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">当你开始使用开发端点时，它们看起来很棒，但是要注意:有很多工作在幕后做的事情是你不能在开发端点中复制的。例如，在将文件移动到目标文件夹之前，作业会将文件写出到临时位置。运行在开发端点中的相同代码不会这样做。如果您的S3权限太受限制，它们可能在开发端点中工作得很好，而在作业中则不行。</p><p id="d8da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们很早就发现的事情，尽管我们仍然广泛使用Dev端点，但它告诉我们预期的限制。不要认为开发端点是你的黄金通行证。这表明您的逻辑是正确的，但是您的权限和基础结构可能完全错误。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3890" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">工作流程</h1><p id="3cbb" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">工作流是将各种粘合任务链接在一起的绝妙机制。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/6d03df47bbefb6d43cb0868e3ee5a1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d-vYyK4QCKifW0U4.png"/></div></div></figure><p id="1ef9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了将一堆不同的粘合工作结合在一起的简单功能之外，生成的图还可以作为一个活的架构图，用于粘合工作中正在进行的数据转换。我们选择了一个具有多个平行链的工作流程，但我们仍在修改它，我们可能会选择将其拆分。我们在这里面临的挑战是如何将我们的基础设施作为代码来管理。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="85ea" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">单独声明工作流</h1><p id="f9e3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如果您使用任何基础设施作为代码，比如Terraform或CloudFormation，我强烈建议在不同于核心组件的模块中声明工作流。您会发现需要对组件进行分组，以使代码更易于管理。例如，数据类型(客户信息、产品信息等。)或数据质量(登陆、处理、管理等)。您最终将得到相似基础设施的模块，它们整齐地捆绑在一起。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/cb3cd055598a8e3b45c8a264aff7a12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*U1PVteeNRomBos3WbTyR-A.png"/></div></figure><p id="819d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">典型的粘合工作流不仅仅涉及一种类型的数据。它涉及许多不同的种类，你对代码的分类可能不符合你的工作流程的界限。那会适得其反。有时，你需要了解客户<em class="nq">和</em>订单，这是一个合理的用例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ebe170dd72d0963acc0d432d6ee20504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*CHw5s1j5yevmcQPCoo4PpQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">您的模块通过您的工作流紧密耦合</p></figure><p id="552c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果您想对您的订单数据模块进行更改，那么您必须先找出对产品数据模块和用户数据模块的影响。这是一个众所周知的代号气味叫做<a class="ae ky" href="https://blog.ndepend.com/shotgun-surgery/" rel="noopener ugc nofollow" target="_blank">霰弹枪手术</a>。因此，不要让所有的模块都相互熟悉，而是添加一个包含工作流的缓冲区。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/94c6c3d08eadb337f4735996413ec11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDL7y6Y_3_fGvBcY1X0Tdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">现在，一个变更最多影响一个额外的模块。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3e6b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">错误信息并不完全清楚</h1><p id="b5a3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们在我们的一个Spark任务中添加了一个<code class="fe nj nk nl na b">Map</code>操作。该操作将解析给定字段中的数据，并在不同的字段中设置稍微调整过的日期。当我们运行它时，作业失败，并出现以下错误:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0e9b" class="ne md it na b gy nf ng l nh ni">Datasource does not support writing empty or nested empty schemas.</span></pre><p id="bbde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这让我们抓耳挠腮。这个作业运行了几分钟，所以它有一些输入数据——模式怎么会是空的呢？</p><p id="c2bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，如果AWS粘合作业中的<code class="fe nj nk nl na b">Map</code>操作有错误，它会将该行标记为错误，并有效地将其从集合中删除。错误不会传播。因此，第一次出现问题是当它试图不写任何东西给S3时，这时就会发生错误。幸运的是，这里有一些选择。</p><h2 id="192d" class="ne md it bd me ns nt dn mi nu nv dp mm li nw nx mo lm ny nz mq lq oa ob ms oc bi translated">Spark UI日志</h2><p id="add4" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">Spark UI是一个极好的工具，可以可视化和分解给定作业运行的执行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/9e35c1a54cbbffc358046da991e7cc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmZLggFybbBBECY-w5H38w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">呈现Spark UI日志可以很好地分解作业运行</p></figure><p id="934d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在典型的错误消息失败的地方，Spark UI日志提供了一些关于正在发生的事情的深刻见解。和往常一样，这里有一些问题:</p><ul class=""><li id="c8a3" class="oe of it lb b lc ld lf lg li og lm oh lq oi lu oj ok ol om bi translated">Spark UI日志包含一个链接，允许您查看给定执行器的日志。这些不会起作用，因为它们直接链接到ec2本身，而ec2已经不存在了。</li><li id="f313" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">默认情况下不会写入这些日志。您需要在作业配置中启用Spark UI日志。你可以在点击AWS Glue UI上的<code class="fe nj nk nl na b">Run Job</code>后完成这项工作。</li><li id="f743" class="oe of it lb b lc on lf oo li op lm oq lq or lu oj ok ol om bi translated">你需要下载<a class="ae ky" href="https://spark.apache.org/docs/latest/monitoring.html" rel="noopener ugc nofollow" target="_blank"> Spark历史服务器</a>来查看日志。AWS不会为您渲染它们。</li></ul><h2 id="5e40" class="ne md it bd me ns nt dn mi nu nv dp mm li nw nx mo lm ny nz mq lq oa ob ms oc bi translated">连续测井</h2><p id="23e8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">连续日志记录是AWS Glue提供的一个特性，它将在作业运行时近乎实时地传输日志。不要等到最后才发现是否有失败，你可以在发生时看到结果。如果问题发生在作业运行开始时，但您的作业又过了15分钟才停止，这将非常有用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/76e8feaf7e22543fbb5a52b1fba52cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGKvUzygz289d6XDjUomcA.png"/></div></div></figure><h2 id="b905" class="ne md it bd me ns nt dn mi nu nv dp mm li nw nx mo lm ny nz mq lq oa ob ms oc bi translated">工作指标</h2><p id="0955" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">最后，作业指标是简单的Cloudwatch指标，为您提供关于给定作业运行的大量信息。例如，给定执行器上使用的JVM堆大小，或者驱动程序上消耗的内存量。当您对内存问题进行故障诊断时，这些指标尤其有用。它们可以在您运行作业时启用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="0847" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">判决</h1><p id="722a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">雅典娜很棒。AWS胶水很棒。太棒了，伙计们！</p><p id="9116" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，这有点痛苦，有时感觉像一场艰苦的战斗。没有什么值得做的事情是容易的，最近我自己经历了这段旅程，我认为它值得坚持下去。上面的列表是我们发现的怪癖的一个小子集，然而通过结合精心编写的文档、StackOverflow帖子和某种疯狂的决心，我们为大数据转换的黑暗、奇异的世界带来了一些光明！</p></div></div>    
</body>
</html>