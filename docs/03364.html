<html>
<head>
<title>Assessing Audio Quality with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用深度学习评估音频质量</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/assessing-audio-quality-with-deep-learning-f66d1761f938?source=collection_archive---------6-----------------------#2020-02-05">https://betterprogramming.pub/assessing-audio-quality-with-deep-learning-f66d1761f938?source=collection_archive---------6-----------------------#2020-02-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3c23" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用TensorFlow 2.0训练深度学习系统来估计平均意见得分(MOS)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/be79ed9844b180f81a8351b42c68170d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-ZLbke44nOR_a_UyPEUkg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@innov8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马蒂厄A </a>在<a class="ae ky" href="https://unsplash.com/s/photos/audio-quality?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="ec8d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="97f2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果您曾经使用过Skype或Hangouts等VoIP(IP语音)应用程序，您就会知道音频质量下降可能是一个问题。在视频或音频会议中，也许是与客户和潜在客户，音频质量很重要。</p><p id="f4eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">“语音质量”听起来像是一个主观的概念。但是有一些众所周知的影响语言清晰度的退化类型。我说的可懂度是指语音可以有多“愉悦”。降低可懂度的一些退化包括回声、混响和背景噪声(通常来自你的同事)。</p><p id="c754" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">评估音频信号质量的一种常用度量是平均意见得分(MOS)。MOS是不同用户给出的个人评分的算术平均值。我们稍后将详细讨论MOS，但如果您以前使用过Skype，您就会知道我在说什么。</p><p id="fbc9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些VoIP服务中的大多数使用类似的策略从用户那里获得mo。当VoIP通话结束时，该工具会要求用户对他们的通话体验进行评分。大多数情况下，用户报告的满意度从1(差)到5(优秀)不等。一旦统计上显著数量的人对给定的音频样本进行了评级，MOS就是所有评级的平均值。</p><p id="bd74" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我们提出了一个深度卷积神经网络(<code class="fe ms mt mu mv b">ConvNet</code>)来解决MOS估计的问题。此外，我们更进一步，训练一个多类<code class="fe ms mt mu mv b">ConvNet</code>来估计MOS，并对给定输入的退化类型进行分类。我们的代码是用TensorFlow 2.0编写的，可以从我们的<a class="ae ky" href="https://github.com/daitan-innovation/cnn-audio-mos-estimator" rel="noopener ugc nofollow" target="_blank"> GitHub页面</a>获得。</p></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="8752" class="kz la it bd lb lc nd le lf lg ne li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">问题是</h1><p id="f865" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将MOS估计问题公式化为一个回归任务。换句话说，给定一组表示音频样本的特征，我们希望预测1到5范围内的真实值(标准MOS范围)。要记住的一件重要事情是，MOS应该衡量用户的体验质量。然而，正如我们将要看到的，音频信号远非稳定。因此，在Skype征求用户反馈的例子中，一个MOS值可能代表不同的情况。</p><p id="f759" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了更清楚地看到这个问题，想象一段来自VoIP呼叫的简短语音，总MOS为3.9。尽管这是一个相当不错的分数，但有不止一种情况可以解释这一最终评级。一个简单明了的例子是一个从头到尾质量都很好的呼叫。在这种情况下，MOS在平均值3.9附近振荡，没有明显的异常值，即较小的标准差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/ca282a30d62228da005fd4362f11dd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*HIcPVIzFVL0AFfqN_2TBfg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给定音频样本的主观评分为3.9。整个3.9 MOS描述了一个从头到尾质量都很好的音频样本。</p></figure><p id="736a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，问题来了，因为算术平均值对异常值非常敏感，所以3.9 MOS也可以用下图来解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d01e2e4d2c454facf548c1691cfc92c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*tZXwQUeGbGMxQi885424jw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给定音频样本的主观评分为3.9。整体3.9 MOS描述了大多数时间具有优秀(高于3.9) MOS的音频样本。质量的突然下降会拉低MOS。</p></figure><p id="dcb2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这种情况下，MOS在大部分时间都很高(高于平均值3.9)。然而，在通话即将结束时，清晰度突然下降，这使得用户对最终体验的评价低于预期。这强调了采用不同策略来更好地测量音频质量的必要性。换句话说，为了获得可靠的音频质量测量，我们需要减轻算术平均值的弱点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/673a0b58046c260f279dbdb26b768d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*bU3YJXNAUAZoQ-m8q2ri8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将音频信号分割成小部分并估计每个单独部分的MOS，给出了对音频质量的整体更好的测量。</p></figure><p id="a57b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要做到这一点，我们可以通过将音频信号分割成小的间隔来测量MOS。对于每个间隔，我们估计一个单独的(本地)MOS。这样，我们对音频样本进行的切片越多，MOS的分布就越高。这反过来提供了更好的最终MOS估计。在上面的例子中，为了得到样本音频的最终MOS，我们只需计算所有五个估计值的平均值。举例来说，<strong class="lt iu">就是3.216 </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/66ad5aa64e2197cbc87eccbcd6e308d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*rBKWNc8EQur1cc7sWjRewA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">切片越多，整体质量衡量越好。</p></figure><p id="aecd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">同样，分数分布越密集，总体MOS就越有代表性。对于上面的例子，<strong class="lt iu">就是3.17 </strong>。</p></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="4e18" class="kz la it bd lb lc nd le lf lg ne li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">数据集</h1><p id="f46f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">很难找到用于MOS估计的人类注释音频数据库。这里，为了简单起见，我们选择了<a class="ae ky" href="http://www.mee.tcd.ie/~sigmedia/Resources/TCD-VoIP" rel="noopener ugc nofollow" target="_blank"> TCD-VoIP数据集</a>。</p><p id="278b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该数据集旨在帮助开发和测试语音质量VoIP系统。它包含一组五种类型的VoIP降级及其相应的主观意见得分(MOS)。该数据集集中于独立于硬件或网络发生退化，并可免费获得。</p><p id="9159" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TCD-VoIP涵盖了VoIP应用中常见的五种降级。这些是:</p><ul class=""><li id="b4cb" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated">背景噪声</li><li id="e02d" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">明白易懂的竞争发言人</li><li id="2ec8" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">回声效果</li><li id="2a79" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">振幅削波</li><li id="c195" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">断断续续的讲话</li></ul><p id="cdec" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于每个音频样本，有来自24个不同听众的个人主观意见得分。同样，最终主观得分(MOS)是24个得分的算术平均值。总共有384个音频文件，两个男性和两个女性扬声器。你可以在下面的图片中看到语言退化和MOS的分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/7eb1812c2f208a5fba74c183eb93530f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f29Tu4Rqlf1JfPliCSV5FQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/6baff826744e270a0295ebbdfb5e7d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Oz0b7r8in92xQt50gzkPw.png"/></div></div></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="3535" class="kz la it bd lb lc nd le lf lg ne li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">特征抽出</h1><p id="f0b1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了从音频样本中提取特征，我们实验了用于音频处理的最流行的表示。具体来说，我们使用短时傅立叶变换(STFT)将音频信号编码成四种不同的特征表示。</p><ul class=""><li id="d48c" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated">STFT的光谱星等</li><li id="38b0" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">STFT的声谱图</li><li id="5335" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">梅尔光谱图</li><li id="b4cf" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">常数Q变换</li><li id="aaad" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">梅尔频率倒谱系数</li></ul><p id="59be" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了简明起见，这里我们只描述STFT特征的大小。事实上，连同STFT 的<strong class="lt iu">声谱图，幅度向量是我们实验中最有效的表示。</strong></p><p id="7f85" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">STFT是音频信号最常见的时频表示。这个想法是在输入信号的一小部分上计算傅立叶变换。由于音频样本是高度不稳定的(主要是音乐信号)，STFT将信号分成更小的部分，作为提供更鲁棒的最终表示的一种方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3fcebbf81a6eeef6622473f1af3f9971.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*8krH2H2ay2IpFstm2C-nng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">STFT的星等矢量。</p></figure><p id="e397" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于STFT变换，我们使用覆盖输入音频信号的512个样本点的汉明窗口。窗口以64点的步幅(跳跃大小)移动，这保证了75%的重叠。最后，我们取STFT的大小，并将其用作最终的特征向量。顺便提一下，要计算STFT的声谱图，我们只需计算STFT的平方。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3461cde15b18d8931c8188a3ac688f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*yfJG1FPM-MnYImyeoQmd9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算STFT的光谱图</p></figure><p id="c848" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于音频样本具有不同的长度，我们使用“环绕”模式填充STFT，使得特征向量具有相同的形状。这样<strong class="lt iu">，STFT有259个频率仓和1241个时间帧</strong>。</p></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="a2d0" class="kz la it bd lb lc nd le lf lg ne li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">解决方案</h1><p id="0cf1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最终解决方案由一个多路输出ConvNet组成，可训练参数约为58K。该架构由以下重复模块组成:</p><ul class=""><li id="320a" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积</a> → <a class="ae ky" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批量归一化</a> → <a class="ae ky" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a> → <a class="ae ky" href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer" rel="noopener ugc nofollow" target="_blank">最大汇集</a> → <a class="ae ky" href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)" rel="noopener ugc nofollow" target="_blank">空间落差</a></li></ul><p id="75bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在训练期间，该模型接收来自STFT震级谱的随机裁剪的小块作为输入。我们在第一个轴上使用Z索引标准化来标准化输入面片。这确保了跨越STFT向量的259个箱的接近0的均值和单位方差。</p><p id="44e7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">随机裁剪预处理减轻了每个输入的训练计算处理。此外，它增加了训练数据，从而减少了过度拟合。该模型接收固定大小的面片<em class="oc"> (257 x 416) </em>作为输入，并产生两个输出。</p><ul class=""><li id="3b14" class="nl nm it lt b lu mn lx mo ma nn me no mi np mm nq nr ns nt bi translated">第一个输出是作为回归任务优化的MOS估计。这里，我们使用均方误差(MSE)作为目标。该模型产生一个介于0和1之间的值，该值对应于归一化的MOS。</li><li id="137a" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">第二个输出是一组概率，用于对信号的退化类型进行分类。对于这个分类任务，目标被设置为交叉熵损失函数。这里，最后一个密集层有5个神经元(每个类一个)和<em class="oc"> softmax </em>激活。</li></ul><p id="9b8c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，我们将这两个成本结合起来产生总损失，并使用Adam优化器将其最小化。查看下面的简化模型架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/f6d6f027cd7bb01969d53ba73780b1d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhuV5pan8UGTa8BZi-jz1Q.png"/></div></div></figure><p id="6865" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了平衡每项任务对最终损失的贡献，我们将每项损失的权重与它们的优势成反比。基本上，我们运行100个时期的短实验，并存储每个时期的单个原始损失。我们接着计算一个损失相对于另一个损失的平均程度。通过这样做，我们可以测量一个损失与另一个损失相比(平均)有多大。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="6623" class="kz la it bd lb lc nd le lf lg ne li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">结果</h1><p id="5701" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">尽管TCD-VoIP数据集很小，但结果相当好，尤其是MOS估计，其平均绝对偏差仅为0.06。您可以看到下图，并比较来自测试集和相应估计值的地面真实MOS。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/7957a4b68126aa1f6af4ae148fa39cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFZHMGdfPdKlX16haC4dUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比较地面真实值和预测MOS估计值。</p></figure><p id="656f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您还可以听一些音频样本(来自测试集)，并将<strong class="lt iu">目标</strong>与<strong class="lt iu">预测的</strong>主观分数进行比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标MOS为3.5、估计得分为3.27的音频样本</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标MOS为1.8、估计得分为2.09的音频样本</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标MOS为1.8、估计得分为2.251的音频样本</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标MOS为3.1、估计得分为3.364的音频样本</p></figure><p id="6775" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面，你还可以看到一个退化任务的混淆矩阵。该模型设法将大多数测试示例分类为正确的类。然而，缺乏更多的训练数据是无法实现更高精度的关键因素。总体平衡准确率为75%，准确率和召回率分别为82%和77%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e911745b5c1fddf4c0f974edbde5c690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*W7x9qMhCAotpczrb5afFgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">VoIP常见退化分类的混淆矩阵。</p></figure></div><div class="ab cl mw mx hx my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="im in io ip iq"><h1 id="02bf" class="kz la it bd lb lc nd le lf lg ne li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">结论</h1><p id="0f62" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">即使有一个相对较小的数据集，包含五种常见的VoIP质量下降，总体结果也非常好。可悲的是，很难找到具有统计上显著的人类注释主观分数的音频数据库。</p><p id="54f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管如此，由于深度学习模型需要大量的数据才能产生良好的结果，我们不应该指望这个模型能够在野外推广所有数据。事实上，TCD-VoIP数据集只包含来自四个不同人的语音样本。这种可变性的缺乏使得模型非常局限于数据本身。然而，使用更大的数据集，如<a class="ae ky" href="http://www.vc-challenge.org/" rel="noopener ugc nofollow" target="_blank">语音转换挑战(VCC) 2018 </a>，这里给出的配方是相同的，我们可以期待更好的泛化。</p><p id="12a2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">感谢阅读。</strong></p><p id="8dda" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="oc">thal les Santos Silva和Daitan的创新团队。</em></p></div></div>    
</body>
</html>