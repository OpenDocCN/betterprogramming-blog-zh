<html>
<head>
<title>How to Use Artificial Intelligence and Twitter to Detect Fake News</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用人工智能和Twitter来检测假新闻</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-use-artificial-intelligence-and-twitter-to-detect-fake-news-a-python-tutorial-75a4132acf7f?source=collection_archive---------3-----------------------#2020-02-07">https://betterprogramming.pub/how-to-use-artificial-intelligence-and-twitter-to-detect-fake-news-a-python-tutorial-75a4132acf7f?source=collection_archive---------3-----------------------#2020-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="57c0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python和TensorFlow创建一个神经网络，该网络可以使用来自Twitter的数据对假新闻文章进行准确分类</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/becdf3f09dcb53a67c8d86a92c3c8c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Scb2B9rPFdziWa4a"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@waldemarbrandt67w?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Waldemar Brandt </a>拍照</p></figure><h1 id="23e8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">背景</strong></h1><p id="694b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">2016年大选后，“假新闻”成为大多数美国人耳熟能详的一句话。不管是好是坏，我们面临的事实是，我们依赖社交媒体来保持我们的信息，这让我们有可能被坏人操纵。</p><p id="a17d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">随着2020年大选的临近，我们将再次受到通过每一个可以想象的社交媒体媒体的政治信息的冲击。其中一些信息是真实的，旨在帮助我们做出明智的投票决定。相比之下，其他信息将被设计成让我们彼此对立，变得更加政治两极化，这一切都是为了推进那些传播它的人的隐藏议程。</p><p id="ce55" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">无论你站在政治通道的哪一边，你都应该关注假新闻已经(并将继续)对我们的民主进程产生的有害影响。政治话语应该由人类来推动，而不是机器人和旨在让我们互相仇恨的算法。</p><p id="06ea" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我相信，人工智能像任何技术或工具一样，有可能让我们受益，也有可能给我们带来巨大的伤害。由我们自己决定如何使用它。</p><p id="cfe2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我想向大家展示，针对我们发布假新闻的工具是如何帮助我们打击假新闻的。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="cd9e" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">先决条件</strong></h1><p id="18c5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">出于本文的目的，我将假设您已经知道如何用Python 3编程。有机器学习的基础知识也会有帮助。您也应该对在命令行中工作感到舒适，因为我们将使用的许多工具缺少<a class="ae kv" href="https://en.wikipedia.org/wiki/Graphical_user_interface" rel="noopener ugc nofollow" target="_blank">图形用户界面(GUI) </a>。</p><p id="6e0b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果你是编程新手，我向大家强烈推荐Chuck Severance博士的Python。如果你已经知道如何编程，但对Python或机器学习不熟悉，我建议你去密歇根大学的<a class="ae kv" href="https://www.coursera.org/specializations/data-science-python" rel="noopener ugc nofollow" target="_blank">Coursera</a>上的应用数据科学与Python专门化<a class="ae kv" href="http://coursera.org" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="3ab7" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">算法</strong></h1><p id="4929" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将用来构建假新闻检测器的算法被称为卷积神经网络或CNN。CNN通常用于图像分类任务(例如面部识别)，但我们也可以将它们用于文本分类任务，如假新闻检测！</p><p id="488e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">解释CNN如何工作超出了本文的范围，但是Adit Deshpande的<a class="ae kv" href="https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">理解卷积神经网络</em> </a> <em class="nb"> </em>的初学者指南提供了一个很好的概述<em class="nb">。那些想要更严谨一点的人可能会对斯坦福大学CS231n课程的材料感兴趣。</em></p><p id="1b68" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">免责声明:我将向您展示的CNN将<em class="nb">而不是</em> <strong class="lq ir"> <em class="nb"> </em> </strong>产生全世界有史以来最好、最先进的假新闻检测结果。有很多研究人员从事这方面的工作，比我强得多。如果你对他们的工作感兴趣，我建议阅读<a class="ae kv" href="https://arxiv.org/abs/1811.00770" rel="noopener ugc nofollow" target="_blank"> Oshikawa等人关于该主题的2018年文献综述</a>。然而，我的算法将允许你用有限的计算资源快速得到相当好的结果。我强烈建议你把它作为你自己进行深度学习和文本分类实验的起点。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="1ae2" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">数据集</strong></h1><p id="dc21" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们在这个项目中使用的数据集是舒等人在他们的论文<a class="ae kv" href="https://arxiv.org/abs/1809.01286" rel="noopener ugc nofollow" target="_blank"> <em class="nb"> FakeNewsNet:一个包含新闻内容、社会背景和时空信息的数据仓库，用于研究社交媒体上的假新闻</em> </a> <em class="nb">中描述的FakeNewsNet数据集。</em></p><p id="b2db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">FakeNewsNet结合了来自<a class="ae kv" href="https://www.politifact.com/" rel="noopener ugc nofollow" target="_blank"> Politifact </a>和<a class="ae kv" href="https://www.gossipcop.com/" rel="noopener ugc nofollow" target="_blank">gossi COP</a>的一组由人工标记的真实和虚假文章，以及来自Twitter的关于发布这些文章的用户的数据。我们可以将文章的原始数据与Twitter数据结合起来，做出比单独使用任何一个来源都更好的假新闻检测器！</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="30cf" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">第一部分:收集数据</strong></h1><p id="c2ba" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在已经聊得够多了。让我们开始编码吧！首先，你要确保你已经安装了<a class="ae kv" href="http://python.org" rel="noopener ugc nofollow" target="_blank">Python</a>(3.6或更高版本)和<a class="ae kv" href="https://git-scm.com/" rel="noopener ugc nofollow" target="_blank"> Git </a>。我们将通过启动一个终端窗口并运行以下命令来克隆<a class="ae kv" href="https://github.com/KaiDMML/FakeNewsNet" rel="noopener ugc nofollow" target="_blank"> FakeNewsNet GitHub存储库</a>开始:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="ed37" class="nh kx iq nd b gy ni nj l nk nl">git clone <a class="ae kv" href="https://github.com/KaiDMML/FakeNewsNet.git" rel="noopener ugc nofollow" target="_blank">https://github.com/KaiDMML/FakeNewsNet.git</a></span></pre><p id="f209" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，将目录切换到我们刚刚下载的FakeNewsNet文件夹，并使用pip安装所需的软件包:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="3a2d" class="nh kx iq nd b gy ni nj l nk nl">cd FakeNewsNet/<br/>pip install -r requirements.txt</span></pre><p id="4383" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这就是事情变得有点混乱的地方。由于Twitter不允许tweet数据被重新分发，我们将不得不通过创建一个<a class="ae kv" href="https://developer.twitter.com/" rel="noopener ugc nofollow" target="_blank"> Twitter开发者账户</a>并获得一个API密钥来收集Tweet数据。如果你还没有Twitter开发者账户，你必须注册并等待几天的批准。在填写申请开发者账户的理由时，一定要彻底和诚实——我听说过用户没有认真对待被拒绝的那部分注册。</p><p id="2eb1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一旦你的开发者账户被批准，登录，悬停在你的名字上，点击下拉菜单中的“应用”。然后点击蓝色的“创建应用程序”按钮。你需要填写一些关于你的应用程序的详细信息。一旦你创建了它，跳转到“密钥和令牌”页面，点击“访问令牌和访问令牌密码”下面的蓝色“生成”按钮</p><p id="5b60" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您需要将这些值与您的API密匙和API密匙粘贴到<code class="fe nm nn no nd b">code/resources/</code>下FakeNewsNet文件夹的<code class="fe nm nn no nd b">tweet_keys_file.json</code> <em class="nb"> </em>中。注意，API密钥和秘密进入<code class="fe nm nn no nd b">app_key</code>和<em class="nb"> </em> <code class="fe nm nn no nd b">app_secret</code>字段，而访问令牌和秘密进入<code class="fe nm nn no nd b">oauth_token</code>和<code class="fe nm nn no nd b">oauth_token_secret</code>字段。<strong class="lq ir"> <em class="nb">不要</em> </strong> <em class="nb">与任何人分享任何这些值</em>或将它们上传到像GitHub这样的公共代码库。如果其他人得到了这些，他们就可以控制你的Twitter账户。</p><p id="ff34" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果您愿意，可以在<code class="fe nm nn no nd b">code/config.json</code>中更改其他设置<em class="nb">。但是对于大多数用例来说，缺省值应该没问题。返回到您的终端窗口，cd进入code文件夹，并使用以下命令运行数据收集服务器:</em></p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="9454" class="nh kx iq nd b gy ni nj l nk nl">cd code/<br/>nohup python -m resource_server.app &amp;&gt; keys_server.out&amp;<br/>nohup python main.py &amp;&gt; data_collection.out&amp;</span></pre><p id="5c86" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">不幸的是，在撰写本文时，数据收集代码还没有得到很好的优化，需要很长时间才能运行。过了一段时间，我厌倦了等待，中断了数据收集，给我留下了20，795篇文章及其下载的推文。其中5100篇是假新闻，15659篇是真新闻。您的里程可能会有所不同。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="0390" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">第二部分:数据处理</strong></h1><p id="d685" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在数据已经收集好了，我们需要执行一些处理步骤来准备将它输入我们的CNN。如果您还没有安装<code class="fe nm nn no nd b">numpy</code>和<code class="fe nm nn no nd b">pandas</code>库，请在您的终端窗口中运行以下命令来安装它们:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="a13e" class="nh kx iq nd b gy ni nj l nk nl">pip install numpy<br/>pip install pandas</span></pre><p id="6998" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在数据收集过程结束后(或者你中断了它)，你应该会留下一个名为<code class="fe nm nn no nd b">fakenewsnet_dataset</code> <em class="nb">的新文件夹。</em>这个文件夹里面应该是另外两个文件夹:<code class="fe nm nn no nd b">politifact</code> <em class="nb"> </em>和<em class="nb"> </em> <code class="fe nm nn no nd b">gossipcop</code> <em class="nb">。在<code class="fe nm nn no nd b">fakenewsnet_dataset</code></em><em class="nb"/>内创建一个新的Python文件，名为<em class="nb"> </em> <code class="fe nm nn no nd b">build_dataset.py</code> <em class="nb">。我会提供你应该放在这个文件中的代码，以及每个部分的简要说明。</em></p><p id="b2c3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先，我们需要导入<code class="fe nm nn no nd b">pandas</code>和<code class="fe nm nn no nd b">numpy</code>库，以及我们需要的其他一些Python模块:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="fe6a" class="nh kx iq nd b gy ni nj l nk nl">import json<br/>import os<br/>import numpy as np<br/>import pandas as pd</span></pre><p id="b486" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后我们创建一个函数，<code class="fe nm nn no nd b">load_data</code>，<em class="nb"> </em>，它将搜索我们下载的所有文件，提取它们的原始JSON数据，并将这些数据组合成一个Python字典列表，我们可以使用它来进行进一步的处理。这种方法确保了文章数据和正确的tweets在我们继续之前正确匹配。如果文章数据或特定文章的推文丢失，它将被跳过，不包括在我们的最终数据集中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="d4f0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">每篇文章都有许多相关的推文，每篇文章可能有不同数量的推文。因此，我们希望总结每篇文章在整个推文中的特征，如关注者数量、关注人数、发布状态数量、喜欢数量、转发数量、验证推文数量和收藏数量。为此，我们可以创建一个名为的函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="7de5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我们编写另一个函数，<code class="fe nm nn no nd b">process_example</code> <em class="nb">，</em>，它获取一篇文章，找到文章标题、文本和真假标签，将这些信息与Twitter统计数据结合起来，并将文章打包成一个简明的Python字典。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="037a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们调用我们根据收集的数据编写的函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="2dcf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我们使用pandas整合所有处理过的数据，并将其写入一个CSV文件— <code class="fe nm nn no nd b">dataset.csv</code>:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="7111" class="nh kx iq nd b gy ni nj l nk nl">df = pd.DataFrame(pf_fake)<br/>df = df.append(pf_real)<br/>df = df.append(gc_fake)<br/>df = df.append(gc_real)</span><span id="9267" class="nh kx iq nd b gy nr nj l nk nl">df.reset_index(inplace=True, drop=True)</span><span id="12db" class="nh kx iq nd b gy nr nj l nk nl">df.to_csv('dataset.csv', index=False)</span></pre><p id="3924" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据你从数据集中下载的文章数量和你机器的计算能力，这可能需要一段时间来运行。然而，一旦完成，您将在<code class="fe nm nn no nd b">dataset.csv</code>中留下所有合并的数据。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="bb04" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">第三部分:文本预处理</strong></h1><p id="0951" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本教程的其余部分，我们将严重依赖于<a class="ae kv" href="http://tensorflow.org" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>，这是一个由<a class="ae kv" href="http://google.com" rel="noopener ugc nofollow" target="_blank"> Google </a>开发的流行的开源机器学习框架。</p><p id="c6aa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">训练CNN的过程是高度计算密集型的，并且可能花费很长时间。幸运的是，TensorFlow内置了对GPU加速的支持，这使得这个过程快了很多。然而，即使您的机器上有兼容的GPU，配置TensorFlow(和CUDA)来使用它的过程也可能是痛苦的。因此，我建议在<a class="ae kv" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>中继续本教程的剩余部分，它将为您处理所有这些问题。</p><p id="1188" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先，将<code class="fe nm nn no nd b"><em class="nb"> dataset.csv</em></code> <em class="nb"> </em>上传到你的Google Drive账号。然后去<a class="ae kv" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/</a>推出新的Python 3笔记本。从“编辑”菜单中，选择“笔记本设置”。确保“运行时类型”设置为Python 3，将“硬件加速器”设置为GPU。</p><p id="69d6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在第一个代码单元中，我们将导入并配置各种Python库，我们将在本教程的剩余部分使用这些库:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="7f2c" class="nh kx iq nd b gy ni nj l nk nl">import tensorflow as tf<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import precision_score, recall_score, f1_score<br/>import matplotlib.pyplot as plt<br/>plt.style.use('ggplot')<br/>import seaborn as sns</span></pre><p id="21ed" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">通过单击单元格左侧的播放按钮来运行该单元格。然后，创建一个新的代码单元格，并粘贴以下代码以允许Colab访问您的Google Drive中的<code class="fe nm nn no nd b">dataset.csv</code> <em class="nb"> </em>。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="d431" class="nh kx iq nd b gy ni nj l nk nl">from google.colab import drive<br/>drive.mount('/content/drive')</span></pre><p id="95d8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">运行此单元格，然后单击出现在它下面的URL。授予Colab访问您的GDrive的权限，然后将您收到的访问代码复制并粘贴到Colab提供的框中，并按enter键。在一个新的单元格中，我们将数据集读入一个<code class="fe nm nn no nd b">pandas</code> <code class="fe nm nn no nd b">DataFrame</code>，并删除任何缺少值的行:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="f7fa" class="nh kx iq nd b gy ni nj l nk nl"># add any necessary folders after "My Drive" if you placed <br/># dataset.csv inside a folder</span><span id="f797" class="nh kx iq nd b gy nr nj l nk nl">df = pd.read_csv('/content/drive/My Drive/dataset.csv')<br/>df.dropna(inplace=True)</span></pre><p id="fa42" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了让TensorFlow理解我们的数据，一切都需要用数字表示。由于我们正在处理文本，这显然是有问题的。</p><p id="5514" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">幸运的是，TensorFlow有一个内置的标记器，可以将文本数据转换为数字格式，其中每个唯一的单词都由一个不同的数字表示。它还会自动去除标点符号，并将所有单词转换为小写。</p><p id="739d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这里，我们将每篇文章的正文文本转换成整数的numpy数组:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="7b91" class="nh kx iq nd b gy ni nj l nk nl">tokenizer = tf.keras.preprocessing.text.Tokenizer()<br/>tokenizer.fit_on_texts(df['text'])<br/>df['text'] = tokenizer.texts_to_sequences(df['text'])</span></pre><p id="8a0f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">不幸的是，CNN要求我们的每个输入都具有相同的维度。然而，由于每篇文章都有不同的长度，这就带来了挑战。一种解决方案是设置最大文章长度，并在小于该长度的任何文章的整数数组的末尾添加0。我们可以选择数据集中最长的文章作为最大值，但是假设<a class="ae kv" href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="noopener ugc nofollow" target="_blank"> Zipf定律</a>成立，我们知道大多数文章会相对较短，少数文章会非常长，导致长尾Zipfian(或<a class="ae kv" href="https://en.wikipedia.org/wiki/Power_law" rel="noopener ugc nofollow" target="_blank">幂定律</a>)分布。让我们画出每篇文章的长度，一探究竟！</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="b6b5" class="nh kx iq nd b gy ni nj l nk nl">X_lens = [len(x) for x in df['text'].values]<br/>X_lens = np.array(X_lens)<br/>sns.distplot(X_lens)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/391ed5b469e8af816222f891459c0b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*TSoVSxRIB-UsXS-cB7u88w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">物品长度分布的密度图</p></figure><p id="ca0f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据你在数据收集阶段下载的文章数量的不同，你的精确图可能会有所不同，但从这个图表中可以清楚地看出，Zipf定律确实适用于文章的长度。我们可以清楚地看到，绝大多数文章都在2500字以下。</p><p id="49ce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我们选择17，500(或更多)个单词作为最大文章长度，我们将拥有非常大的向量，而这些向量只对数据集的一小部分有益。此外，使用17，500维向量会使训练CNN的计算复杂度大得多。</p><p id="6239" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相反，我决定计算平均文章长度，文章长度的标准差，并使用平均值加上两个标准差作为我的最大文章长度。这里的直觉是选择一个最大长度，它不会缩短绝大多数文章，但仍然足够小，以允许有效的计算。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="4939" class="nh kx iq nd b gy ni nj l nk nl"># calculate the mean article length - mine was 622.0563602325352<br/>np.mean(X_lens)</span><span id="1533" class="nh kx iq nd b gy nr nj l nk nl"># calculate the standard deviation of article length<br/># mine was 1182.0848019657983<br/>np.std(X_lens)</span></pre><p id="cb4c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我将这些数字四舍五入到最接近的整数，得到622个单词的近似平均值和大约1182个单词的标准偏差。在近似平均值上加上两倍的近似标准差后，我得到的最大文章长度为2986字。为了验证这不会删减太多文章，我进行了以下计算:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="b8fd" class="nh kx iq nd b gy ni nj l nk nl">MAX_LENGTH = 2986<br/>np.unique((X_lens &gt;= MAX_LENGTH), return_counts=True)</span><span id="1f9b" class="nh kx iq nd b gy nr nj l nk nl"># for me this returns:<br/># (array([False,  True]), array([19689,   609]))</span></pre><p id="af5c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里的返回值告诉我，我的文章中有19689篇小于等于2986字，609篇大于2986字。通过执行以下计算:19，689/(19，689+609)我知道我的文章中有超过96.9997%的文章少于或正好是2986个单词。对我来说，这个数字是可以接受的。但是如果少于95%的文章小于或等于你的最大长度，你可能希望考虑使用更大的最大长度。</p><p id="70e1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我对文章标题进行了标记化处理，使用了与处理文章正文相同的过程:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="b7a2" class="nh kx iq nd b gy ni nj l nk nl">title_tokenizer = tf.keras.preprocessing.text.Tokenizer()<br/>title_tokenizer.fit_on_texts(df['title'])<br/>df['title'] = title_tokenizer.texts_to_sequences(df['title'])</span></pre><p id="b559" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我重复了计算最大标题长度和计算最大文章正文长度的相同过程。为了简单起见，我不会在这里显示计算，但是您可以简单地重用相同的代码，用df['title']替换df['text']。我计算的最大标题长度是19，98.0293%的文章标题小于或等于19。我将结果保存为常量:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="bbb0" class="nh kx iq nd b gy ni nj l nk nl">MAX_TITLE = 19</span></pre><p id="a147" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在设置了最大标题长度和最大文章长度之后，我们需要去掉一些不必要的特性。</p><p id="1ed3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们需要删除文章ID，因为虽然它有助于收集数据和将所有东西放在一起，但它与文章是否是假的没有任何关系。我也把文章来源信息掉了。您可能希望尝试一下，但是我发现包含源信息是没有帮助的，因为有太多的源来对它们进行有意义的编码。此外，研究表明，包括来源信息可能会引入过度的偏见。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="dc8b" class="nh kx iq nd b gy ni nj l nk nl">df.drop('source', axis=1, inplace=True)<br/>df.drop('id', axis=1, inplace=True)</span></pre><p id="1b6e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们将标签从其余数据中分离出来，用<code class="fe nm nn no nd b">0</code>替换“真实”标签，用<code class="fe nm nn no nd b">1</code>替换“虚假”标签:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="a040" class="nh kx iq nd b gy ni nj l nk nl">labels = df.pop('label')<br/>labels.replace({'fake': 1, 'real': 0}, inplace=True)</span></pre><p id="b918" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我们将数据分为训练集和测试集:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="079f" class="nh kx iq nd b gy ni nj l nk nl">X_train, X_test, y_train, y_test = train_test_split(df, labels)</span></pre><p id="4998" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们将文章标题和正文从其余数据中分离出来。他们每个人都将从推特数据中独立出来，输入CNN。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="df0c" class="nh kx iq nd b gy ni nj l nk nl">X_train_title = X_train.pop('title')<br/>X_test_title = X_test.pop('title')</span><span id="7738" class="nh kx iq nd b gy nr nj l nk nl">X_train_text = X_train.pop('text')<br/>X_test_text = X_test.pop('text')</span></pre><p id="ad00" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我们通过切掉太长标题的末端，并为太短标题的向量添加<code class="fe nm nn no nd b">0</code> s，来应用最大标题长度:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="a2c2" class="nh kx iq nd b gy ni nj l nk nl">X_train_title = tf.keras.preprocessing.sequence.pad_sequences(X_train_title,<br/>                                              maxlen=MAX_TITLE,<br/>                                              padding='post',<br/>                                              truncating='post')<br/>X_test_title = tf.keras.preprocessing.sequence.pad_sequences(X_test_title,<br/>                                              maxlen=MAX_TITLE,<br/>                                              padding='post',<br/>                                              truncating='post')</span></pre><p id="2612" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对文章正文重复相同的过程:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="e05b" class="nh kx iq nd b gy ni nj l nk nl">X_train_text = tf.keras.preprocessing.sequence.pad_sequences(X_train_text,<br/>                                              maxlen=MAX_LENGTH,<br/>                                              padding='post',<br/>                                              truncating='post')<br/>X_test_text = tf.keras.preprocessing.sequence.pad_sequences(X_test_text,<br/>                                              maxlen=MAX_LENGTH,<br/>                                              padding='post',<br/>                                              truncating='post')</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="1326" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">第四部分:模型创建与培训</strong></h1><p id="a83d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">下一步是构建和编译CNN。由于我们创建的模型不遵循线性层序列，我们将使用Keras functional API来定义它:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ccec" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在不涉及太多细节的情况下，该模型接受三个独立的输入:文章标题、文章正文和Twitter数据。文章标题和正文被传递到单独的嵌入层，每个单词被转换成一个50维向量。产生的矩阵被传递到卷积层，在那里应用256个滤波器。最后，使用全局最大池化将矩阵池化为向量，并将得到的向量与Twitter向量连接，并传递到前馈网络以产生二进制分类。</p><p id="5280" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我的最终模型的架构图如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/9d90610bd011e863a9e7465e95620a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-m-1fVEthNGp8eZkkuACg.png"/></div></div></figure><p id="803e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">模型完全编译后，我们现在必须训练它。为了确保我们在测试集上保留具有最佳精确度的模型，我们在五个时期后应用<code class="fe nm nn no nd b">Early Stopping</code>而没有精确度改进。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="06b0" class="kw kx iq bd ky kz mw lb lc ld mx lf lg jw my jx li jz mz ka lk kc na kd lm ln bi translated"><strong class="ak">第五部分:模型评估</strong></h1><p id="4b2c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一旦模型完成训练，我们需要在测试集上测试它的准确性——模型以前没有见过的例子。我们首先需要模型来输出对所有测试文章的预测:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="83d4" class="nh kx iq nd b gy ni nj l nk nl">preds = model.predict([X_test_text, X_test_title, X_test])</span></pre><p id="4e23" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一旦做出了预测，我们就可以对它们的准确性进行评分:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="5599" class="nh kx iq nd b gy ni nj l nk nl">model.evaluate([X_test_text, X_test_title, X_test], y_test)</span></pre><p id="652c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对我来说，准确率是90.84%。您的准确性可能会有所不同，这取决于您下载的特定文章以及您最终获得的特定培训/测试结果。一般来说，你应该期望看到80多或90多的准确度。</p><p id="d97a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，我们正在处理不平衡的类(假文章的数量明显少于真文章的数量)，所以准确性不能给我们完整的故事。我们应该关注的一些其他指标包括精确度、召回率和F1分数。</p><p id="39f3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Precision告诉我们有多少被模型<em class="nb">标记为</em>假的文章实际上是<em class="nb">假的。在我的模型中，分数是82.59%，但是您可以用下面的代码检查您的分数:</em></p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="5c95" class="nh kx iq nd b gy ni nj l nk nl">precision_score(y_test.values, preds.reshape(-1).astype('int64'))</span></pre><p id="4fb2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">回忆告诉我们有多少<em class="nb">实际上</em>是假的文章，被正确地<em class="nb">标记为</em>是假的。在我的模型中，这个分数是78.74%。您可以通过以下方式查看您的分数:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="52e5" class="nh kx iq nd b gy ni nj l nk nl">recall_score(y_test.values, preds.reshape(-1).astype('int64'))</span></pre><p id="7af9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们可以计算F1分数，它是精确度和召回率的调和平均值。我的是0.8061，但您可以通过以下方式获得您的:</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="e3c4" class="nh kx iq nd b gy ni nj l nk nl">f1_score(y_test.values, preds.reshape(-1).astype('int64'))</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="09ea" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这就是全部了。</p><p id="7660" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们已经成功地创建了一个模型，可以从一篇文章的文本、标题和Twitter的社交媒体上下文中对假新闻进行分类！</p><p id="1d7d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在未来的工作中，我将发布一个教程，介绍如何将该模型推广到不在原始数据集中的文本，以及如何使其对未知单词具有鲁棒性。</p></div></div>    
</body>
</html>