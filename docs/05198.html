<html>
<head>
<title>Build a SwiftUI + Core ML Emoji Hunt Game for iOS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为iOS构建一个SwiftUI + Core ML表情符号狩猎游戏</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/build-a-swiftui-core-ml-emoji-hunt-game-for-ios-eb4465ec4153?source=collection_archive---------6-----------------------#2020-06-19">https://betterprogramming.pub/build-a-swiftui-core-ml-emoji-hunt-game-for-ios-eb4465ec4153?source=collection_archive---------6-----------------------#2020-06-19</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="34c8" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">让我们创建一个有趣的机器学习iOS相机应用程序，让你在家里搜索类似表情符号的东西</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/f45d08a3bb376b5755914cb9fa5fe3b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9EtVukiTNijhgWKM"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/@vbcreative?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凡妮莎·布切里</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="89a1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">手机上机器学习的出现为一系列新机会打开了大门。虽然它允许ML专家进入移动领域，但等式的另一端实际上是出风头者。让移动应用程序开发人员涉足机器学习实际上已经使移动应用程序开发变得如此令人兴奋。</p><p id="672a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最好的事情是，你不必为了训练或运行模型而成为机器学习专家。苹果的机器学习框架Core ML提供了一个易于使用的API，让你可以在设备上运行推理(模型预测)、微调模型或者重新训练<em class="lw">。</em></p><p id="7a57" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另一方面，Create ML允许您使用拖放式macOS工具或在Swift Playgrounds中创建和训练定制的机器学习模型(目前支持图像、对象、文本、推荐系统和线性回归)。</p><p id="8b16" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如果这没有让你感到惊讶，那么考虑一下<a class="ae kz" href="https://developer.apple.com/xcode/swiftui/" rel="noopener ugc nofollow" target="_blank"> SwiftUI </a>，这款新的声明式UI框架在2019年WWDC期间向iOS社区发布时引起了一场风暴。鉴于快速构建用户界面是多么容易，仅这一点就导致了大量开发人员学习Swift和iOS dev。</p><p id="86a6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">只有SwiftUI、Core ML和<a class="ae kz" href="https://developer.apple.com/documentation/vision" rel="noopener ugc nofollow" target="_blank">Vision</a>(Core ML之前的苹果计算机视觉框架)一起才能产生基于智能人工智能的应用。但这还不是全部...你也可以利用机器学习的力量来构建有趣的游戏。</p><p id="6d11" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在接下来的几个部分中，我们将构建一个基于摄像头的iOS应用程序，让您在家中搜寻表情符号，就像寻宝一样，这必须是我们现在正在玩的流行室内游戏之一，因为我们发现自己被隔离了。</p><h1 id="8418" class="lx ly iu bd lz ma mb mc md me mf mg mh ka mi kb mj kd mk ke ml kg mm kh mn mo bi translated">行动（或活动、袭击）计划</h1><ul class=""><li id="260c" class="mp mq iu lc b ld mr lg ms lj mt ln mu lr mv lv mw mx my mz bi translated">我们将使用一个<code class="fe na nb nc nd b"><a class="ae kz" href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md" rel="noopener ugc nofollow" target="_blank">MobileNet</a></code>核心ML模型来对来自相机帧的对象进行分类。如果您想阅读更多关于MobileNet架构的内容，请阅读这篇文章<a class="ae kz" href="https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/" rel="noopener ugc nofollow" target="_blank">以获得详细的概述。</a></li><li id="ee53" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">为了设置摄像头，我们将使用AVFoundation，这是苹果自己的音频视频框架。在<code class="fe na nb nc nd b">UIViewRepresentable</code>的帮助下，我们将把它集成到我们的SwiftUI视图中。</li><li id="9223" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">我们将使用Vision框架驱动我们的核心ML模型，将模型的推理与正确的表情符号相匹配(因为每个表情符号都有一个含义)。</li><li id="4abc" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">我们的游戏将包括一个计时器，用户将相机对准给定区域周围的不同对象，以找到与表情符号匹配的对象。</li></ul></div><div class="ab cl nj nk hy nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="in io ip iq ir"><h1 id="1433" class="lx ly iu bd lz ma nq mc md me nr mg mh ka ns kb mj kd nt ke ml kg nu kh mn mo bi translated">入门指南</h1><p id="b411" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj nv ll lm ln nw lp lq lr nx lt lu lv in bi translated">启动Xcode，选择SwiftUI作为iOS应用的UI模板。接下来，转到<code class="fe na nb nc nd b">info.plist</code>文件，添加带有描述的摄像机隐私权限。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ny"><img src="../Images/6180e9e58265a152e7c43814c66873da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*N3fLz3FEV2nufO1jK-I9rA.png"/></div></figure><h1 id="7ce0" class="lx ly iu bd lz ma mb mc md me mf mg mh ka mi kb mj kd mk ke ml kg mm kh mn mo bi translated">使用AVFoundation创建自定义摄像机视图</h1><p id="02f8" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj nv ll lm ln nw lp lq lr nx lt lu lv in bi translated">SwiftUI不提供AVFoundation的原生支持。幸运的是，我们可以利用SwiftUI与UIKit的互操作性。在此之前，让我们先设置一个自定义的摄像机视图控制器。我们最终会用SwiftUI <code class="fe na nb nc nd b">struct</code>把它包起来。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nz oa l"/></div></figure><p id="b0e8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">总的来说，上面的代码做了四件事:</p><ul class=""><li id="9005" class="mp mq iu lc b ld le lg lh lj ob ln oc lr od lv mw mx my mz bi translated">创建捕获会话。</li><li id="225f" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">获取并配置必要的捕获设备。我们会用后面的摄像头。</li><li id="b863" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">使用捕获设备设置输入。</li><li id="e6d5" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">配置显示摄像机帧的输出对象。</li></ul><p id="2e20" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">此外，我们还添加了一个自定义协议:<code class="fe na nb nc nd b">EmojiFoundDelegate</code>，当找到表情符号的等效图像时，它最终会通知SwiftUI视图。这是协议的代码:</p><pre class="kk kl km kn gu oe nd of og aw oh bi"><span id="b040" class="oi ly iu nd b gz oj ok l ol om">protocol EmojiFoundDelegate{<br/>func emojiWasFound(result: Bool)<br/>}</span></pre><p id="c023" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">您还会注意到在类声明中定义的协议:<code class="fe na nb nc nd b">AVCaptureVideoDataOutputSampleBufferDelegate</code>。为了符合这一点，我们需要实现<code class="fe na nb nc nd b">captureOutput(_:didOutputSampleBuffer:from)</code>函数，其中我们可以访问提取的帧缓冲区，并将它们传递给Vision-Core ML请求。</p></div><div class="ab cl nj nk hy nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="in io ip iq ir"><h1 id="b763" class="lx ly iu bd lz ma nq mc md me nr mg mh ka ns kb mj kd nt ke ml kg nu kh mn mo bi translated">使用Vision和CoreML处理摄像机画面</h1><p id="c497" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj nv ll lm ln nw lp lq lr nx lt lu lv in bi translated">既然我们的相机已经设置好了，让我们提取帧并实时处理它们。我们将把帧传递给运行核心ML模型的Vision请求。</p><p id="42b4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在我们上面定义的<code class="fe na nb nc nd b">CameraVC</code>类中添加下面这段代码:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nz oa l"/></div></figure><ul class=""><li id="4f04" class="mp mq iu lc b ld le lg lh lj ob ln oc lr od lv mw mx my mz bi translated">我们将我们的<code class="fe na nb nc nd b">CoreML</code>模型(从这里下载<a class="ae kz" href="https://developer.apple.com/machine-learning/models/" rel="noopener ugc nofollow" target="_blank"> MobileNet版本</a>，或者您可以在本文末尾的GitHub资源库中找到它)包装在一个<code class="fe na nb nc nd b">VNCoreMLRequest</code>中。</li><li id="f3c1" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated"><code class="fe na nb nc nd b">captureOutput</code>函数将从实时摄像机帧中获取的<code class="fe na nb nc nd b">CGSampleBuffer</code>转换为<code class="fe na nb nc nd b">CVPixelBuffer</code>，最终传递给<code class="fe na nb nc nd b">updateClassification</code>函数。</li><li id="7ae6" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated"><code class="fe na nb nc nd b">VNImageRequestHandler</code>负责将输入图像转换成核心ML模型所需的约束——从而将我们从一些样板代码中解放出来。</li><li id="1c91" class="mp mq iu lc b ld ne lg nf lj ng ln nh lr ni lv mw mx my mz bi translated">在<code class="fe na nb nc nd b">processClassifications</code>函数中，我们将由核心ML模型识别的图像与<code class="fe na nb nc nd b">emojiString</code>进行比较(这是从SwiftUI主体接口传递过来的，我们很快就会看到)。一旦有匹配，我们就调用代理来更新SwiftUI视图。</li></ul><p id="dd55" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">既然困难的部分已经过去了，让我们跳到SwiftUI。</p><h1 id="6275" class="lx ly iu bd lz ma mb mc md me mf mg mh ka mi kb mj kd mk ke ml kg mm kh mn mo bi translated">构建我们的SwiftUI游戏</h1><p id="8501" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj nv ll lm ln nw lp lq lr nx lt lu lv in bi translated">我们的游戏由四种状态组成:<code class="fe na nb nc nd b">emoji found</code>、<code class="fe na nb nc nd b">not found</code>、<code class="fe na nb nc nd b">emoji search</code>和<code class="fe na nb nc nd b">game over</code>。由于SwiftUI是一个状态驱动的框架，我们将创建一个<code class="fe na nb nc nd b">@State</code> enum类型，它在上述状态之间切换，并相应地更新用户界面。下面是保存表情数据的<code class="fe na nb nc nd b">enum</code>和<code class="fe na nb nc nd b">struct</code>的代码:</p><pre class="kk kl km kn gu oe nd of og aw oh bi"><span id="e029" class="oi ly iu nd b gz oj ok l ol om">enum EmojiSearch{<br/>    case found<br/>    case notFound<br/>    case searching<br/>    case gameOver<br/>}</span><span id="edc8" class="oi ly iu nd b gz on ok l ol om">struct EmojiModel{<br/>    var emoji: String<br/>    var emojiName: String<br/>}</span></pre><p id="e7af" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在下面的代码中，我们设置了一个给定秒数的<code class="fe na nb nc nd b">Timer</code>(在我们的例子中为10秒)，在此期间，用户需要搜索一个类似表情符号的图像。根据用户是否能够做到这一点，用户界面会相应地更新:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nz oa l"/></div></figure><p id="0fd9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">调用以下两个函数来重置每一级的定时器:</p><pre class="kk kl km kn gu oe nd of og aw oh bi"><span id="8e40" class="oi ly iu nd b gz oj ok l ol om">func instantiateTimer() {</span><span id="47db" class="oi ly iu nd b gz on ok l ol om">self.timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()<br/>}</span><span id="e40f" class="oi ly iu nd b gz on ok l ol om">func cancelTimer() {<br/>  self.timer.upstream.connect().cancel()<br/>}</span></pre><p id="fa56" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，SwiftUI在使用<code class="fe na nb nc nd b">body</code>中的switch语句时并没有真正发挥出最佳效果——除非您将它们包装在一个通用参数<code class="fe na nb nc nd b">AnyView</code>中。相反，我们将switch语句放在函数<code class="fe na nb nc nd b">emojiResultText</code>中，如下所示:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nz oa l"/></div></figure><p id="49ae" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最后，我们需要为最初创建的<code class="fe na nb nc nd b">CameraVC</code>创建一个包装器结构。下面的代码实现了这一点，并通过了<code class="fe na nb nc nd b">emojiString</code>，它最终与ML模型的分类结果相匹配:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nz oa l"/></div></figure><p id="1296" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在<code class="fe na nb nc nd b">Coordinator</code>类中定义的<code class="fe na nb nc nd b">@Binding</code>属性包装器允许您从<code class="fe na nb nc nd b">CustomCameraRepresentable</code>结构更新SwiftUI状态。基本上,<code class="fe na nb nc nd b">Coordinator</code>类充当了UIKit和SwiftUI之间的桥梁——允许您通过使用委托和绑定属性包装器从一个更新另一个。</p><p id="fe0e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们看看SwiftUI游戏的一些实际输出:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oo"><img src="../Images/ab6b5ede2338ca2248c929b16387f89a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xvh6FXG2s3Z6_mkwHkVUnw.png"/></div></div></figure><p id="fbe6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这是在一堆不同对象上运行的应用程序的屏幕截图:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj op"><img src="../Images/e53b8c49d487e97f71ac1880df5ec375.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/1*OUeWRE6Ikk7EN05DR5vVaQ.gif"/></div></figure></div><div class="ab cl nj nk hy nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="in io ip iq ir"><h1 id="8a23" class="lx ly iu bd lz ma nq mc md me nr mg mh ka ns kb mj kd nt ke ml kg nu kh mn mo bi translated">结论</h1><p id="72aa" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj nv ll lm ln nw lp lq lr nx lt lu lv in bi translated">我们很快就能够使用SwiftUI、Core ML和Vision构建一个小型表情符号猎人游戏。当找到表情符号等同的图像时，您可以通过添加音频来进一步改善这种体验。此外，通过使用这个神奇的库<a class="ae kz" href="https://github.com/onmyway133/Smile" rel="noopener ugc nofollow" target="_blank"> Smile </a>，你可以快速搜索表情符号的关键字名称，反之亦然。</p><p id="01e9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">随着WWDC 2020即将到来，看看苹果如何给Core ML和SwiftUI开发者带来惊喜将会很有趣。AVFoundation与SwiftUI的简单集成以及核心ML模型层的扩展将有助于在设备上训练更多种类的ML模型。</p><p id="b509" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">例如，RNN的层，如LSTM，将为基于股票市场预测的应用开辟可能性(也许现在只是为了娱乐目的。—做投资决策时不要用它们)。这是iOS社区热切期待的。</p><p id="4c8a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">你可以从这个<a class="ae kz" href="https://github.com/anupamchugh/iowncode/tree/master/SwiftUIVisionEmojiHunt" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>下载完整的项目。</p><p id="69d1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这一次到此为止。我希望你喜欢😎</p></div></div>    
</body>
</html>