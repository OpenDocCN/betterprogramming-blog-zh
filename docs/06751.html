<html>
<head>
<title>Compressing Puppy Image Using Rank-K Approximation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用秩K逼近压缩小狗图像</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/compressing-puppy-image-using-rank-k-approximation-a-doodle-explanation-c19de5dfd951?source=collection_archive---------3-----------------------#2020-11-02">https://betterprogramming.pub/compressing-puppy-image-using-rank-k-approximation-a-doodle-explanation-c19de5dfd951?source=collection_archive---------3-----------------------#2020-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5d4b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释图像压缩、秩和奇异值分解的涂鸦方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1cd0921e01b3f281e753103527804630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yz7FsSDGbz7O9TdAtLzcYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者</p></figure></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="782b" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">Bitesize机器学习— Bite #1</h1><p id="b3f2" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">有很多图像压缩算法，但本文将重点讨论秩K近似。本文涵盖的概念将作为其他常见机器学习问题的基础，如PCA(主成分分析)、矩阵补全和降噪。(详见最后一段。)</p><p id="cc0a" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">通过图像更容易理解Rank-K近似。为了简单起见，让我们假设我们只处理灰度图像，所以每个数据点位于0和1之间，白色为0，黑色为1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1cd0921e01b3f281e753103527804630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yz7FsSDGbz7O9TdAtLzcYw.png"/></div></div></figure></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="e295" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">等一下。我们所说的压缩是什么意思？</h1><p id="cb82" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">在深入算法和数学之前，让我们对正在解决的问题有一个直觉。压缩一个矩阵是什么意思？或者在这种情况下，压缩一幅图像意味着什么？</p><p id="bd03" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">近似的关键思想是找到一个与原始矩阵/图像“相似”并且占用更少空间的矩阵/图像。(我们稍后将定义“相似的”。目前，我们只需要一种直觉。)</p><p id="9edc" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">让我们首先想象我们有一个500 x 500的灰度图像。在这种情况下，我们需要存储至少250，000个数据点来显示图像。</p><p id="734a" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">这里有一个问题:如果压缩图像应该占用更少的存储空间，这是否意味着我们必须缩小原始图像的宽度和高度才能得到压缩图像？如果我们缩小宽度和高度，那就不是真正的压缩了，不是吗？这就是图像裁剪。因此，是否有可能存储更少的数据点，同时仍然保持相同的图像大小(在本例中为500 x 500)？</p><p id="0aba" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">这就是令人困惑的地方。如果我们仍然希望压缩图像为500 x 500，这意味着我们仍然需要250，000个数据点来显示。我们如何存储不到250，000个数据点，但最终仍然有250，000个数据点？听起来很矛盾！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/acd972803a69ca0575f1b8719fa85356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTKkDviGf4WF52EOR0KQrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终的压缩图像仍然具有与原始图像相同的像素数。</p></figure><h1 id="fc8d" class="lf lg it bd lh li mz lk ll lm na lo lp jz nb ka lr kc nc kd lt kf nd kg lv lw bi translated"><strong class="ak">玩具场景</strong></h1><p id="f039" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">让我们想象一个玩具场景，它会使矩阵压缩变得容易。</p><p id="8948" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">为了简单起见，假设我们有一个图像矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/35061944740d9f9252227e074fcad8dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*ce5BniQ4RKQOc4pGYXibqQ.png"/></div></figure><p id="6345" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">乍一看，似乎我们别无选择，只能存储总共九个数据点。然而，这个矩阵有一些特殊之处，它总共只需要六个数据点来存储。你看到了吗？(先不要向下滚动查看解决方案。)</p><p id="a3dd" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">如果我们看M的三个列向量，我们意识到它们是线性依赖的。在其他情况下，列向量是彼此的倍数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/20426b5ceda3a90c7395015abf4b8483.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*yPTDImZ0DuF6J2CX__N6qg.png"/></div></figure><p id="773c" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">这意味着我们可以将矩阵<code class="fe ne nf ng nh b">M</code>分解成一个列向量乘以三个标量倍数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bdcffabbcc9b6ece990708411ca7e391.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*PM6dzaxX-C70PRr5NjasXg.png"/></div></figure><p id="17c6" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">在这种情况下，原本会占用九个数据点的图像现在只占用六个数据点。我们所要做的就是对这两个向量进行矩阵乘法，就可以得到原来的9个数据点。万岁，压缩在工作！</p><p id="0631" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">如果所有的图像都有上述相似的属性就好了…</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="8acf" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">但是他们有！什么是等级？</h1><p id="bd5c" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">上面提到的玩具矩阵被定义为<em class="nl">秩1矩阵</em>。这意味着矩阵的所有列都是彼此的倍数。</p><p id="3e6d" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">形式上，秩1矩阵被定义为可以由列向量乘以行向量来表示的矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/19e992a0a98c596183a36f0d3cf2c3ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*0rW0HQ5aFIAKzpBqJL_sbw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/53764cbfd70e8390ac0212e5bc469c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbaYZy2gJPJQZ5UIbCzaWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">秩1矩阵</p></figure><p id="df72" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">定义了秩为1的矩阵后，我们现在可以定义更高秩的矩阵:</p><p id="866b" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">A <strong class="lz iu"> </strong> <em class="nl">秩为2的矩阵</em>可以表示为两个秩为1的矩阵之和，A本身不是秩为1的矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2d330ae0f295d5305f9a325bc68e0288.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*hvYgXEmUIB5rQh7Fob8Vgg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/5bd24262389d7e49a68559451e8f871b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EL9DyAHE9ZG_CwJuIk2B8Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">秩2矩阵</p></figure><p id="cff4" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">按照相同的逻辑，秩为k的矩阵<strong class="lz iu"> </strong>可以写成k个秩为1的矩阵的和，而不是秩为1、秩为2、…、或秩为-(k-1)的矩阵。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="8363" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak">奇异值分解</strong></h1><p id="f118" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">在上面的玩具例子中，由于完美的线性依赖，在压缩中没有信息丢失。事实上，完美的线性相关性在图像中很少见，因此我们允许在压缩过程中丢失一些数据。</p><p id="9b75" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">任何形状的矩阵都可以分解成它的SVD(奇异值分解)形式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0e7ad022ea4b48e053d01c8473f9b58f.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*ZaeZ4zL1SA2QkQsEHSYyyA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/077ba49b67e468d810adb94497ce873f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6ffq7Wj-pggUQkIDPPzBw.png"/></div></div></figure><p id="0e9f" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">带有S的条目:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5e3358ad52ae8653c2f57c14f956497b.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*SnJHuFgHPsSKWL1-PWauDA.png"/></div></figure><p id="b78c" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">我们可以选择前k个奇异值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/472df306184c334bc7a87e1f30ac400f.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*f6jPNzoUrQI4XNj3uwblpA.png"/></div></figure><p id="da76" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">和k个右奇异向量来逼近原始矩阵。换句话说，我们已经找到了原始矩阵的“秩k近似”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/78110577ec42073b319af4fc253648f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjKPDumdRNiOq3guXhVizw.png"/></div></div></figure><p id="3d1e" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">直觉上，我们在矩阵中寻找“最重要”或“最具代表性”的向量，并使用这些“最具代表性”的向量来重建整个矩阵。在K秩近似中，我们寻找K个最有代表性的向量。</p><p id="27f4" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">事实证明，原本会占用<code class="fe ne nf ng nh b">m x n</code>个数据点的图像现在只占用<code class="fe ne nf ng nh b">m x k + n x k = k (m+n)</code>个数据点。令人惊奇的是，最终的图像仍然是一个<code class="fe ne nf ng nh b">m x n</code>图像。我们没有为了压缩而缩小图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/fbe8c799d4b455c089da1437bd6f1f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PMhjhyBPXVvlK1qixx8BXw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们只需要k(m+n)个数据点来表示一个mxn矩阵</p></figure><p id="c831" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">最后一行显示矩阵<code class="fe ne nf ng nh b">M</code>可以近似写成k秩矩阵。因此得名——秩K近似。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="ad33" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">利用秩K近似压缩小狗图像</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/5c79d7c42894c2cc7bd5b39981ba73d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AJA81DWhWx7NQeQs"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae nx" href="https://unsplash.com/@florenciapotter?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">弗洛伦西亚·波特</a>在<a class="ae nx" href="https://unsplash.com/s/photos/corgi?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="df3a" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">第一步。将图像转换为灰度</h2><p id="62b7" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">为简单起见，让我们将图像转换为灰度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/9a1f8cea4d00e9a7e9ec8c1e1d84f9a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DI09g9jooh_-CUN8yo74Gg.png"/></div></div></figure><p id="7f4a" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">注意:记住在<code class="fe ne nf ng nh b">plt.imshow()</code>中设置<code class="fe ne nf ng nh b">cmap='gray'</code>以查看灰度图像。</p><h2 id="b361" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">第二步。用<code class="fe ne nf ng nh b">numpy.linalg.svd</code>计算奇异值分解</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="4355" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">注:对于图像，<code class="fe ne nf ng nh b">full_matrices</code>可以设置为<code class="fe ne nf ng nh b">True</code>或<code class="fe ne nf ng nh b">False</code>。</p><h2 id="c9a3" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">第三步。选择前K个奇异值和相应的K个左奇异向量和K个右奇异向量</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><h2 id="3334" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">第四步。重构秩K近似</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><h2 id="c8c5" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">用不同的K秩重建图像的结果</h2><p id="807f" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">让我们用不同的k值来重建图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/de78320e27cc46fd44b5c54c2d72b34b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SnNnKluN4Jyt5X3OlnqJCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k = [40，30，20，10].0</p></figure><p id="f528" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">我们可以看到，随着k值的降低，图像变得更加模糊。直观地说，这意味着我们正在从不太重要的向量中重建图像。</p><h2 id="25be" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">这些近似值有多好？</h2><p id="1b98" class="pw-post-body-paragraph lx ly it lz b ma mb ju mc md me jx mf mg mh mi mj mk ml mm mn mo mp mq mr ms im bi translated">我们想知道压缩图像与原始图像有多相似。</p><p id="1a4d" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">在统计领域，这是通过标准差来衡量的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3fa437a95a335020fd607b11c6d8aaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*v6uDPSrYde8-HCRubIWSlg.png"/></div></figure><p id="0e7c" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated">两个矩阵之间的相似性可以通过Frobenius范数来测量，该范数定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/27c000d978483212d96e89a104751f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*c3_kh2ymsMk7VLOroxz8lQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/34128d100c55bbe444f37feccb499fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*-0z3CmtQEA78a_zdgt6mUw.png"/></div></figure><p id="deb9" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated"><code class="fe ne nf ng nh b">M</code>:原始图像矩阵</p><p id="0652" class="pw-post-body-paragraph lx ly it lz b ma mt ju mc md mu jx mf mg mv mi mj mk mw mm mn mo mx mq mr ms im bi translated"><code class="fe ne nf ng nh b">M'</code>:压缩图像矩阵</p><h2 id="06a9" class="ny lg it bd lh nz oa dn ll ob oc dp lp mg od oe lr mk of og lt mo oh oi lv oj bi translated">完整的Python代码</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="a601" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak">进一步的应用</strong></h1><ol class=""><li id="b170" class="or os it lz b ma mb md me mg ot mk ou mo ov ms ow ox oy oz bi translated"><strong class="lz iu">主成分分析(PCA) </strong>:利用奇异值分解(SVD)的性质降低数据维数</li><li id="4e3c" class="or os it lz b ma pa md pb mg pc mk pd mo pe ms ow ox oy oz bi translated"><strong class="lz iu">矩阵填充</strong>:当一个矩阵中有数据缺失时，我们可以利用秩K近似来填充缺失的数据。</li><li id="57c7" class="or os it lz b ma pa md pb mg pc mk pd mo pe ms ow ox oy oz bi translated"><strong class="lz iu">降噪</strong>:当我们想要消除数据矩阵中的噪声时，我们可以使用前k个最重要的向量来重建矩阵。</li></ol></div></div>    
</body>
</html>