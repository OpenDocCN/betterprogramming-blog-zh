<html>
<head>
<title>Mastering Web Scraping in Python: Crawling From the Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">掌握Python中的Web抓取:从头开始抓取</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/mastering-web-scraping-in-python-crawling-from-scratch-cb510bcb9fb6?source=collection_archive---------3-----------------------#2021-08-11">https://betterprogramming.pub/mastering-web-scraping-in-python-crawling-from-scratch-cb510bcb9fb6?source=collection_archive---------3-----------------------#2021-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="22e4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">建立一个大规模抓取的网络爬虫。从一个接一个地访问页面开始，并以线程安全的方式扩展并行性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5d331ba27bed423897de618b235d6b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-C1uVde5ctPIRZ3A"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@minimxlist_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">穆赫德·阿斯拉夫</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="5411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你试过爬上千页吗？进一步放大？处理系统故障并从中恢复？</p><p id="b0d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在看过如何<a class="ae ky" href="https://medium.com/codex/mastering-web-scraping-in-python-from-zero-to-hero-51e27705b51b" rel="noopener">从一个网站提取内容</a>和<a class="ae ky" href="https://uxdesign.cc/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja-8cb76db119ae" rel="noopener" target="_blank">如何避免被屏蔽</a>之后，我们再来看看抓取的过程。要获得大规模的数据，手动获取几个URL不是一个选项。我们需要使用一个自动系统，将发现新的网页，并访问它们。</p><p id="9c0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">免责声明:对于真实世界的使用，请找到合适的软件。本指南假装是对爬行过程如何工作和做基础工作的介绍。但是还有很多细节需要解决。</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="1d18" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">先决条件</h1><p id="6595" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">为了让代码工作，你需要安装<a class="ae ky" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank"> python3和</a>。有些系统已经预装了它。之后，通过运行<code class="fe na nb nc nd b">pip install</code>安装所有必要的库。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="44fa" class="ni me it nd b gy nj nk l nl nm">pip install requests beautifulsoup4</span></pre><h1 id="852a" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">如何获取页面上的所有链接</h1><p id="04ff" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">从本系列的第一篇文章中，我们知道使用<code class="fe na nb nc nd b">requests.get</code>和<code class="fe na nb nc nd b">BeautifulSoup</code>很容易从网页中获取数据。我们将从在为测试刮痧准备的<a class="ae ky" href="https://scrapeme.live/shop/page/1/" rel="noopener ugc nofollow" target="_blank">山寨店中找到链接开始。</a></p><p id="878d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获取内容的基础都是一样的。然后我们获取分页器上的所有链接，并将这些链接添加到一个<code class="fe na nb nc nd b">set</code>中。我们选择了一套以避免重复。如您所见，我们为链接硬编码了选择器，这意味着它不是一个通用的解决方案。目前，我们将专注于手头的页面。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h1 id="5ee5" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">一次一个URL，连续</h1><p id="753d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">现在我们有几个链接，但没有办法全部访问。我们需要某种循环来为每个可用的URL执行提取部分，以解决这个问题。</p><p id="750c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也许最直接的方法，虽然不是可伸缩的方法，是使用相同的循环。但在此之前，有一个缺失的部分:避免抓取同一页面两次。</p><p id="6f0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将跟踪另一个<code class="fe na nb nc nd b">set</code>中已经访问过的链接，并通过在每次请求前检查它们来避免重复。在这种情况下，<code class="fe na nb nc nd b">to_visit</code>没有被使用，只是出于演示目的而维护。</p><p id="174e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了防止访问每个页面，我们还将添加一个<code class="fe na nb nc nd b">max_visits</code>变量。目前，我们忽略<code class="fe na nb nc nd b">robots.txt</code>文件，但我们必须文明友好。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="c81b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个递归函数，有两个退出条件:没有链接可访问，或者我们达到了最大访问量。在这两种情况下，它都将退出并打印已访问的链接和待定的链接。</p><p id="7264" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，同一个链接可以多次添加，但只会被抓取一次。在一个大项目中，想法是设置一个计时器，只在几天后请求每个URL。</p><h1 id="4bc7" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">关注点分离</h1><p id="d786" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们说过这与提取或解析内容无关，但是我们需要在内容纠缠在一起之前将它们分开。</p><p id="0068" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们将创建三个助手函数:获取HTML、提取链接和提取内容。正如他们的名字所暗示的，他们中的每一个都将执行网络搜集的主要任务之一。</p><p id="acf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个将使用与前面相同的库从一个URL获取HTML，但是为了安全起见，将它包装在一个<code class="fe na nb nc nd b">try</code>块中。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="7015" class="ni me it nd b gy nj nk l nl nm">def get_html(url): <br/>	try: <br/>		return requests.get(url).content <br/>	except Exception as e: <br/>		print(e) <br/>		return ''</span></pre><p id="911a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个，提取链接，将像以前一样工作。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="9f63" class="ni me it nd b gy nj nk l nl nm">def extract_links(soup): <br/>	return [a.get('href') for a in soup.select('a.page-numbers') <br/>		if a.get('href') not in visited]</span></pre><p id="b918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一个将是提取我们想要的内容的占位符。由于我们正在简化这一部分，它将从同一页面获取基本信息，无需在详细信息页面输入。</p><p id="936c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了表明我们可以提取一些内容，我们将打印每个产品的标题(神奇宝贝名称)。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="6179" class="ni me it nd b gy nj nk l nl nm">def extract_content(soup): <br/>	for product in soup.select('.product'): <br/>		print(product.find('h2').text) <br/> <em class="lv"># Bulbasaur, Ivysaur, ...</em></span></pre><p id="d356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">把它们组装在一起。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="34b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意到什么不同了吗？爬行逻辑不附属于链接提取部分。每个助手处理一个单独的部分。而<code class="fe na nb nc nd b">crawl</code>函数通过调用它们并应用结果来充当指挥者。</p><p id="b3b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着项目的发展，所有这些部分都可以被转移到文件中，或者作为参数/回调来传递。如果核心独立于所选的页面和内容，我们可以概括用例。</p><p id="b6d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们错过了什么吗？<br/>我们需要添加第一个URL并调用爬行函数。由于<code class="fe na nb nc nd b">crawl</code>不再是递归的，我们将在一个单独的循环中处理它。</p><pre class="kj kk kl km gt ne nd nf ng aw nh bi"><span id="485e" class="ni me it nd b gy nj nk l nl nm">to_visit.add('https://scrapeme.live/shop/page/1/') <br/> <br/>while (len(to_visit) &gt; 0 and len(visited) &lt; max_visits): <br/>	crawl(to_visit.pop())</span></pre><h1 id="9e5c" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">并行请求</h1><p id="a617" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这里缺少了一个重要的部分:并行性。HTTP请求处理程序大部分时间都是空闲的，等待响应返回。这意味着我们可以同时发送几个，而不会使机器过载。然后在它们回来时进行处理。</p><p id="b5c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，这种方法仅在命令不是强制性的情况下才有效。但是我们已经在使用集合，根据<a class="ae ky" href="https://docs.python.org/3/tutorial/datastructures.html#sets" rel="noopener ugc nofollow" target="_blank"> Python的定义</a>，“集合是一个没有重复元素的<strong class="lb iu">无序集合</strong>”这意味着我们的过程从一开始就是无序的。</p><p id="9b36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入研究并行请求之前，我们必须理解几个概念:同步和队列。</p><h1 id="65e4" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">同步队列</h1><p id="0db4" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">线程或<a class="ae ky" href="https://en.wikipedia.org/wiki/Parallel_computing" rel="noopener ugc nofollow" target="_blank">并行计算</a>存在巨大的风险:从不同的线程修改相同的变量或数据结构。这意味着我们的两个请求是向集合添加新链接(即<code class="fe na nb nc nd b">to_visit</code>)。由于数据结构不受保护，两者都可以像这样读写它:</p><ul class=""><li id="7bc7" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">既读其内容，即<code class="fe na nb nc nd b">(1, 2, 3)</code> <em class="lv">(简体)</em></li><li id="7643" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">线程一给页面添加链接<code class="fe na nb nc nd b">4, 5</code> : <code class="fe na nb nc nd b">(1, 2, 3, 4, 5)</code></li><li id="e2d1" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">线程二添加页面链接<code class="fe na nb nc nd b">6, 7</code> : <code class="fe na nb nc nd b">(1, 2, 3, 6, 7)</code></li></ul><p id="0fdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是怎么发生的？当线程2编写新链接时，它将它们添加到只有三个元素的集合中。<br/> <em class="lv">这是一个非常简化的版本；查看链接了解更多信息。</em></p><p id="7fdb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能做些什么来避免这些冲突？同步或锁定。来自<a class="ae ky" href="https://docs.python.org/3/library/queue.html" rel="noopener ugc nofollow" target="_blank">文档</a>:“队列使用锁来临时阻塞竞争线程。”这意味着线程一将获得集合上的锁，没有任何问题地进行读写，然后自动释放锁。同时，线程2必须等到锁可用。然后才读和写。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="bf36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就目前而言，它不起作用。不要担心。现有代码的变化是最小的:我们用一个队列替换了<code class="fe na nb nc nd b">to_visit</code>。</p><p id="0461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是队列需要处理程序或工人来处理它们的内容。通过上面的操作，我们创建了一个队列并添加了一个条目(原来的那个)。</p><p id="9d8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还修改了<code class="fe na nb nc nd b">crawl</code>函数，将链接放入队列中，而不是更新之前的集合。</p><p id="3970" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将创建一个使用<a class="ae ky" href="https://docs.python.org/3/library/threading.html" rel="noopener ugc nofollow" target="_blank">线程模块</a>来处理该队列的工人。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="3394" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们定义了一个新的函数来处理排队的项目。为此，我们进入了一个无限循环，当所有的处理完成时，这个循环就会停止。</p><p id="61ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后<code class="fe na nb nc nd b">get</code>一个项目，它将阻止，直到一个项目可用。我们处理那个项目；目前，只需打印它来展示它是如何工作的。它稍后会调用<code class="fe na nb nc nd b">crawl</code>。</p><p id="2156" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们通过调用<code class="fe na nb nc nd b">task_done</code>通知队列该项目已经被处理。</p><p id="5168" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦队列得到所有项目的通知并为空，它将停止执行并结束无限循环。这就是<code class="fe na nb nc nd b">join</code>函数所做的，“阻塞，直到队列中的所有项目都被获取和处理。”</p><p id="bc7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们还需要两件事:处理项目和创建更多的线程(它不会与一个并行，不是吗？).</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="fab4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行时要小心，因为<code class="fe na nb nc nd b">num_workers</code>和<code class="fe na nb nc nd b">max_visits</code>中的大数字会启动大量请求。如果脚本因为任何原因有一些小错误，您可能会在几秒钟内执行数百个请求。</p><h1 id="162e" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">表演</h1><p id="65fb" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们使用不同的设置运行基准测试，只是作为参考。</p><ul class=""><li id="f9b5" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">连续请求:29，32秒</li><li id="ba9c" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">一个工人排队(<code class="fe na nb nc nd b">num_workers = 1</code>):2941秒</li><li id="81cc" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">两个工人排队(<code class="fe na nb nc nd b">num_workers = 2</code>):20.05秒</li><li id="9221" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">五人队列(<code class="fe na nb nc nd b">num_workers = 5</code>):1197人</li><li id="9175" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">十人队列(<code class="fe na nb nc nd b">num_workers = 10</code>):1202</li></ul><p id="4dc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顺序请求和只有一个工作者之间几乎没有区别。线程会带来一些开销，但是在这里几乎看不到。这将需要更严格的负载测试。一旦我们开始增加工人，开销就有回报了。我们可以添加更多，但这不会影响结果，因为他们大多数时间都是空闲的。</p><h1 id="20ae" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">分布式处理</h1><p id="82ef" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们不会讨论下面的扩展步骤:在几个服务器之间分布爬行过程。<a class="ae ky" href="https://docs.python.org/3/library/multiprocessing.html#using-a-remote-manager" rel="noopener ugc nofollow" target="_blank"> Python允许的</a>，有些库可以帮你做(<a class="ae ky" href="https://docs.celeryproject.org/en/stable/" rel="noopener ugc nofollow" target="_blank">芹菜</a>或者<a class="ae ky" href="https://python-rq.org/" rel="noopener ugc nofollow" target="_blank"> Redis队列</a>)。这是一个巨大的进步，我们今天已经走得够远了。</p><p id="33be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个快速预览，其背后的想法与线程的想法是一样的。每个项目都将像我们之前看到的那样被处理，但是是在不同的线程中，甚至是在运行相同代码的机器中。</p><p id="45bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方法，我们可以进一步扩展；理论上，没有限制。但在现实中，总有一个极限或瓶颈，通常是处理分发的中心节点。</p><h1 id="d985" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">在扩大规模时考虑</h1><p id="9183" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">出于教育目的，我们展示了一个简化版本的爬行过程。要大规模应用所有这些，你应该首先考虑几件事。</p><h1 id="0348" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">构建vs购买vs开源</h1><p id="79b7" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在您编写自己的爬行库之前，请尝试一些选项。很多很棒的开源库都可以实现:<a class="ae ky" href="https://docs.scrapy.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>，<a class="ae ky" href="http://docs.pyspider.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> pyspider </a>，<a class="ae ky" href="https://github.com/bda-research/node-crawler" rel="noopener ugc nofollow" target="_blank"> node-crawler </a> (Node.js)，或者<a class="ae ky" href="https://github.com/gocolly/colly" rel="noopener ugc nofollow" target="_blank"> Colly </a> (Go)。以及很多为你提供抓取抓取解决方案的公司和服务。</p><h1 id="f41f" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">避免被屏蔽</h1><p id="839f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">正如我们在以前的帖子中看到的，我们可以采取一些措施来避免阻塞。其中一些是代理和头。下面是一个简单的代码片段，将它们添加到我们当前的代码中。<br/> <em class="lv">注意，这些</em> <a class="ae ky" href="https://free-proxy-list.net/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">免费代理</em> </a> <em class="lv">可能不适合你。它们是短命的。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h1 id="6f8d" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">提取内容</h1><p id="4eb0" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们在这里不详细介绍，只介绍一个简单的代码片段，用于提取每个商品的id、名称和价格。我们将所有东西存储在一个<code class="fe na nb nc nd b">data</code>数组中，这不是一个好主意。但是对于演示来说已经足够了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h1 id="b535" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">持续</h1><p id="10b9" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们没有保存任何东西，这是不可伸缩的。在现实世界中，我们应该存储内容，甚至HTML本身，以便以后处理。</p><p id="db07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以及所有发现的带有时间戳时间的URL。这听起来似乎需要一个数据库。根据需要，我们可以只存储实际的内容或整个URL、日期、HTML等等。</p><h1 id="135a" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">礼服</h1><p id="f620" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">链接提取部分不考虑<a class="ae ky" href="https://en.wikipedia.org/wiki/Canonical_link_element" rel="noopener ugc nofollow" target="_blank">规范链接</a>。一个页面可以有多个URL:查询字符串或散列可能会修改它。在我们的例子中，我们会爬两次。现在不是问题，是需要考虑的事情。</p><p id="8531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正确的方法是将规范的URL(如果存在的话)添加到访问列表中。那么我们可以从不同原始URL到达相同的页面，但是我们会检测到它是重复的。我们还可以使用<a class="ae ky" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.url_query_cleaner" rel="noopener ugc nofollow" target="_blank"> url_query_cleaner </a>删除一些查询字符串参数。</p><h1 id="7ef1" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">Robots.txt</h1><p id="d57e" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们没有检查它，因为我们正在使用一个为刮准备的测试网站。但是请检查robots文件，并在搜索实际目标时遵守它。除此之外，不要造成超过他们处理能力的流量。再次强调，要文明礼貌；)</p><h1 id="9298" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">最终代码</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h1 id="009f" class="md me it bd mf mg nn mi mj mk no mm mn jz np ka mp kc nq kd mr kf nr kg mt mu bi translated">结论</h1><p id="2491" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们希望你放弃三个要点:</p><ol class=""><li id="b66a" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu oi oa ob oc bi translated">将获取HTML和从爬行本身提取链接分开。</li><li id="44b1" class="nu nv it lb b lc od lf oe li of lm og lq oh lu oi oa ob oc bi translated">为您的用例选择合适的系统:简单顺序、并行或分布式。</li><li id="6993" class="nu nv it lb b lc od lf oe li of lm og lq oh lu oi oa ob oc bi translated">从零开始大规模建设可能会带来伤害。看看免费或付费的图书馆/解决方案。</li></ol><p id="d737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们即将完成这个关于网络抓取的系列文章。请继续关注下一篇关于进一步扩展这个爬行过程的文章。</p><p id="54a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要忘记看看本系列的其他文章。<br/> + <a class="ae ky" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-scaling-to-distributed-crawling?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=crawling_scratch" rel="noopener ugc nofollow" target="_blank">缩放到分布式抓取</a> (4/4) <br/> + <a class="ae ky" href="https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=crawling_scratch" rel="noopener ugc nofollow" target="_blank">像忍者一样躲避阻挡</a> (2/4) <br/> + <a class="ae ky" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-from-zero-to-hero?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=crawling_scratch" rel="noopener ugc nofollow" target="_blank">掌握抽取</a> (1/4)</p><p id="4983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="5e96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">最初发表于</em><a class="ae ky" href="https://zenrows.com/blog/mastering-web-scraping-in-python-crawling-from-scratch?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=crawling_scratch" rel="noopener ugc nofollow" target="_blank"><em class="lv">https://www.zenrows.com</em></a></p></div></div>    
</body>
</html>