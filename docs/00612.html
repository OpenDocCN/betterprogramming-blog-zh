<html>
<head>
<title>Real-Time Object Detection on GPUs in 10 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在10分钟内在GPU上进行实时对象检测</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/real-time-object-detection-on-gpus-in-10-minutes-6e8c9b857bb3?source=collection_archive---------1-----------------------#2019-06-19">https://betterprogramming.pub/real-time-object-detection-on-gpus-in-10-minutes-6e8c9b857bb3?source=collection_archive---------1-----------------------#2019-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="17ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何在10分钟内在GPU上运行高性能对象检测管道进行推理。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c9367f5700218919c627d10ebfd67624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OAuVCw-k7XZOekBa"/></div></div></figure><p id="11bd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">物体检测是一种非常受欢迎的深度学习应用，从简单的家庭自动化应用到安全关键的自动驾驶。事实证明，GPU在执行深度学习训练和推理方面非常强大。现在有许多库和工具来执行这些任务。在这篇文章中，我们将向您展示如何在10分钟内在GPU上运行高性能对象检测管道进行推理。</p><p id="2357" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们的Python应用程序从实时视频流中提取帧，并在GPU上执行对象检测。我们使用预训练的单镜头检测(SSD)模型和Inception V2，应用TensorRT的优化，为我们的GPU生成运行时，然后对视频馈送执行推理以获得标签和边界框。然后，应用程序用这些边界框和类标签注释原始帧。产生的视频馈送具有覆盖在其上的来自我们的对象检测网络的边界框预测。同样的方法可以扩展到其他任务，如分类和分割。</p><p id="3aab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然GPU和NVIDIA软件的知识不是必需的，但您应该熟悉对象检测和Python编程。使用的一些软件工具包括来自<a class="ae ln" href="https://ngc.nvidia.com/catalog/landing" rel="noopener ugc nofollow" target="_blank"> NVIDIA GPU Cloud </a> (NGC)的Docker容器来设置我们的环境，<a class="ae ln" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"> OpenCV </a>来运行来自相机的馈送，以及<a class="ae ln" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank">tensort</a>来加速我们的推理。虽然阅读这篇文章会让你受益匪浅，但是你需要一个支持CUDA的GPU和一个连接到你的机器上的摄像头来运行这个例子。</p><p id="2b89" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文结束时，您将了解设置端到端对象检测推理管道所需的组件，如何在GPU上应用不同的优化，以及如何在管道上执行FP16和INT8 precision中的推理。作为参考，所有代码(以及如何安装所有内容的详细自述文件)都可以在<a class="ae ln" href="https://github.com/NVIDIA/" rel="noopener ugc nofollow" target="_blank"> NVIDIA GitHub页面</a>上找到。</p><p id="3b2e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">用命令<code class="fe lo lp lq lr b">nvidia-smi</code>测试你有一个正常工作的GPU。CUDA GPUs列表在<a class="ae ln" href="https://developer.nvidia.com/cuda-gpus" rel="noopener ugc nofollow" target="_blank">本页</a>。</p><p id="a48b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们使用的网络是以<a class="ae ln" href="https://arxiv.org/abs/1512.00567v3" rel="noopener ugc nofollow" target="_blank"> InceptionV2 </a>为骨干的<a class="ae ln" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">单发探测</a>网络。这个应用中使用的所有代码都可以在这个<a class="ae ln" href="https://github.com/NVIDIA/object-detection-tensorrt-example" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中获得。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="0309" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">运行样本！</h1><p id="2853" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">我们使用Docker容器来设置环境并打包它们以供分发。我们可以回忆起许多使用容器很容易立即从冲突和崩溃中恢复的情况，所以在尝试这个例子之前，请确保您的机器上有<a class="ae ln" href="https://docs.docker.com/v17.12/install/linux/docker-ce/ubuntu/" rel="noopener ugc nofollow" target="_blank"> Docker </a>和<a class="ae ln" href="https://github.com/NVIDIA/nvidia-docker" rel="noopener ugc nofollow" target="_blank"> NVIDIA Docker </a>。</p><p id="ddb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">导航到主<code class="fe lo lp lq lr b">object-detection-webcam</code>文件夹，运行下面的部分来构建容器并运行应用程序:</p><pre class="kg kh ki kj gt mw lr mx my aw mz bi"><span id="6ed0" class="na ma iq lr b gy nb nc l nd ne">./setup_environment.sh</span><span id="c322" class="na ma iq lr b gy nf nc l nd ne">python SSD_Model/detect_objects_webcam.py</span></pre><p id="47e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这将弹出一个窗口，显示来自您的网络摄像头的视频源，带有如图1所示的边框和标签。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/dd68022a45bdee974f143b4f84394d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLm8Q-i_VOzbQP43dFCx5w.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated"><strong class="bd nl">图一。</strong>命令提示符上的输出显示了推理所用的时间和目标类的前1名预测</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="c26f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">使用NGC和TensorRT开源软件进行设置</h1><p id="dda1" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">让我们回顾一下设置，设置的所有代码都可以在<code class="fe lo lp lq lr b">setup_environment.sh</code>中找到。有4个关键步骤:</p><ol class=""><li id="4bea" class="nm nn iq kt b ku kv kx ky la no le np li nq lm nr ns nt nu bi translated">为Docker设置环境变量以查看网络摄像头</li><li id="2de1" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">下载VOC数据集用于INT8校准(我们将在博客后面看到)</li><li id="4541" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">构建一个包含我们运行代码所需的所有库的<code class="fe lo lp lq lr b">Dockerfile</code></li><li id="5eca" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">启动那个<code class="fe lo lp lq lr b">Dockerfile</code>，这样我们就可以在正确的环境中运行应用程序</li></ol><p id="2f44" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因为我们使用Docker容器来管理我们的环境，所以我们需要让我们的容器访问主机中的所有硬件。这大部分是由Docker自动处理的，除了我们手动添加的网络摄像头。我们需要为Docker设置访问X11的权限，X11用于打开webcam feed的GUI。通过使用环境变量和设置在docker run命令期间传递到容器中的权限来实现这一点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="fa19" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">接下来，我们下载用于INT8校准的PASCAL VOC数据集，我们将在后面的章节中讨论。该数据集包含常见家居用品和日常物品的图像。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="7f6a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后我们建立一个<code class="fe lo lp lq lr b">Dockerfile</code>，它拥有我们的整个开发环境。<code class="fe lo lp lq lr b">Dockerfile</code>安装以下部件:</p><ol class=""><li id="8942" class="nm nn iq kt b ku kv kx ky la no le np li nq lm nr ns nt nu bi translated">TensorRT和所需的库</li><li id="40e8" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">TensorRT开源软件，替换TensorRT安装中的插件和解析器</li><li id="c89d" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">我们的应用程序的其他依赖项</li></ol><p id="f52b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用来自<a class="ae ln" href="https://ngc.nvidia.com/catalog/landing" rel="noopener ugc nofollow" target="_blank">英伟达NGC </a>的tensort容器，安装tensort非常简单。该容器包含所需的库，如<a class="ae ln" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank"> CUDA </a>、<a class="ae ln" href="https://developer.nvidia.com/cudnn" rel="noopener ugc nofollow" target="_blank"> cuDNN </a>和<a class="ae ln" href="https://developer.nvidia.com/nccl" rel="noopener ugc nofollow" target="_blank"> NCCL </a>。NGC是一个预建容器的存储库，这些容器每月更新一次，并跨平台和云服务提供商进行测试。参见<a class="ae ln" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-05.html#rel_19-05" rel="noopener ugc nofollow" target="_blank">发行说明</a>中TensorRT容器中的内容。因为除了tensort之外，我们还需要组合多个其他库和包，所以我们将创建一个自定义的<code class="fe lo lp lq lr b">Dockerfile</code>,使用tensort容器作为基本映像。</p><p id="36ec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因为TensorRT插件和解析器的最新版本是开源的，所以我们在例子中使用了它们。插件提供了一种在TensorRT模型中使用定制层的方法，并且已经包含在TensorRT容器中。例如，SSD模型使用插件库中的flattenConcat插件。严格地说，在这个例子中，我们不需要使用开源版本的插件，使用TensorRT容器中的版本也可以。了解这一点很方便，您可以扩展和定制这些组件，以支持模型中的定制层。</p><p id="49cf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了获得开源插件，我们克隆了<a class="ae ln" href="https://github.com/NVIDIA/TensorRT" rel="noopener ugc nofollow" target="_blank">tensort GitHub repo</a>，使用<em class="oc"> cmake </em>构建组件，并用新版本替换tensort容器中这些组件的现有版本。TensorRT应用程序将在这个路径下搜索TesnorRT核心库、解析器和插件。</p><p id="67ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，我们可以安装应用程序所需的其他依赖项，主要是OpenCV及其渲染库。OpenCV是一个计算机视觉库，我们用它来与我们的网络摄像头进行交互。</p><p id="956c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用docker build命令构建<code class="fe lo lp lq lr b">Dockerfile</code>中的所有组件:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="3e54" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">启动容器，打开新的开发环境，如下所示。在这个命令中，我们将运行时设置为Nvidia，让Docker知道我们的主机有GPU，然后我们将GitHub repo挂载到Docker容器中以访问其中的代码，最后通过后续的挂载和环境变量转发有关如何与网络摄像头交互的信息。有关我们使用的标志的更多信息，请查看<a class="ae ln" href="https://docs.docker.com/engine/reference/run/" rel="noopener ugc nofollow" target="_blank"> Docker文档</a>。</p><pre class="kg kh ki kj gt mw lr mx my aw mz bi"><span id="c059" class="na ma iq lr b gy nb nc l nd ne">docker run — runtime=nvidia -it -v `pwd`/:/mnt — device=/dev/video0 -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH object_detection_webcam</span></pre><p id="50ab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一旦容器启动，您就可以使用以下命令运行您的应用程序:</p><pre class="kg kh ki kj gt mw lr mx my aw mz bi"><span id="54ab" class="na ma iq lr b gy nb nc l nd ne">python detect_objects_webcam.py.</span></pre><h1 id="6d91" class="lz ma iq bd mb mc od me mf mg oe mi mj jw of jx ml jz og ka mn kc oh kd mp mq bi translated">优化模型，构建推理引擎</h1><p id="69e3" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">在<strong class="kt ir"> detect_objects_webcam.py </strong>中，该应用程序的伪代码如下，如图2所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/985ab4e4be72a432bbdb82a5fcb2df3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mnywPWQIQW5j0Paf"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated"><strong class="bd nl">图二。</strong>本博客将涵盖该工作流程中的所有步骤，从构建TensorRT引擎到将其插入一个简单的应用程序。</p></figure><p id="df3e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第一步是从TensorFlow模型动物园下载冻结的SSD对象检测模型。这是在<code class="fe lo lp lq lr b">prepare_ssd_model</code> <strong class="kt ir"> </strong>中完成的<strong class="kt ir"> </strong> <code class="fe lo lp lq lr b">model.py</code> <strong class="kt ir"> : </strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="fbb8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下一步是优化这个推理模型，并生成一个在您的GPU上执行的运行时。我们使用TensorRT，这是一个深度学习优化器和运行时引擎。TensorRT从这个应用程序为每个NVIDIA GPU生成运行时。您需要应用程序提供尽可能低的延迟来实时执行推理。让我们来看看如何用TensorRT实现这一点。</p><p id="c3b1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用<code class="fe lo lp lq lr b">model.py</code>中可用的实用程序将冻结的张量流图转换为通用框架格式(UFF)。现在使用解析器将UFF模型导入TensorRT，应用优化，并生成运行时引擎。优化是在构建过程中秘密应用的，您不需要做任何事情来应用它们。例如，TensorRT可以将卷积、ReLU和Bias等多个层融合到一个层中。这就是所谓的层融合。另一种优化是张量融合或层聚合，其中共享相同输入的层融合到单个核中，然后将它们的结果去连接。</p><p id="20c5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要构建运行时引擎，您需要指定四个参数:</p><ol class=""><li id="b326" class="nm nn iq kt b ku kv kx ky la no le np li nq lm nr ns nt nu bi translated">我们模型的UFF文件的路径</li><li id="97be" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">推理引擎的精度(FP32、FP16或INT8)</li><li id="88f7" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">校准数据集(仅在运行INT8时需要)</li><li id="316d" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm nr ns nt nu bi translated">推断过程中使用的批量</li></ol><p id="379b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">参见<code class="fe lo lp lq lr b">engine.py</code>中的发动机建造规范。建造引擎的功能叫做<code class="fe lo lp lq lr b">build_engine</code>。</p><p id="c968" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">较低精度的推理(FP16和INT8)提高了吞吐量并提供了更低的延迟。使用FP16 precision在张量核上提供比FP32快几倍的性能，而模型精度实际上没有下降。INT8中的推理可以在模型精度下降不到1%的情况下进一步提高性能。TensorRT从FP32和您允许的任何精度中选择内核。当您启用FP16精度时，TensorRT会从FP16和FP32精度中选择内核。要使用FP16和INT8 precision，请使两者都能获得尽可能高的性能。</p><p id="b58c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">校准用于确定图形中张量的动态范围，因此您可以有效地使用INT8精度的有限范围。稍后会详细介绍。</p><p id="34fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后一个参数，批处理大小，用于为推理工作负载选择最佳内核。您可以将引擎用于比创建时指定的批量更小的批量。然而，性能可能并不理想。我通常为最常见的批量生成几个引擎，并在它们之间切换。在本例中，我们将从网络摄像头中一次抓取一帧，使批量大小为一。</p><p id="5523" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还需要注意的是，TensorRT会自动检测GPU上的任何专用硬件。因此，如果你的GPU有张量核，它会自动检测到这一点，并在这些张量核上运行你的FP16内核。</p><p id="862b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们看看<strong class="kt ir"> </strong> <code class="fe lo lp lq lr b">engine.py</code>，看看所有这些参数是如何工作的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="a761" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> </strong> <code class="fe lo lp lq lr b">build_engine</code> <strong class="kt ir"> </strong>函数为构建器、解析器和网络创建一个对象。解析器以UFF格式导入SSD模型，并将转换后的图形放在网络对象中。当我们使用UFF解析器来导入转换后的TensorFlow模型时，TensorRT还包括Caffe和<a class="ae ln" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>的解析器。这两者都可以在TensorRT开源回购中获得。使用这个模型的ONNX格式仅仅意味着改为调用ONNXParser代码的其余部分是相同的。</p><p id="5827" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第71行指定了TensorRT应用优化应该使用的内存。这只是暂存空间，您应该提供系统允许的最大大小；我提供了两个GB。条件代码根据推理的精度来设置参数。对于第一次运行，我们使用默认的FP32精度。</p><p id="d0d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">接下来的几行为解析器指定了输入节点和输出节点的名称和形状。parser.parse实际上使用我们上面指定的参数在我们的UFF文件上执行解析器。最后，<code class="fe lo lp lq lr b">builder.build_cuda_engine</code>对网络进行优化，生成引擎对象。</p><p id="5e56" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">脚本<code class="fe lo lp lq lr b">engine.py</code> <strong class="kt ir"> </strong>有两个额外的关键功能:<code class="fe lo lp lq lr b">save_engine</code> <strong class="kt ir"> </strong>和<code class="fe lo lp lq lr b">load_engine</code>。一旦你生成了一个引擎，你可以把它保存到磁盘上以备将来使用，这个过程叫做<em class="oc">序列化</em>。序列化生成一个计划文件，您可以随后从磁盘加载该文件，这通常比从头开始重新构建引擎要快得多。这就是这些加载和保存函数的作用。如果您更改了用于构建引擎的参数、使用的模型或使用的GPU，您需要重新生成引擎，因为TensorRT会选择不同的内核来构建引擎。</p><p id="a0e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您可以从<a class="ae ln" href="https://ngc.nvidia.com/catalog/models?orderBy=modifiedDESC&amp;query=tensorrt&amp;quickFilter=&amp;filters=" rel="noopener ugc nofollow" target="_blank"> NGC模型</a>下载预训练模型、参数和精度的几种组合的计划文件。如果我使用的是标准模型，我通常首先会检查NGC上是否有计划文件可以直接用于我的应用程序。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="f68c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">使用TensorRT引擎运行推理</h1><p id="9b5b" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">我们现在可以使用TensorRT引擎来执行对象检测。为了在我们的例子中使用引擎，我们将从网络摄像头中一次获取一帧，并将其传递给<strong class="kt ir"> </strong> <code class="fe lo lp lq lr b">inference.py</code>中的TensorRT引擎，更具体地说是在函数<code class="fe lo lp lq lr b">infer_webcam</code>中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="8ab4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该函数首先从网络摄像头加载图像(第174行)，然后在函数<code class="fe lo lp lq lr b">load_img_webcam</code>中执行一些预处理步骤。我们的示例将轴的顺序从HWC移动到CHW，对图像进行归一化，使所有值都落在-1和+1之间，然后对数组进行扁平化。您还可以在此函数中添加管道所需的任何其他预处理操作。</p><p id="a899" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第182行开始计时，测量TensorRT引擎执行推理所需的时间。这有助于理解整个推理管道的延迟。</p><p id="e7d1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们调用<code class="fe lo lp lq lr b">do_inference</code>来执行推理。这个函数将我们的数据发送到TensorRT引擎进行推理，并返回两个参数:<code class="fe lo lp lq lr b">detection_out</code>和<code class="fe lo lp lq lr b">keepCount_out</code>。<code class="fe lo lp lq lr b">detection_out</code> <strong class="kt ir"> </strong>包含关于每个检测的边界框坐标、置信度和类别标签的所有信息，并且<code class="fe lo lp lq lr b">keepCount_out</code>跟踪网络发现的检测的总数。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="78c9" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">把所有的放在一起</h1><p id="120a" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">到目前为止，我们已经了解了如何从TensorFlow model zoo导入预训练的模型，将其转换为UFF格式，应用优化并生成TensorRT引擎，最后使用该引擎对来自网络摄像头的单个图像进行推理。</p><p id="9441" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们看看所有这些组件是如何组合在一起的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="644b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">解析命令行参数后，<code class="fe lo lp lq lr b">prepare_ssd_model</code> <strong class="kt ir"> </strong>使用<code class="fe lo lp lq lr b">model.py</code> <strong class="kt ir"> </strong>将冻结的张量流图转换为UFF格式。然后我们在第153行初始化一个tensort推理对象，如上所述使用<code class="fe lo lp lq lr b">engine.py<strong class="kt ir"> </strong></code>中的<code class="fe lo lp lq lr b">build_engine</code>来实际构建tensort引擎。如前所述，如果在我们的<code class="fe lo lp lq lr b">args.trt_engine_path</code>中没有已经保存的引擎文件，那么我们需要从头构建一个。我们模型的UFF版本也是如此。我们将在默认的FP32精度下运行，这样就不需要提供校准数据集。最后，因为我们只在一个网络摄像头上运行实时推理，所以我们将保持我们的批处理大小= 1。</p><p id="efb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们将它集成到操作网络摄像头的应用程序中。如果相机标志打开(默认)，应用程序将使用OpenCV(第164行)启动视频流，并在第167行进入主循环。在这个循环中，我们将不断地从网络摄像头获取新的帧，如第169行所示，然后对该帧执行推理，如第172行所示。</p><p id="e7fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，我们将边界框结果叠加到原始帧上(第176–180行)，然后使用<code class="fe lo lp lq lr b">imshow</code>将它们显示给用户。</p><p id="93f0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就是我们的整个管道！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="be16" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">TensorRT在INT8精度中的推理</h1><p id="84d1" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">与框架内推理相比，该应用程序在GPU上使用TensorRT进行推理的速度快了几倍。然而，你可以让它快几倍。到目前为止，我们使用单精度(FP32)进行推理，其中每个数字都用32位表示。在FP32中，激活值可以在+/- 3.4x10 <em class="oc"> ⁸ </em>的范围内，并且需要32位来存储每个数字。数量越大，执行时需要的存储空间就越多，同时也会导致性能下降。当切换到使用较低精度的FP16时，大多数型号的精度几乎相同。使用NVIDIA提供的模型和技术，您可以使用INT8 precision进行推理，从而获得最高的性能。不过，请注意，图2中INT8精度的动态范围明显较低。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/795f1e699c576c2d5271242e88f82ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8uKgTsM0vfn-MrWuJHWlIA.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated"><strong class="bd nl">图二。</strong>可以用FP32、FP16和INT8精度表示的值的动态范围</p></figure><p id="bace" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要使用INT8精度并获得类似FP32推理的精度，您需要执行一个称为校准的附加步骤。在校准过程中，对与最终数据集相似的训练数据进行推理，并收集激活值的范围。TensorRT然后计算一个比例因子，将INT8值的范围分布在每个节点的激活值的范围内。图3显示，如果一个节点的激活范围在-6到+6之间，您希望用INT8表示的256个值只覆盖这个范围。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7bafb9de3193e06ddca4efc68d316b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/0*3d0yI273-owBB3hJ"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated"><strong class="bd nl">图3。</strong>校准和量化是转换为INT8精度的关键步骤。</p></figure><p id="21f2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用以下命令重新构建TensorRT引擎，以便在您的应用中使用INT8实现精度，执行校准并运行推理。整个过程可能需要几分钟:</p><pre class="kg kh ki kj gt mw lr mx my aw mz bi"><span id="c99c" class="na ma iq lr b gy nb nc l nd ne">python detect_objects_webcam -p 8</span></pre><p id="7aef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您应该会看到相同的结果，但性能比之前使用FP32 precision获得的结果更高。</p><p id="79b4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们来看看<strong class="kt ir"> </strong> <code class="fe lo lp lq lr b">engine.py</code>中的<code class="fe lo lp lq lr b">build_engine</code>是怎么做到的。根据您为推理启用的精度，条件块启用不同的构建器模式。默认情况下，TensorRT总是选择FP32内核。如果您启用FP16模式，它也会尝试运行在FP16 precision中的内核；INT8也是如此。</p><p id="1cd8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，仅仅因为你允许较低精度的内核，并不意味着它们在性能上总是优于较高精度的内核。例如，即使我们将精度模式设置为INT8，也可能有一些FP16或FP32内核运行得更快。TensorRT将选择速度最佳的选项。</p><p id="3dbc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TensorRT检测专门硬件的存在，如张量核，并将在其上使用FP16内核以获得可能的最高性能。TensorRT自动选择最佳内核的能力称为内核自动调整。这使得TensorRT能够在提供高性能的同时，广泛应用于各种应用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="d27e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意，在INT8条件块中，我们使用了一个函数<code class="fe lo lp lq lr b">SSDEntropyCalibrator</code>。该类在批量校准期间通过您的模型运行校准数据。为此，您需要做的就是在<code class="fe lo lp lq lr b">calibrator.py</code>中实现名为<code class="fe lo lp lq lr b">get_batch</code>的函数，从您的校准数据集中获取下一批数据。参见下面<strong class="kt ir"> </strong> <code class="fe lo lp lq lr b">calibrator.py</code> <strong class="kt ir"> </strong>中的<code class="fe lo lp lq lr b">SSDEntropyCalibrator</code>代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="eadd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该函数将图像目录作为输入进行校准，并将存储缓存文件的位置作为输入。此缓存文件包含网络激活所需的所有比例因子。如果保存激活值，您只需为特定配置运行一次校准，并且可以为任何后续运行加载此缓存表。</p><p id="e7ba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就是使用TensorRT执行INT8校准所需的全部工作！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4e65" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">从这里去哪里？</h1><p id="9e82" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">这篇文章展示了如何在GPU上快速设置和运行对象检测应用程序。它涵盖了很多方面，包括设置、在INT8 precision中部署、使用TensorRT中新的开源插件和解析器、连接到网络摄像头和叠加结果。</p><p id="bfeb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">将为您留下一些与本文相关的资源:</p><ul class=""><li id="346a" class="nm nn iq kt b ku kv kx ky la no le np li nq lm ol ns nt nu bi translated">基于相同代码库的网络研讨会:<a class="ae ln" href="https://www.nvidia.com/en-us/about-nvidia/webinar-portal/?D2C=2003671" rel="noopener ugc nofollow" target="_blank">如何对常见应用进行推理</a></li><li id="7b09" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm ol ns nt nu bi translated"><a class="ae ln" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html" rel="noopener ugc nofollow" target="_blank"> TensorRT样品</a>例如本应用中使用的SSD样品</li><li id="f131" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm ol ns nt nu bi translated"><a class="ae ln" href="https://github.com/NVIDIA/TensorRT" rel="noopener ugc nofollow" target="_blank"> TensorRT开源GitHub repo </a>用于最新版本的插件、示例和解析器</li><li id="ef32" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm ol ns nt nu bi translated">TensorRT入门博客:<a class="ae ln" href="https://devblogs.nvidia.com/speed-up-inference-tensorrt/" rel="noopener ugc nofollow" target="_blank">如何用tensort加速推理</a></li><li id="5c65" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm ol ns nt nu bi translated"><a class="ae ln" href="https://devblogs.nvidia.com/object-detection-pipeline-gpus" rel="noopener ugc nofollow" target="_blank">为GPU博客创建对象检测管道</a></li><li id="9c18" class="nm nn iq kt b ku nv kx nw la nx le ny li nz lm ol ns nt nu bi translated">NVIDIA NGC上的<a class="ae ln" href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt" rel="noopener ugc nofollow" target="_blank">tensort容器</a>和<a class="ae ln" href="https://ngc.nvidia.com/catalog/models?orderBy=modifiedDESC&amp;query=tensorrt&amp;quickFilter=&amp;filters=" rel="noopener ugc nofollow" target="_blank">tensort计划文件</a></li></ul><p id="7b25" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们希望你喜欢阅读这篇文章，就像我们喜欢开发它一样。交给你了，你是怎么用GPU进行推理的？</p><p id="0909" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们一直在为博客和教程寻找很酷的应用程序创意。请在下面留言，告诉我们你觉得最具挑战性的是什么。</p><p id="ad67" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果您在使用此应用程序时遇到问题，请务必查看此示例的<a class="ae ln" href="https://github.com/NVIDIA/object-detection-tensorrt-example" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中的问题，了解类似的问题和解决方案。</p><p id="f2a1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果您对使用TensorRT有任何疑问，请随时查看<a class="ae ln" href="https://devtalk.nvidia.com/default/board/304/" rel="noopener ugc nofollow" target="_blank"> NVIDIA TensorRT开发者论坛</a>，看看TensorRT社区的其他成员是否有解决方案。NVIDIA注册开发者程序也可以在https://developer.nvidia.com/nvidia-developer-program的<a class="ae ln" href="https://developer.nvidia.com/nvidia-developer-program" rel="noopener ugc nofollow" target="_blank">提交bug。</a></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="bd94" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><p id="5649" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">【刘等2016】刘，魏，【SSD:单次多盒探测器。“欧洲计算机视觉会议。施普林格，查姆，2016。</p><p id="cbd8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">【Szegedy等人2016】Szegedy，Christian等人】<a class="ae ln" href="https://arxiv.org/abs/1512.00567" rel="noopener ugc nofollow" target="_blank">重新思考计算机视觉的盗梦空间架构。</a>《IEEE计算机视觉和模式识别会议论文集》。2016.</p><p id="d33e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">【林等2014】林，宗毅等.<a class="ae ln" href="https://arxiv.org/abs/1405.0312" rel="noopener ugc nofollow" target="_blank">微软COCO:情境中的公共对象。</a>“欧洲计算机视觉会议。施普林格，查姆，2014年。</p><blockquote class="om on oo"><p id="3a54" class="kr ks oc kt b ku kv jr kw kx ky ju kz op lb lc ld oq lf lg lh or lj lk ll lm ij bi translated">NVIDIA作者:解决方案架构师加里·伯内特和产品营销经理西达尔特·夏尔马</p></blockquote></div></div>    
</body>
</html>