<html>
<head>
<title>Blur Detection via GPU-Accelerated Metal Performance Shaders on iOS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过iOS上GPU加速的金属性能着色器进行模糊检测</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/blur-detection-via-metal-on-ios-16dd02cb1558?source=collection_archive---------4-----------------------#2019-10-12">https://betterprogramming.pub/blur-detection-via-metal-on-ios-16dd02cb1558?source=collection_archive---------4-----------------------#2019-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d5f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用ARKit的更改图像跟踪功能通过相机跟踪矩形对象，并实时转换它们</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/16273a9fefbd893b9c5f5f2dfeb2bc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMMRrHM1TfqAV6MJ-GbeCA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@pawelskor?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">保罗·斯科鲁普斯卡斯</a>在<a class="ae ky" href="https://unsplash.com/s/photos/focus?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="0685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当在相机帧上进行实时视频或图像处理时，一个令人烦恼的问题是，有时相机可能需要时间来对焦。在此期间，所有捕获的帧都是模糊的，这可能会导致您试图检测或跟踪的任何东西出现误报。因此，将图像的模糊程度作为算法的输入或简单地跳过它可能会有所帮助，从而提高输出的准确性。</p><p id="90f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的情况下，我正在遵循苹果的ARKit样本，名为<a class="ae ky" href="https://developer.apple.com/documentation/arkit/tracking_and_altering_images" rel="noopener ugc nofollow" target="_blank">改变图像跟踪</a>，它跟踪通过相机看到的矩形物体，并实时转换它们。我发现的一个问题是，如果它最初检测到图像失焦，那么即使在相机焦点固定后，它也会继续尝试跟踪模糊的图像，导致图像不匹配，从而导致不可靠的跟踪。这种技术让我可以在开始跟踪之前，快速确保捕捉到的图像对焦。</p><p id="b3f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现Adrian Rosebrock关于这个主题的一个很棒的<a class="ae ky" href="https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/" rel="noopener ugc nofollow" target="_blank">博客帖子</a>，它提出了一个可行的方法，称为拉普拉斯的<em class="lv">方差，我使用金属性能着色器重新实现了这个方法，得到了令人满意的结果，我想在这里分享一下。</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="28e2" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">方法</h1><p id="7010" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">它只需执行两个步骤:</p><ol class=""><li id="3008" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">使用<code class="fe nj nk nl nm b">MPSImageLaplacian</code>将源图像转换为拉普拉斯图像。</li><li id="14fe" class="na nb it lb b lc nn lf no li np lm nq lq nr lu nf ng nh ni bi translated">使用<code class="fe nj nk nl nm b">MPSImageStatisticsMeanAndVariance</code>获得拉普拉斯的方差。</li></ol><p id="67da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果将是一个表示模糊度的数字。数字越低，越模糊。对我来说，0、1或2的方差会太模糊，导致我的算法给我假阳性。方差为3或更高(对于我的测试数据，通常达到5)，我能够得到更好的结果:</p><h2 id="f379" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">方差= 1</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f756ece9fc2669433df26dfc1776873b.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*K6C9Ka0B3Mdl38Wd1EBFmA.png"/></div></figure><h2 id="ee4e" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">方差= 2</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/48762e066f403d19f2c5b4879f090ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*fGJtQsHhYbSZOpET_NILHw.png"/></div></figure><h2 id="1710" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">方差= 5</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d1d441a02519ab1e26c5c8a7555b806d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*9Fgyrfd79xVVADjRyAXp6w.png"/></div></figure><h2 id="1f2b" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">拉普拉斯算子用于方差为4的图像</h2><p id="fd47" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">出于好奇，下面是拉普拉斯算子对聚焦图像的结果。失焦图像的拉普拉斯基本上是一个黑色的矩形。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/cc17411fc3e8aa8f5179821ab6d985be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*3awjpKCaitTqv3XMRMNSAQ.png"/></div></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="9121" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">密码</h1><p id="0912" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">以下是Swift中的一些代码:</p><pre class="kj kk kl km gt oi nm oj ok aw ol bi"><span id="e1f7" class="ns me it nm b gy om on l oo op"><strong class="nm iu">import</strong> Metal<br/><strong class="nm iu">import</strong> MetalPerformanceShaders<br/><strong class="nm iu">import</strong> MetalKit</span><span id="655f" class="ns me it nm b gy oq on l oo op">...<br/><strong class="nm iu">// Initialize MTL</strong><br/>self.mtlDevice = MTLCreateSystemDefaultDevice()<br/>self.mtlCommandQueue = mtlDevice?.makeCommandQueue()<br/>...</span><span id="3af8" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Create a command buffer for the transformation pipeline<br/>let</strong> commandBuffer = <strong class="nm iu">self</strong>.mtlCommandQueue.makeCommandBuffer()!</span><span id="3597" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// These are the two built-in shaders we will use<br/>let</strong> laplacian = MPSImageLaplacian(device: <strong class="nm iu">self</strong>.mtlDevice)<br/><strong class="nm iu">let</strong> meanAndVariance = MPSImageStatisticsMeanAndVariance(device: <strong class="nm iu">self</strong>.mtlDevice)</span><span id="ca86" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Load the captured pixel buffer as a texture<br/>let</strong> textureLoader = MTKTextureLoader(device: <strong class="nm iu">self</strong>.mtlDevice)<br/><strong class="nm iu">let</strong> sourceTexture = <strong class="nm iu">try</strong>! textureLoader.newTexture(cgImage: referenceImagePixelBuffer.toCGImage()!, options: nil)</span><span id="cd1d" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Create the destination texture for the laplacian transformation<br/>let</strong> lapDesc = MTLTextureDescriptor.texture2DDescriptor(pixelFormat: sourceTexture.pixelFormat, width: sourceTexture.width, height: sourceTexture.height, mipmapped: <strong class="nm iu">false</strong>)<br/>lapDesc.usage = [.shaderWrite, .shaderRead]<br/><strong class="nm iu">let</strong> lapTex = <strong class="nm iu">self</strong>.mtlDevice.makeTexture(descriptor: lapDesc)!</span><span id="ca01" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Encode this as the first transformation to perform</strong><br/>laplacian.encode(commandBuffer: commandBuffer, sourceTexture: sourceTexture, destinationTexture: lapTex)</span><span id="2ac6" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Create the destination texture for storing the variance.<br/>let</strong> varianceTextureDescriptor = MTLTextureDescriptor.texture2DDescriptor(pixelFormat: sourceTexture.pixelFormat, width: 2, height: 1, mipmapped: <strong class="nm iu">false</strong>)<br/>varianceTextureDescriptor.usage = [.shaderWrite, .shaderRead]<br/><strong class="nm iu">let</strong> varianceTexture = <strong class="nm iu">self</strong>.mtlDevice.makeTexture(descriptor: varianceTextureDescriptor)!</span><span id="e66f" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Encode this as the second transformation</strong><br/>meanAndVariance.encode(commandBuffer: commandBuffer, sourceTexture: lapTex, destinationTexture: varianceTexture)</span><span id="cb8d" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// Run the command buffer on the GPU and wait for the results</strong><br/>commandBuffer.commit()<br/>commandBuffer.waitUntilCompleted()</span><span id="4419" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">// The output will be just 2 pixels, one with the mean, the other the variance.<br/>var</strong> result = [Int8](repeatElement(0, count: 2))<br/><strong class="nm iu">let</strong> region = MTLRegionMake2D(0, 0, 2, 1)<br/>varianceTexture.getBytes(&amp;result, bytesPerRow: 1 * 2 * 4, from: region, mipmapLevel: 0)</span><span id="e6cf" class="ns me it nm b gy oq on l oo op"><strong class="nm iu">let</strong> variance = result.last!</span></pre></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="9d31" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论</h1><p id="584b" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这种检查图像是否模糊的方法是GPU加速的，对于大约1000 x 600的图像，在iPhone 8上需要三到九毫秒。</p></div></div>    
</body>
</html>