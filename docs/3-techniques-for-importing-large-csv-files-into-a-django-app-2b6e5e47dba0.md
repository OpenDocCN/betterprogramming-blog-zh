# 将大型 CSV 文件导入 Django 应用程序的 3 个技巧

> 原文：<https://betterprogramming.pub/3-techniques-for-importing-large-csv-files-into-a-django-app-2b6e5e47dba0>

## 将大量数据加载到数据库变得更加容易

![](img/23f23e84a346c83323a767e7061a1623.png)

图片由作者提供。

# **问题概述和 App 配置**

通常情况下，您希望将数据从 CSV 文件加载到数据库中。通常，这根本不是问题，但是在某些情况下可能会出现性能问题，尤其是当您想要加载大量数据时。在这种情况下，“海量”意味着 CSV 文件具有 500MB 到 1GB 的数据和数百万行。

在本文中，我将重点关注无法使用数据库实用程序加载 CSV 文件的情况(如 PostgreSQL `COPY`)，因为您需要在这个过程中进行转换。

此外，值得注意的是，这种规模的数据负载应该总是受到质疑，您应该尝试找到更合适的方法来完成它。经常检查是否可以使用数据库引擎实用程序(如`COPY`)将数据直接复制到数据库中。这些类型的操作几乎总是比使用 ORM 和您的应用程序代码更高效。

假设我们有两种型号:`Product`和`ProductCategory`。我们从不同的组织部门获取数据，并且必须将数据加载到系统中。我们的 Django 模型将如下所示:

数据结构非常简单，但足以显示大量数据负载的问题。这里值得注意的一点是`Product`和`ProductCategory`的关系。在这种情况下，我们可以预计产品类别的数量将比产品数量低几个数量级。我们以后会用到这些知识。

我们还需要一个 CSV 文件的生成器。CSV 文件包含以下各列:

*   `product_name`
*   `product_code`
*   `price`
*   `product_category_name`
*   `product_category_code`

使用上面的脚本，您可以用我们进行负载测试所需的数据创建一个 CSV 文件。调用参数时可以传递一个数字，这将是生成的文件中的行数:

```
python3 csv_mock_data_create.py 10000
```

上面的命令将创建一个包含 10，000 个产品的文件。注意，脚本现在跳过了 CSV 文件头。我稍后将回到这一点。

这里要小心，因为 1000 万行将创建一个大约 600MB 的文件。

现在我们只需要一个简单的 Django 管理命令来加载文件。我们不会通过视图来完成，因为正如我们已经知道的，文件非常大。这意味着我们将需要使用一个请求处理程序来上传大约 500MB 的文件，并因此将文件加载到内存中。这是低效的。

该命令现在有了数据加载的简单实现，还显示了处理 CSV 文件所需的时间:

```
python3 manage.py load_csv /path/to/your/file.csv 
```

对于 200 个产品，上面的代码在 0.220191 秒内被执行。对于 100，000 件产品，耗时 103.066553 秒。一百万件产品可能需要十倍的时间。我们能让它更快吗？

# **1。不要将整个文件加载到内存中**

首先要注意的是，上面的代码将整个 CSV 加载到内存中。更有趣的是，它做了两次。这两句台词真的很烂:

```
data = list(csv.reader(csv_file, delimiter=","))
for row in data[1:]:
    ...
```

试图跳过这样的头处理是一个常见的错误。代码从列表中的第二个元素开始迭代，但是`csv.reader`是一个迭代器，这意味着它是内存高效的。如果程序员强制进行`list`转换，那么 CSV 文件将被加载到一个列表中，从而加载到进程的内存中。在没有足够 RAM 内存的情况下，这可能是一个问题。当在`for`循环中使用`data[1:]`时，完成数据的第二次复制。那么我们该如何处理呢？

```
data = csv.reader(csv_file, delimiter=",")
next(data)
for row in data:
    ...
```

调用`next`会将迭代器移动到下一项，我们将能够跳过一个 CSV 文件头(在大多数情况下，处理时不需要它)。此外，该进程的内存占用将会低得多。这个变化对执行时间没有大的影响(可以忽略不计)，但是对进程使用的内存有很大的影响。

# **2。迭代**时不要进行不必要的查询

我特别指的是这条线:

```
product_category = ProductCategory.objects.get_or_create(name=row[3], code=row[4])
```

我们在这里获取的是使用类别名称和代码的每个循环上的`ProductCategory`实例。我们如何解决这个问题？

我们可以在`for`循环之前加载类别，只有当它们不在数据库中时才添加它们:

仅这一项变化就将 100，000 件产品的生产时间缩短了 34 秒(约 30%)。该命令在更改后的 69 秒执行。

# **3。不要一次保存一个元素**

当我们创建`Product`的实例时，我们要求数据库提交每个循环中的更改:

```
Product.objects.create(
    name=row[0],
    code=row[1],
    price=row[2],
    product_category=product_category
)
```

这是每个循环的 I/O 操作。它一定很贵。由于它非常快，这里的问题是可能有数百万次这样的操作，我们可以显著减少这样的操作的数量。怎么会？通过使用 Django 的`bulk_create`方法:

这种变化产生了巨大的影响。对于 100，000 个产品，该命令只需 3.5 秒即可执行。你需要记住最后一个循环在`products`列表中仍然可以有条目(在我们的例子中少于 5000 个)。这需要在循环之后处理:

```
if products:
    Product.objects.bulk_create(products)
```

我们共同做出的这三项更改让我们将命令的性能提高了 96%以上。代码很重要。好的代码更重要。最后的命令如下所示:

用上面的代码，30 秒就加载了 100 万个产品！

# **专业提示:使用多进程**

另一个提高大型 CSV 加载速度的方法是使用多重处理，我在这里只介绍这个方法。在上面的命令中，您可以将一个大的 CSV 文件分割成多个较小的文件(最好的方法是尝试使用行索引),并将每批工作放在一个单独的进程下。如果您可以在一台机器上使用多个 CPU，那么扩展将是线性的(2 个 CPU—快 2 倍，4 个 CPU—快 4 倍)。

假设您有一百万行要处理。然后，第一个进程可以获取编号为`0`–`99999`的行，第二个进程获取编号为`100000`–`199999`的行，以此类推，直到最后一个进程获取编号为`900000`–`999999`的行。

这里唯一的缺点是你需要有十个空闲的 CPU。

# **总结**

*   您应该避免将文件加载到内存中。请改用迭代器。
*   如果您正在逐行处理文件，请避免在`for`循环体中查询数据库。
*   不要在每个循环中保存一个元素。使用`bulk_create`方法。

感谢阅读！