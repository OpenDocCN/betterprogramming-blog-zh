<html>
<head>
<title>A Deep Dive Into Residual Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究剩余神经网络</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/a-deep-dive-into-residual-neural-networks-5d5b3ef60d21?source=collection_archive---------12-----------------------#2020-10-19">https://betterprogramming.pub/a-deep-dive-into-residual-neural-networks-5d5b3ef60d21?source=collection_archive---------12-----------------------#2020-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e37a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“图像识别的深度剩余学习”一文综述</h2></div><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="ab gu cl kk"><img src="../Images/36c3034de788d07f5d1f7b1c1ccc1ebc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*S3TlG0XpQZSIpoDIUCQ0RQ.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片来源:<a class="ae kr" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p></figure><p id="bc73" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在这篇博客中，我将向您介绍<strong class="ku ir"> ResNet </strong>架构，并总结其论文<a class="ae kr" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">“图像识别的深度残差学习”</a> (PDF)。我将解释它的来源以及这种架构背后的思想，所以让我们开始吧！</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="4e36" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">介绍</h1><p id="fed9" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">当ResNet论文发布时(2015年)，人们开始尝试建立越来越深的神经网络。这是因为它提高了<strong class="ku ir"> ImageNet竞赛</strong>的准确性，这是一场在超过1400万张图像的数据集上进行的视觉对象识别竞赛。</p><p id="543d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">但是在某一点上，随着神经网络变大，精确度停止变好。这时候ResNet出来了。人们知道增加神经网络的深度可以使它更好地学习和概括，但也更难训练它。问题不在于过度拟合，因为当训练误差较低时，测试误差不会上升。所以才发明了<strong class="ku ir">残块</strong>。来看看背后的想法吧！</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/6bc0ae06117ac37c4c5b8b6cef7cbdd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8nfHRhjeNur9-UuKx5crg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图一。</p></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="06d5" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">ResNet架构背后的理念</h1><p id="411b" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">ResNet架构背后的想法是，我们至少应该能够通过复制浅层神经网络的层(例如，具有五层的神经网络)并向其中添加学习<strong class="ku ir">身份函数</strong>的层(即，不改变输出的层，称为<strong class="ku ir">身份映射</strong>)来训练更深层的神经网络。问题在于，让图层学习身份函数非常困难，因为大多数权重都是在零附近初始化的，或者使用权重衰减/l2正则化等技术时会趋于零。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mx"><img src="../Images/c75e6311affcab112b3aece897b989f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d85Jf-fEHmN1Wzo7.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图二。正态分布</p></figure><h2 id="bb38" class="my lw iq bd lx mz na dn mb nb nc dp mf lb nd ne mh lf nf ng mj lj nh ni ml nj bi translated">剩余连接</h2><p id="a779" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated"><strong class="ku ir">不是试图让层学习identity函数，而是让前一层的输入默认保持不变</strong>，我们只学习需要改变的内容。因此，每个函数都不需要学习很多东西，基本上都是相同的函数。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nk"><img src="../Images/46d04d8ac30226b55467205d0ac44cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*A7igJ9nTahpbQGiJjU-M7A.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图3。残余块</p></figure><p id="2488" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在图3中，F(x)表示需要改变x，即输入。正如我们前面所说的，<strong class="ku ir">权重趋于零，所以F(x) + x就变成了恒等函数！</strong></p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="a97c" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">VGG-19模型和残差网络的比较</h1><p id="271f" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">VGG-19模型有很多参数，需要大量的计算(向前传递需要196亿次浮点运算！)与具有34个参数层的残差神经网络的36亿次浮点运算相反。残差网络中的层小于VGG-19模型。也有<strong class="ku ir">更多的层，但他们不必学习很多所以参数的数量更小</strong>。残差神经网络也使用两个的<strong class="ku ir">步距，而不是执行汇集操作。通过增加层，简单的34层普通神经网络实际上失去了性能，但通过增加<strong class="ku ir">跳过连接</strong>解决了这个问题。</strong></p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2b9613d2b6e8e9a5abf3ff8363d5f432.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*Agr5BfVtGOfe8lYBYiQlfA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图4。ResNet架构</p></figure><p id="57cb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">添加<strong class="ku ir">跳过连接</strong>产生了另一个问题，在步幅2的每个卷积之后，输出是之前的一半大小，同时下一个卷积中的滤波器数量是之前的两倍。</p><p id="ce5c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">那么，我们如何处理这个问题，让身份功能发挥作用呢？本文探讨了解决这一问题的三种思路。一个是添加零填充，第二个是<strong class="ku ir">添加1x1卷积到那些特定的连接(虚线的)</strong>，最后一个是添加1x1卷积到每个连接。更受欢迎的想法是第二个，因为第三个与第二个相比没有太大的改进，而且增加了更多的参数。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nm"><img src="../Images/205971852e1d18e6bc3fcc4e19458b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bz4Y-COOVbSzffRXexdS7w.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图5。</p></figure><p id="b5c6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如图5所示。更深层次的架构比18层架构的性能更好，这与左侧显示普通18层和普通34层架构的图表相反。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="c479" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">深层残差神经网络</h1><p id="0783" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">随着神经网络越来越深入，计算变得更加昂贵。为了解决这个问题，他们引入了一个“<strong class="ku ir">瓶颈块。</strong>“它有三层，两层1x1卷积，第三层3x3卷积。</p><p id="cd57" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">第一个1x1层负责减少维度，最后一个负责恢复维度，留下具有较小输入/输出维度的3x3层，并降低其复杂性。添加1x1层不成问题，因为它们的计算强度比3x3层低得多。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6d1c5f857595554464a22ab5f76caddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*iS9CGjSby_RUfb8q2iYaaA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图6。右侧的“瓶颈”</p></figure><p id="394d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如图7所示。，他们能够训练56层甚至110层的剩余神经网络，这在这篇论文发表之前是从未见过的。在尝试了非常大量的层1202之后，由于过拟合，精度最终下降。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fbb964b88552b49666856f90de5c0600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*4kVfrnzl6RG4vFHtNpzs2w.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图7。</p></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="94ad" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">结论</h1><p id="5827" class="pw-post-body-paragraph ks kt iq ku b kv mn jr kx ky mo ju la lb mp ld le lf mq lh li lj mr ll lm ln ij bi translated">“用于图像识别的深度剩余学习”论文在发布时是深度学习的一大突破。它引入了具有50层甚至更多层的大型神经网络，并表明随着神经网络变得更深而没有太多参数(比我们之前谈论的VGG-19模型少得多)，有可能提高ImageNet的准确性。</p><p id="4289" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">可以<a class="ae kr" href="https://github.com/aurelien-peden/Deep-Learning-paper-implementations/blob/master/resnet_tensorflow.py" rel="noopener ugc nofollow" target="_blank">在我的GitHub </a>上用TensorFlow检查ResNet架构的实现！你可以<a class="ae kr" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">点击这个链接</a>阅读这篇论文。</p><p id="d738" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">感谢你阅读这篇文章，我希望这个摘要能帮助你理解这篇文章。敬请关注即将到来的深度学习教程。</strong></p></div></div>    
</body>
</html>