<html>
<head>
<title>Fine-tuning GPT-J 6B on Google Colab or Equivalent Desktop or Server GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google Colab或同等的桌面或服务器GPU上微调GPT-J 6B</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/fine-tuning-gpt-j-6b-on-google-colab-or-equivalent-desktop-or-server-gpu-b6dc849cb205?source=collection_archive---------1-----------------------#2022-06-17">https://betterprogramming.pub/fine-tuning-gpt-j-6b-on-google-colab-or-equivalent-desktop-or-server-gpu-b6dc849cb205?source=collection_archive---------1-----------------------#2022-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2a43" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">微调艾路瑟·艾的模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f921efd06d9426b7e725079d57427999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TpSwpWuyjanmiZkX0wE20g.png"/></div></div></figure><p id="1e88" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">自然语言处理(NLP)已经通过最近的研究和来自OpenAI(GPT-2和GPT-3)、Eleuther AI (GPT-NEO和GPT-J)和谷歌(BERT，T5，PaLM)等公司的新的基于转换器的大型语言模型(LLM)的发布而发生了革命性的变化。</p><p id="12a2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">大型语言模型是基于transformer架构的生成式深度学习模型。它们似乎主导了绝大多数常见的NLP任务，如摘要、文本生成、关键字和命名实体提取、对话聊天机器人、语言翻译、文章或博客文章写作、生成计算机代码、编写智能合同等等。</p><p id="fb4b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">值得注意的是，他们可以执行传统NLP模型所做的所有这些任务，但只需一个模型，无需任何特定任务或监督培训。</p><p id="21a4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">他们通常只需要很少甚至不需要例子就能理解给定的任务(包括他们从未接受过训练的任务),并且表现优于以监督方式训练的最先进的模型。</p><p id="6bbd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GPT-3和GPT-J是当今最流行和最容易访问的大型语言模型。</p><h1 id="aa23" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">什么是GPT-3和GPT-J？</h1><h2 id="f06a" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">GPT-3</h2><p id="a0d6" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">根据维基百科的说法，<strong class="kt ir"> GPT-3 </strong>是一个<a class="ae mw" href="https://en.wikipedia.org/wiki/Autoregressive_model" rel="noopener ugc nofollow" target="_blank">自回归</a> <a class="ae mw" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>，它使用<a class="ae mw" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>产生类似人类的文本。它是一个通用的学习者，这意味着它没有被专门训练来做任何一件事。但是，它非常擅长翻译文本、回答问题、总结段落和生成文本输出。</p><p id="6c33" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">基本上，你可以通过以提示的形式提供一些任务的例子来指示GPT-3执行任何基于文本的任务，然后它可以学习出色地执行该任务，以便当给定输入时，它将生成适当或正确的输出。GPT三号拥有1750亿个参数，将于2020年向公众展示。尽管它的源代码从未向公众公开，但对<a class="ae mw" href="https://en.wikipedia.org/wiki/GPT-3" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>的访问是通过<a class="ae mw" href="https://en.wikipedia.org/wiki/Microsoft" rel="noopener ugc nofollow" target="_blank">微软</a>提供的<a class="ae mw" href="https://en.wikipedia.org/wiki/Application_programming_interface" rel="noopener ugc nofollow" target="_blank"> API </a>独家提供的。<br/>它是GPT-2(最大的模型有15亿个参数)的继任者，后者也是由OpenAI在2019年2月创建的，GPT-2则是open ai 2018年GPT模型的“直接放大”和继任者。</p><p id="3cb3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GPT-3有一些限制:</p><ul class=""><li id="b97a" class="mx my iq kt b ku kv kx ky la mz le na li nb lm nc nd ne nf bi translated">非常大的模型无法装入高端PC甚至昂贵的GPU服务器。</li><li id="24c6" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">它非常昂贵，只能通过<a class="ae mw" href="https://openai.com/api/" rel="noopener ugc nofollow" target="_blank"> OpenAI API </a>访问。</li><li id="4c4c" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">缺乏透明度</li></ul><h2 id="393a" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">GPT J</h2><p id="c2fc" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">GPT-J 6B是由一个名为Eleuther AI的非营利研究组织(成立于2020年7月)发布的60亿参数模型。Eleuther AI是一个分散的志愿者研究人员、工程师和开发人员的集体，专注于人工智能对齐、缩放和开源人工智能研究。<br/> GPT-J在<a class="ae mw" href="https://pile.eleuther.ai" rel="noopener ugc nofollow" target="_blank">桩数据集</a>上接受训练。</p><p id="79ac" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该组织的目标是民主化，建立和开源大型语言模型。因此，他们发布了GPT-J 6B和其他型号(<a class="ae mw" href="https://github.com/EleutherAI/gpt-neo" rel="noopener ugc nofollow" target="_blank">GPT-尼奥</a>)目前公开发售。另一方面，由OpenAI发布的GPT3具有1750亿个参数，并且不向公众公开。<br/>尽管在参数数量上存在巨大差距，但事实证明，GPT-J在代码生成任务和聊天机器人对话等一些领域的表现优于GPT3。有关它们的比较信息，请查看此处的<a class="ae mw" href="https://www.ankursnewsletter.com/p/openais-gpt-3-vs-open-source-alternatives?s=r" rel="noopener ugc nofollow" target="_blank">和</a></p><p id="afff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GPT-J通常比OpenAI的GPT-3模型(Ada和Babbage)的较小版本性能更好，但不如达芬奇(GPT-3最强大和最昂贵的模型)。</p><p id="c8bc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">任何人都可以在服务器上免费使用和部署来自Eleuther AI的GPT-J、GPT-NeoX 20B(最近发布)和其他大型语言模型。</p><h2 id="8a36" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">微调</h2><p id="9eb8" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">虽然很明显，一个模型的参数越多，它的性能通常就越好，但在探索微调时，这个规则有一个例外。微调是指针对特定任务或用例，在数据集上进一步训练基于transformer的语言模型的实践。</p><p id="de0d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用这种强大的技术，GPT-J可以在一些特定的任务上轻松超越甚至是最强大的GPT-3型达芬奇。事实上，微调是一种强大而宝贵的技术，不仅用于提高语言模型或NLP的性能，还用于深度学习和机器学习的其他领域，如计算机视觉、文本到语音(TTS)、语音到文本(STT)、GANs、图像字幕等。</p><p id="2f77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，在这篇文章中，我们将专注于微调Eleuther AI的GPT-J 6B模型的一个特殊版本，它被称为<strong class="kt ir">量子化的EleutherAI/gpt-j-6b，具有8位权重。</strong>它是由<a class="ae mw" href="https://huggingface.co/hivemind" rel="noopener ugc nofollow" target="_blank"> Hivemind </a>修改和开发的，这篇文章改编了大部分代码。</p><p id="79ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于微调原始或普通GPT-J 6B的教程，<a class="ae mw" href="https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md" rel="noopener ugc nofollow" target="_blank">查看伊柳瑟的指南。</a></p><h1 id="1e99" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">使用您的自定义数据集在google colab上微调GPT-J-6B:使用低秩适配器的8位权重(LoRA)</h1><p id="5973" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">用于微调的概念验证笔记本在此处<a class="ae mw" href="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es" rel="noopener ugc nofollow" target="_blank">可用</a>，用于推理的笔记本也在此处<a class="ae mw" href="https://colab.research.google.com/drive/1m3KQYva980cQnRoycCMAMEEcAyeallZJ" rel="noopener ugc nofollow" target="_blank">可用</a>。</p><p id="edb1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你想关注GitHub上的讨论，你可以查看这里的<a class="ae mw" href="https://github.com/huggingface/transformers/issues/14839" rel="noopener ugc nofollow" target="_blank"/></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/b373918fe2148417ce52e6f844cb3bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jTZlTBRR8QUiAdPDSNxJA.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">在colab上安装依赖项</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/fc34aebbb2b6ad6642d7624a6216a2db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aYtoJ7ET75aPQ7rOoVT_w.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">在colab上微调60亿GPT J</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/a170f38ca46caa5022080f251b76744c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwSJIerhegEG9IzwWk5KsQ.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">在colab上微调60亿GPT J</p></figure><p id="e924" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你很好奇，想深入了解内部工作原理和细节，你应该看看<a class="ae mw" href="https://huggingface.co/hivemind/gpt-j-6B-8bit" rel="noopener ugc nofollow" target="_blank">模型卡</a>，它有更详细的解释和辅助笔记本(如模型转换和困惑检查)。</p><h1 id="73e0" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">模型描述</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/825b1ea3c845e111871a1fbbf856eb1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oN0JMWDBT_O0wvsgP7Zw2g.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">信用<a class="ae mw" href="https://huggingface.co/hivemind/gpt-j-6B-8bit" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/hivemind/gpt-j-6B-8bit</a></p></figure><p id="44d6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个修改后的版本使你能够在google colab或者一台配有高端GPU ( <strong class="kt ir">例如single 1080Ti)的PC上生成并微调模型。</strong></p><p id="c294" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"/><a class="ae mw" href="https://huggingface.co/EleutherAI/gpt-j-6B/tree/main" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">原厂GPT-J </strong> </a>光是float32参数就超过22+ GB内存，再加上渐变，和优化器。即使你将整个模型转换为16位，它仍然不适合大多数单GPU设置，如V100、A6000、英伟达GTX 1080-Ti和<strong class="kt ir"> </strong> A100。你也许可以用它在CPU或TPU上进行推理，但是微调在计算上要昂贵得多。</p><p id="a659" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">应用了几项巧妙的技术，使GPT-J 6B可以在一个大约11 GB内存的GPU上使用和微调:</p><ul class=""><li id="4b79" class="mx my iq kt b ku kv kx ky la mz le na li nb lm nc nd ne nf bi translated">将模型转换为8位:这是使用facebook的<code class="fe nt nu nv nw b"><a class="ae mw" href="https://github.com/facebookresearch/bitsandbytes" rel="noopener ugc nofollow" target="_blank">bitsandbytes</a></code>库完成的。这将模型的大小从20Gb减少到6Gb。大权重张量使用动态8位量化进行量化，并在乘法运算时及时解量化。(线性图层偏差未转换为8位，因为它们只占模型权重的不到1%)</li><li id="79a8" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">使用梯度检查点来存储每层仅一个激活:使用显著更少的内存，代价是训练速度慢30%。</li><li id="b982" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">利用<a class="ae mw" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank"> LoRA </a>和<a class="ae mw" href="https://arxiv.org/abs/2110.02861" rel="noopener ugc nofollow" target="_blank"> 8位Adam </a>进行可扩展微调</li></ul><p id="cabb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">换句话说，所有大的权重矩阵都被冻结在8位，你只训练小的适配器和可选的一维张量(层标，偏差)。</p><p id="a86e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我敢肯定，此时您的脑海中一定会有这样一个问题，<strong class="kt ir">8位模型会影响模型质量吗？</strong>技术上可以，但实际上效果可以忽略不计，量化模型甚至稍好，但没有定性意义。一些研究和调查证实了这一点，你可以在这里看一看<a class="ae mw" href="https://nbviewer.org/urls/huggingface.co/hivemind/gpt-j-6B-8bit/raw/main/check_perplexity.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="ef48" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，在性能方面，根据使用的GPU和批量大小，量化模型比原始模型慢1-10%左右。</p><p id="631b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，去量化权重的开销并不取决于批量大小，因此，你能适应的批量越大，你训练的效率就越高。</p><h2 id="829b" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">开源状态</h2><ul class=""><li id="9399" class="mx my iq kt b ku mr kx ms la nx le ny li nz lm nc nd ne nf bi translated">此处提供了模型实现<a class="ae mw" href="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es" rel="noopener ugc nofollow" target="_blank"/></li><li id="63cb" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">模型重量可用<a class="ae mw" href="https://huggingface.co/hivemind/gpt-j-6B-8bit" rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h2 id="f0e4" class="mf lo iq bd lp mg mh dn lt mi mj dp lx la mk ml lz le mm mn mb li mo mp md mq bi translated">作者:</h2><ul class=""><li id="505f" class="mx my iq kt b ku mr kx ms la nx le ny li nz lm nc nd ne nf bi translated">最初的J-6B 由<a class="ae mw" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank">艾</a>训练(引用:王贲和小松崎阿然)</li><li id="b8a3" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">由<a class="ae mw" href="https://github.com/TimDettmers" rel="noopener ugc nofollow" target="_blank"> Tim Dettmers </a>从<a class="ae mw" href="https://github.com/facebookresearch/bitsandbytes" rel="noopener ugc nofollow" target="_blank">位和字节</a>进行快速量化</li><li id="3ebd" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">胡等人<a class="ae mw" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">(2021)</a>提出了类模型的低阶适配器</li><li id="7808" class="mx my iq kt b ku ng kx nh la ni le nj li nk lm nc nd ne nf bi translated">这个笔记本是我(<a class="ae mw" href="https://github.com/deniskamazur" rel="noopener ugc nofollow" target="_blank"> @deniskamazur </a>)在Yozh ( <a class="ae mw" href="https://github.com/justheuristic" rel="noopener ugc nofollow" target="_blank"> @justheuristic </a>)的帮助下完成的</li></ul><p id="a7a6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你可以在colab和其他免费的GPU提供商上进行微调:<a class="ae mw" href="https://towardsdatascience.com/amazon-sagemaker-studio-lab-a-great-alternative-to-google-colab-7194de6ef69a" rel="noopener" target="_blank"> kaggle </a>、<a class="ae mw" href="https://towardsdatascience.com/amazon-sagemaker-studio-lab-a-great-alternative-to-google-colab-7194de6ef69a" rel="noopener" target="_blank"> aws sagemaker </a>，或者<a class="ae mw" href="https://docs.paperspace.com/gradient/more/instance-types/free-instances" rel="noopener ugc nofollow" target="_blank"> paperspace </a>。</p><h1 id="2432" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">使用您的自定义数据集在Paperspace上微调GPT-J-6B:使用低秩适配器(LoRA)的8位权重</h1><p id="f3e3" class="pw-post-body-paragraph kr ks iq kt b ku mr jr kw kx ms ju kz la mt lc ld le mu lg lh li mv lk ll lm ij bi translated">我能够在<a class="ae mw" href="https://docs.paperspace.com/gradient/more/instance-types/free-instances" rel="noopener ugc nofollow" target="_blank"> paperspace </a>上微调GPT-J 6B 8位。我用几个支持GPU的服务器进行了实验。我能够用5000在50分钟内微调一个大约2MB的数据集(由10k个单词的文章组成),用A6000在20分钟内微调。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/18e848e30f548ec649df7650e867163e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTKjYFjm2myYJYmb2KWOsA.png"/></div></div></figure><p id="af9b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要优化8位GPT-J-6B，您所需要的只是一组训练和测试示例，这些示例格式化为多个文本文件，每个示例通常由单个输入示例及其相关输出组成。格式化数据集的最佳方式将取决于您的特定用例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/b1aa20118f91498e7c33f19af1293a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1t0_SpFUgQcRAzYjkT6VYQ.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">微调代码</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/5eff44910072b27e78089f26d74ab515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nd3lZ5N0imt_cNyhGQMEdQ.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">图纸空间的微调</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/dbe7eab6217914ec0d51879af4bc3ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I2WnGyD7P7-jH5kPSEH26A.png"/></div></div><p class="nm nn gj gh gi no np bd b be z dk translated">图纸空间的微调</p></figure><p id="42df" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里有一个<a class="ae mw" href="https://github.com/sleekmike/Finetune_GPT-J_6B_8-bit" rel="noopener ugc nofollow" target="_blank">链接</a>来访问我用来在纸上微调GPT-J 6B 8位的代码。<a class="ae mw" href="https://github.com/sleekmike/Finetune_GPT-J_6B_8-bit" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>还包含一个简单API的代码，该API是用FastAPI构建的，用于执行模型推理。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="775a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我希望你喜欢这篇文章，并发现它很有用。</p><p id="8a8f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">特别感谢<a class="ae mw" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank">艾</a>和<a class="ae mw" href="https://huggingface.co/hivemind" rel="noopener ugc nofollow" target="_blank"> Hivemind </a>。</p><pre class="kg kh ki kj gt ol nw om on aw oo bi"><span id="78f4" class="mf lo iq nw b gy op oq l or os">Connect with me on <a class="ae mw" href="https://www.linkedin.com/in/michael-ohanu-b69481a8/" rel="noopener ugc nofollow" target="_blank">Linkedin</a> , <a class="ae mw" href="https://twitter.com/sleekmyk" rel="noopener ugc nofollow" target="_blank">Twitter</a> and <a class="ae mw" href="https://github.com/sleekmike/" rel="noopener ugc nofollow" target="_blank">Github</a>, <a class="ae mw" href="https://www.alphasofttechnologies.ai/" rel="noopener ugc nofollow" target="_blank">website</a>. </span></pre><p id="015d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您是否希望通过尖端的人工智能和机器学习解决方案推动您的业务向前发展？只需看看我们的咨询和开发服务！访问我们的<a class="ae mw" href="https://www.alphasofttechnologies.ai" rel="noopener ugc nofollow" target="_blank">网站</a></p><p id="61ff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们由经验丰富的设计师、软件开发人员、数据科学家和机器学习专家组成的团队可以帮助您利用数据的力量获得宝贵的见解，做出数据驱动的决策，并提供定制的解决方案。</p><p id="b4f5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">无论您是希望构建或微调定制模型，还是优化现有算法，或者在GPT-4、GPT-3、ChatGPT-3、Dalle-3、GPT-J、GPT-NEO、BERT、T5、Stable Diffusion等前沿模型和API的基础上开发全新的应用程序，我们都有专业知识来帮助您实现目标！</p><p id="ffd9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们全面的开发服务包括从设计、数据科学、人工智能、ML和建模到软件开发和云部署的一切。我们将与您密切合作，了解您独特的业务需求，并开发可交付可衡量结果的定制解决方案。</p><p id="0675" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不要让你的数据未被开发或你的想法消失！<br/> <a class="ae mw" href="https://www.alphasofttechnologies.ai/" rel="noopener ugc nofollow" target="_blank">立即联系我们</a>了解有关我们的咨询和开发服务的更多信息，并将您的业务推向下一步！<br/><br/># data science # machine learning # consulting services，# deep learning<strong class="kt ir"># chat GPT # GPT-3 # GPT-4 # LLM # generative ai # cloud # chat bots # NLP</strong></p></div></div>    
</body>
</html>