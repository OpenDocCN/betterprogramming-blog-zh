<html>
<head>
<title>How To Make Your PyTorch Code Run Faster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何让你的PyTorch代码运行得更快</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b?source=collection_archive---------1-----------------------#2021-04-05">https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b?source=collection_archive---------1-----------------------#2021-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="798d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">让您的代码运行速度惊人！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/407d3dc647282c8aa3eed8378c0d4b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cTlESGuUbRkcWo0k"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">珍妮·希尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><p id="742e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PyTorch因其灵活性受到研究人员的高度赞赏，并已进入希望跟上最新突破性研究的主流行业。</p><p id="baa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，如果你是深度学习的实践者，你迟早要和PyTorch面对面。</p><p id="6dfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，我将介绍一些技巧，它们将大大减少PyTorch模型的培训时间。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8f62" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数据加载</h1><p id="e47f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了给我们的模型加载数据，我们使用了<code class="fe mz na nb nc b"><a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">torch.utils.data.DataLoader</a></code>，它在数据集上创建了一个Python iterable。</p><p id="fa7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看它的签名:</p><pre class="kj kk kl km gt nd nc ne nf aw ng bi"><span id="bd8a" class="nh md it nc b gy ni nj l nk nl">DataLoader(dataset, batch_size<strong class="nc iu">=</strong>1, shuffle<strong class="nc iu">=False</strong>, sampler<strong class="nc iu">=None</strong>,<br/>           batch_sampler<strong class="nc iu">=None</strong>, num_workers<strong class="nc iu">=</strong>0, collate_fn<strong class="nc iu">=None</strong>,<br/>           pin_memory<strong class="nc iu">=False</strong>, drop_last<strong class="nc iu">=False</strong>, timeout<strong class="nc iu">=</strong>0,<br/>           worker_init_fn<strong class="nc iu">=None</strong>, <strong class="nc iu">*</strong>, prefetch_factor<strong class="nc iu">=</strong>2,<br/>           persistent_workers<strong class="nc iu">=False</strong>)</span></pre><p id="42e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，引擎盖下有许多东西。为了加快我们的训练，我们需要把注意力集中在两个方面。</p><h2 id="6708" class="nh md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated"><strong class="ak">工人数量</strong></h2><p id="26dc" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">默认情况下，其值为<code class="fe mz na nb nc b">0</code>。那是什么意思？这意味着数据将由运行训练代码的主进程加载。这是非常低效的，因为不是训练你的模型，主过程将只关注加载数据。</p><p id="6059" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有更好的方法。如果我们设置<code class="fe mz na nb nc b">num_workers</code> &gt; <code class="fe mz na nb nc b">0</code>，那么将会有一个单独的进程来处理数据加载。您的数据加载将是异步的(也就是说，它不会干扰模型加载)。</p><p id="1f57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这会给我带来多大的速度提升？</p><p id="109b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">工人越多越好。您可以用不同的<code class="fe mz na nb nc b">num_workers</code>运行一个时期，并测试哪个<code class="fe mz na nb nc b">num_workers</code>值最适合您。</p><p id="5dc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最坏的情况，如果你只是保持<code class="fe mz na nb nc b">num_workers</code> = <code class="fe mz na nb nc b">1</code>，它还是会给你<a class="ae ky" href="https://www.youtube.com/watch?v=9mS1fIYj1So&amp;ab_channel=ArunMallya" rel="noopener ugc nofollow" target="_blank">1.2倍的加速</a>。</p><h2 id="3c76" class="nh md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">pin _存储器</h2><p id="d0e7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><em class="nx">注意:这必须连同</em><code class="fe mz na nb nc b"><em class="nx">num_workers</em></code><em class="nx">&gt;</em><code class="fe mz na nb nc b"><em class="nx">0</em></code><em class="nx">条件一起完成。否则没有速度提升。</em></p><p id="07c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，其默认值为<code class="fe mz na nb nc b">False</code>。</p><p id="4fe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那是什么意思？</p><p id="1e79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CPU到GPU的内存传输是同步的(即，您的模型训练将停止并进行CPU到GPU的内存传输，从而降低您的训练速度)。</p><p id="9004" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您将它设置为<code class="fe mz na nb nc b">True</code>时，这些传输将变成异步的，您的主过程将只关注模型训练。这又是一个推动因素。</p><p id="4386" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提升了多少？</p><p id="e93e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最坏的情况，当<code class="fe mz na nb nc b">num_workers</code> = <code class="fe mz na nb nc b">1</code>、<code class="fe mz na nb nc b">pin_memory</code> = <code class="fe mz na nb nc b">True</code>时，你会得到<a class="ae ky" href="https://www.youtube.com/watch?v=9mS1fIYj1So&amp;ab_channel=ArunMallya" rel="noopener ugc nofollow" target="_blank">一个1.22倍的加速</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="380d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">使用cuDNN自动调优器</h1><p id="110b" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><em class="nx">注意:这仅适用于模型中有卷积的情况。</em></p><p id="d7b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">cuDNN有很多计算卷积的算法。根据您的硬件，不同的算法会执行得更快。但是由于默认情况下不使用自动调优器，所以速度没有提高。</p><h2 id="f073" class="nh md it bd me nm nn dn mi no np dp mm li nq nr mo lm ns nt mq lq nu nv ms nw bi translated">如何打开自动调谐器:</h2><pre class="kj kk kl km gt nd nc ne nf aw ng bi"><span id="0e12" class="nh md it nc b gy ni nj l nk nl">torch.backends.cudnn.benchmark = True</span></pre><p id="5561" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将导致模型的初始训练步骤变慢，因为它将尝试不同的算法并选择最好的一个。不过，总的来说，训练时间会减少。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="81a9" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">使用AMP(自动混合精度)</h1><p id="22e2" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">通常，权重和偏差的值存储在GPU上的32位浮点中。但是并不是所有的内存都被利用了(因为你在某些地方使用32位来存储0)。这不是最佳的。</p><p id="3b7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自动混合精度旨在通过存储不同精度的值来减少GPU内存的浪费。根据需要，您的值可以存储为16位或32位。这个过程称为自动铸造。</p><p id="3e04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，将会发生的一件事是，一个16位变量的梯度也将存储在16位中。但如果它下溢(即梯度太小而无法存储在16位中，在这种情况下，它将变成零)呢？比例因子乘以这样的梯度，将零梯度值推到一个相当大的值，并且不影响训练。这个过程被称为梯度缩放。</p><p id="dc3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将释放GPU中的内存。</p><p id="c374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络训练的基本原则是什么？尽可能使用<em class="nx">更大的批量</em>。</p><p id="a522" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，AMP帮助我们做到了这一点。更大的批量也意味着一个时期内更少的步骤，这又节省了我们的培训时间。</p><p id="2299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是来自<a class="ae ky" href="https://pytorch.org/docs/stable/notes/amp_examples.html" rel="noopener ugc nofollow" target="_blank">官方PyTorch文档</a>的一个小例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="1bc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，为了利用AMP的能力，不需要做太多的改变。但是有一个警告:这将导致稍微差一点的准确性。也就是说，下跌幅度还不足以影响你的最终目标。</p><p id="c711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，速度的提高是真实的。在最坏的情况下，加速至少是2倍(见PyTorch博客)。</p><p id="a8e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于批量大小的变化，你将不得不调整你的学习率。您也可以加入<a class="ae ky" href="https://github.com/Tony-Y/pytorch_warmup#radam-warmup" rel="noopener ugc nofollow" target="_blank">学习率预热</a>和<a class="ae ky" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank">学习率衰减</a>。除此之外，你可以在你的模型中使用<a class="ae ky" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank">重量衰减</a>。</p><p id="26be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，您可以选择专门为较大批量设计的优化器，如<a class="ae ky" href="https://paperswithcode.com/method/lars" rel="noopener ugc nofollow" target="_blank"> LARS </a>、<a class="ae ky" href="https://github.com/btahir/tensorflow-LAMB?utm_source=catalyzex.com" rel="noopener ugc nofollow" target="_blank"> LAMB </a>或<a class="ae ky" href="https://pytorch-optimizer.readthedocs.io/en/latest/_modules/torch_optimizer/novograd.html" rel="noopener ugc nofollow" target="_blank"> NovoGrad </a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b363" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">禁用归一化层后直接卷积的偏差</h1><p id="a88d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如果卷积层之后是归一化层，则偏差的使用无效。为什么？</p><p id="83ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为在归一化中，执行的第一个操作是减去平均值。这消除了偏见的使用。因此，如果你这样做，你只是在你的模型中引入了无用的参数。这毫无理由地减缓了你的训练。</p><p id="fd34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只需为卷积层设置<code class="fe mz na nb nc b">bias=False</code>，然后是归一化层。</p><p id="c0e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将给你一个明确的速度提升，因为你减少了要计算的参数数量。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9c3c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">有效地将你的渐变设置为零</h1><p id="b6bb" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><code class="fe mz na nb nc b">model.zero_grad()</code>是每个PyTorch代码中都有的东西。这会将最后一步的渐变设置为零。这是一个巨大的问题。</p><p id="68d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们调用这个函数时，会启动单独的CUDA内核，从而产生不必要的开销。此外，该功能导致反向传播期间的读+写操作(由于使用了<code class="fe mz na nb nc b">+=</code>操作，读由<code class="fe mz na nb nc b">+</code>触发，写由<code class="fe mz na nb nc b">=</code>触发)。</p><p id="c225" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，有一个更有效的方法来消除你的梯度。你应该<em class="nx">总是</em>使用这个:</p><pre class="kj kk kl km gt nd nc ne nf aw ng bi"><span id="12c7" class="nh md it nc b gy ni nj l nk nl">for param in model.parameters():<br/>    param.grad = None</span></pre><p id="2ee7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么这样更好？它不会为每个变量设置不必要的内存开销。它直接设置梯度(即只进行写操作，与<code class="fe mz na nb nc b">model.zero_grad()</code>不同)。</p><p id="38cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您获得了速度提升，因为每次迭代发生时您都剔除了单个操作(读取操作)。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="48b8" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">禁用最终培训的调试API</h1><p id="2ea2" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">一旦您完成了模型的调试，您应该停止使用所有的调试API，因为它们有很大的开销。</p><p id="c1c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在代码中的导入后添加以下行:</p><pre class="kj kk kl km gt nd nc ne nf aw ng bi"><span id="ecee" class="nh md it nc b gy ni nj l nk nl">torch.autograd.set_detect_anomaly(False)<br/>torch.autograd.profiler.profile(False)<br/>torch.autograd.profiler.emit_nvtx(False)</span></pre><p id="f954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一行警告您当<code class="fe mz na nb nc b">True</code>时，任何梯度将获得NaN或无穷大值。</p><p id="4986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二行告诉你<code class="fe mz na nb nc b">True</code>时CPU和GPU上每个操作花费的时间。</p><p id="1510" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第三行为您的跑步创建一个带注释的时间线，当<code class="fe mz na nb nc b">True</code>时，NVIDIA Visual Profiler (NVP)可以可视化该时间线。</p><p id="da89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦你停止所有这些调试工具，你将再次减少你的培训时间。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ed41" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="e7be" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们看到了如何提高PyTorch代码的速度。这导致更好地利用资源和我们的时间。我也解释了为什么每一招都有效，这样你就不会在黑暗中射箭。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cecf" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><ul class=""><li id="3cc5" class="oa ob it lb b lc mu lf mv li oc lm od lq oe lu of og oh oi bi translated"><a class="ae ky" href="https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf" rel="noopener ugc nofollow" target="_blank">https://NV labs . github . io/ECC v2020-mixed-precision-tutorial/files/szy mon _ MIGA cz-py torch-performance-tuning-guide . pdf</a></li></ul></div></div>    
</body>
</html>