<html>
<head>
<title>An Introduction to Open Domain Question-Answering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">开放领域问答简介</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/question-answering-46b4dec54e4?source=collection_archive---------3-----------------------#2022-12-02">https://betterprogramming.pub/question-answering-46b4dec54e4?source=collection_archive---------3-----------------------#2022-12-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="70dd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用深度学习打开非结构化数据的宝库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/70910bcc8ebe08775b0e3c9ecc1fcdbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eu736n182V66yHFwiiXrKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供，文章最初发布在<a class="ae ky" href="https://www.pinecone.io/learn/question-answering/" rel="noopener ugc nofollow" target="_blank"> Pinecone.io </a></p></figure><p id="6ff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">搜索是全球许多应用程序和公司的重要功能。无论是在制造业、金融业、医疗保健业，还是<em class="lv">几乎</em>任何其他行业，组织都拥有庞大的内部信息和文档库。</p><p id="94c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，许多公司的数据规模意味着信息的组织和可访问性会变得非常低效。对于基于语言的信息，这个问题更加严重。</p><p id="15ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语言是人们交流抽象思想和概念的工具。自然地，想法和概念对于计算机来说更难理解并以有意义的方式存储。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="32aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数组织依靠托管在各种<em class="lv">【内部门户】</em>上的一组基于关键词的搜索界面来处理语言数据。如果做得好，这可以满足某些数据的业务需求。</p><p id="c5d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一个人知道他们在找什么，以及他们需要的信息的关键词和术语，基于关键词的搜索是理想的。当答案的关键词和术语未知时，关键词搜索是不充分的。人们在庞大的文档库中寻找未知的答案是对生产力的一种消耗。</p><p id="c426" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何最小化这个问题？答案在于<em class="lv">语义搜索</em>，特别是语义搜索的问答(QA)风格。</p><p id="43bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语义搜索允许我们基于概念和想法而不是关键词进行搜索。给定一个短语，语义搜索工具从存储库中返回语义最相似的<em class="lv">短语。</em></p><p id="bff5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问答通过使用自然语言问题进行搜索并返回相关文档和特定答案，进一步发展了这一思想。QA旨在尽可能模仿自然语言。</p><p id="1016" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们问一个店员，<em class="lv">“那些美味的、新鲜出炉的、不是饼干但看起来像饼干的东西在哪里？”我们会期待带我们去那些地方的方向。这种自然的对话形式正是QA想要复制的。</em></p><p id="eb07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将介绍不同形式的QA，这些<em class="lv">‘QA栈’</em>的组成部分，以及我们可能在哪里使用它们。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="d137" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">问题解答一览</h1><p id="a0b6" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在我们深入细节之前，让我们先描绘一下QA的高层次图景。</p><p id="13b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们的重点是<em class="lv">开放域</em> QA (ODQA)。ODQA系统处理跨广泛主题的问题，不能依赖于代码中的特定规则。</p><p id="8285" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">开放域</em>的替代方案是<em class="lv">封闭域</em>，它专注于有限的域/范围，而<em class="lv">可以</em>经常依赖于显式逻辑。我们将<strong class="lb iu">而不是</strong>涵盖<em class="lv">封闭领域</em> QA。</p><p id="16cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的剩余部分，我将交替使用ODQA和QA。ODQA模型可以分成几个子类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/d5ade6cf5f1280f43558033f8f1e58b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AUYE9uZShQKgfXcm.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有几种回答问题的方法。</p></figure><p id="7b62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最常见的问答形式是<strong class="lb iu">开卷摘录问答</strong>(上图左上角)。这里我们结合了信息检索(IR)步骤和阅读理解(RC)步骤。</p><p id="e0a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">任何<em class="lv">开卷</em> QA都需要一个IR步骤来<em class="lv">从“开卷”中检索</em>相关信息。</p><p id="57b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像开卷考试一样，学生可以在考试中参考书本获取信息，该模型可以参考外部信息源。该信息来源可能是公司内部文档、维基百科、Reddit或任何其他与模型本身无关的信息来源。</p><p id="9fe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">IR步骤检索相关文档，并将它们传递给RC (reader)步骤。RC包括<em class="lv">从一个句子或段落中提取</em>一个简洁的答案，通常被称为<em class="lv">文档</em>或<em class="lv">上下文</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/00bb19b27e353baa448a4ca8acc015ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dyvYx-e612iPfcAV.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个<em class="ne">问题</em>、相关<em class="ne">上下文</em>和一个<em class="ne">答案</em>的示例。</p></figure><p id="caf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他两种类型的问答依赖于<em class="lv">生成</em>答案，而不是<em class="lv">提取</em>答案。OpenAI的GPT模型是众所周知的生成变压器模型。</p><p id="537d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="lv">开卷</em>抽象QA中，第一个IR步骤与提取QA相同；从外部来源检索相关上下文<em class="lv"/>。这些上下文被传递给文本生成模型(如GPT ),并用于<em class="lv">生成</em>(不是提取)答案。</p><p id="1407" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，我们可以使用<em class="lv">闭卷</em>抽象问答。这里只有文本生成模型和<em class="lv">没有</em> IR步骤。生成器模型将基于其自身内部学习到的世界表示来生成答案。它<em class="lv">不能</em>引用任何外部信息源，因此得名<em class="lv">闭卷</em>。</p><p id="ae26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们深入研究每一种方法，并了解每种方法的适用范围。</p><h1 id="0f82" class="mf mg it bd mh mi nf mk ml mm ng mo mp jz nh ka mr kc ni kd mt kf nj kg mv mw bi translated">提取质量保证</h1><p id="2362" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">提取式问答可以说是最广泛适用的问答形式。它允许我们提出一个问题，然后<em class="lv">从一篇短文中提取</em>一个答案。例如，我们有文本(或<em class="lv">上下文</em>):</p><pre class="kj kk kl km gt nk nl nm bn nn no bi"><span id="4258" class="np mg it nl b be nq nr l ns nt">Super Bowl 50 was an American football game to determine the<br/>champion of the National Football League (NFL) for the 2015 season.<br/>The American Football Conference (AFC) champion Denver Broncos<br/>defeated the National Football Conference (NFC) champion Carolina<br/>Panthers 24–10 to earn their third Super Bowl title. The game was<br/>played on February 7, 2016, at Levi's Stadium in the San Francisco<br/>Bay Area at Santa Clara, California.</span></pre><p id="a757" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以问这个问题，<code class="fe nu nv nw nl b">"which team represented the AFC at Super Bowl 50?"</code>，我们应该期望返回<code class="fe nu nv nw nl b">"Denver Broncos"</code>。</p><p id="34a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们给出一个<em class="lv">上下文</em>并提取答案的例子是阅读理解(RC)。</p><p id="282e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单独来看，RC并不是特别有用，但是我们可以将它与外部数据源结合起来，并通过<em class="lv">许多上下文</em>进行搜索，而不仅仅是一个。我们称之为“开卷抽取式质量保证”。更普遍的说法是仅仅指<em class="lv">提取QA </em>。它不是一个单一的型号，而是实际上由三个部件组成:</p><ul class=""><li id="fa1d" class="nx ny it lb b lc ld lf lg li nz lm oa lq ob lu oc od oe of bi translated">索引数据(文档存储/矢量数据库)</li><li id="dc40" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">检索器模型</li><li id="05e8" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">读者模型</li></ul><p id="20fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在提问之前，开卷QA需要索引我们的检索器模型以后可以访问的数据。通常这将是句子到段落大小的文本块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/7ee0e0d700c654ec1443aceed36b6c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nTBb_RmrWzvJIwr5.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">开卷抽取QA栈包括“开卷”数据库、检索器模型和读者模型。</p></figure><p id="ff59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个例子。首先，我们需要数据。一个流行的问答数据集是斯坦福问答数据集(SQuAD)。我们可以使用拥抱脸的<code class="fe nu nv nw nl b">datasets</code>库下载这个数据集，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="7deb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们有<em class="lv">上下文</em>特性。正是这些上下文应该在我们的数据库中被索引。</p><p id="edd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据库类型的选项因检索器型号而异。传统的检索器使用TF-IDF或BM25的<em class="lv">稀疏向量</em>检索。</p><p id="009e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些模型基于<em class="lv">上下文</em>和<em class="lv">问题</em>之间匹配单词的频率返回<em class="lv">上下文</em>。更多的单词匹配等同于更高的相关性。Elasticsearch是最受欢迎的数据库解决方案，因为它具有可伸缩性和强大的关键字搜索能力。</p><p id="4088" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种选择是使用<em class="lv">密集向量</em>检索由<a class="ae ky" href="https://www.pinecone.io/learn/transformers/" rel="noopener ugc nofollow" target="_blank">变压器模型</a>构建的句子向量，如<a class="ae ky" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> BERT </a>。密集向量的优势在于能够通过语义进行搜索。根据<em class="lv">“美味、新鲜出炉的东西”</em>示例中描述的问题的含义进行搜索。为此，需要一个像<a class="ae ky" href="https://www.pinecone.io/" rel="noopener ugc nofollow" target="_blank">松果</a>这样的矢量数据库或者像<a class="ae ky" href="https://www.pinecone.io/learn/faiss/" rel="noopener ugc nofollow" target="_blank"> Faiss </a>这样的独立矢量索引。</p><p id="2eb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将尝试<em class="lv">密集矢量</em>方法。首先，我们用一个QA模型对我们的<em class="lv">上下文</em>进行编码，就像句子转换器中的<code class="fe nu nv nw nl b">multi-qa-MiniLM-L6-cos-v1</code>一样。我们用以下内容初始化模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="d3e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用该模型，我们对数据集对象<code class="fe nu nv nw nl b">qa</code>中的上下文进行编码，以创建要在向量数据库中索引的句子向量表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="32ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以将这些存储在向量数据库中。在这个例子中，我们将使用Pinecone(它确实需要一个<a class="ae ky" href="https://app.pinecone.io/" rel="noopener ugc nofollow" target="_blank">免费API密匙</a>)。首先，我们初始化一个到松果的连接，创建一个新的索引，并连接到它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="cd5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那里开始，我们需要做的就是<code class="fe nu nv nw nl b">upsert</code> ( <em class="lv"> up </em> load，并在<em class="lv"> sert </em>中将我们的向量插入松果索引。我们分批进行，每个样本是一个<code class="fe nu nv nw nl b">(id, vector)</code>的元组。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="ec50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦在数据库中对上下文进行了索引，我们就可以继续QA流程了。</p><p id="6ee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个问题/查询，<em class="lv">检索器</em>创建一个稀疏/密集向量表示，称为<em class="lv">查询向量</em>。将该查询向量与数据库中所有已经索引的<em class="lv">上下文向量</em>进行比较。返回最相似的n个。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="9952" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些最相似的上下文与原始问题一起被传递给<em class="lv">读者</em>模型(一次一个)。给定一个问题和上下文，读者预测答案的开始和结束位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/46225c5238ac87867212fe9f1db00b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rdpsjLUuyE5Q1rsf.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">读者模型预测给定问题和包含答案的上下文的答案的开始和结束位置。</p></figure><p id="0dc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用拥抱脸<code class="fe nu nv nw nl b">transformers</code>中的<code class="fe nu nv nw nl b">deepest/electra-base-squad2</code>模型作为我们的读者模型。我们所做的就是建立一个<code class="fe nu nv nw nl b">'question-answering'</code>管道，并把我们的<em class="lv">查询</em>和<em class="lv">上下文</em>逐个传递给它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="bdef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对每个上下文重复读者预测。如果愿意，我们可以从这里使用检索器和/或阅读器模型输出的分数来排序“答案”。</p><p id="b88d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，模型返回的是<code class="fe nu nv nw nl b">'Denver Broncos'</code>的正确答案，得分为0.99。大多数其他答案只返回很小的分数，这表明我们的读者模型很容易区分好答案和坏答案。</p><h1 id="16a2" class="mf mg it bd mh mi nf mk ml mm ng mo mp jz nh ka mr kc ni kd mt kf nj kg mv mw bi translated">抽象问答</h1><p id="676e" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">正如我们之前看到的，抽象问答可以分为两种类型:开卷和闭卷。我们将从<em class="lv">开卷</em>开始，作为之前提取QA管道的自然延续。</p><h2 id="06e8" class="om mg it bd mh on oo dn ml op oq dp mp li or os mr lm ot ou mt lq ov ow mv ox bi translated">打开的书</h2><p id="1b48" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">作为<strong class="lb iu">开卷</strong> <em class="lv">抽象</em> QA，我们可以使用用于<em class="lv">提取QA </em>的相同数据库和检索器组件。这些组件以相同的方式工作，并向我们的<em class="lv">生成器</em>模型提供一组<em class="lv">上下文</em>，它取代了来自extract QA的<em class="lv">阅读器</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/ceb5dda40640d923ab4f51a5ce8e4d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fWeCo1xVbR6QgKhs.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">打开抽象QA管道，注意与提取QA栈相比，<em class="ne">阅读器</em>模型已经被一个<em class="ne">生成器</em>模型(突出显示)所取代。</p></figure><p id="b0ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与<em class="lv">提取</em>答案不同，上下文被用作生成序列对序列(seq2seq)模型的输入(与问题一起)。该模型使用问题和上下文来<em class="lv">生成</em>答案。</p><p id="de1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大型变压器模型在其参数中存储知识的“表示”。通过将相关的上下文和问题传递到模型中，我们<em class="lv">希望</em>模型将使用上下文及其“存储的知识”来回答更多的<em class="lv">抽象</em>问题。</p><p id="a7f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用的seq2seq模型通常基于BART或T5。我们将继续使用BART模型初始化seq2seq管道，该模型针对抽象QA — <code class="fe nu nv nw nl b">yjernite/bart_eli5</code>进行了优化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="8e05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们之前问的问题很具体。我们在寻找<code class="fe nu nv nw nl b">Denver Broncos</code>的一个简洁明了的答案。抽象问答不适合这些类型的问题:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="d3c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，抽象问答的好处来自于更“抽象”的问题，比如<code class="fe nu nv nw nl b">"Do NFL teams only care about playing at the Super Bowl?"</code>在这里，我们几乎是在征求意见。不太可能有<em class="lv">确切的</em>答案。我们来看看抽象的QA方法对此是怎么看的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="ff3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些答案看起来比我们的“特定”问题好得多。返回的上下文不包括球队是否关心超级碗的直接信息。相反，它们包含具体的NFL/超级碗细节片段。</p><p id="f4e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">seq2seq模型结合了这些细节及其自身的内部“知识”,以产生一些关于该问题的深刻想法:</p><ul class=""><li id="f4da" class="nx ny it lb b lc ld lf lg li nz lm oa lq ob lu oc od oe of bi translated"><em class="lv">“不，因为这是职业足球的巅峰”</em> —指出超级碗中的球队(无论他们是否获胜)已经知道他们处于顶端；在某种程度上，他们已经<em class="lv">【已经赢了】</em>。</li><li id="7cbd" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">“他们不在乎输球，他们只在乎是否有一大群人来为他们欢呼”——球员们很高兴能娱乐他们的球迷；也就是超级碗不那么重要了。</li><li id="66b0" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated"><em class="lv">“他们拿了很多钱去参加超级碗比赛”</em>——指出了一个更明显的问题“谁不想要一大笔钱呢？”。</li></ul><p id="73fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多矛盾和观点，但这通常是更抽象的问题，尤其是我们提出的问题。</p><p id="53fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管这些结果很有趣，但并不完美。我们可以调整诸如<code class="fe nu nv nw nl b">temperature</code>这样的参数来增加/减少答案的随机性，但是抽象的QA在一致性方面会受到限制。</p><h2 id="aac9" class="om mg it bd mh on oo dn ml op oq dp mp li or os mr lm ot ou mt lq ov ow mv ox bi translated">一无所知的东西</h2><p id="9170" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们要看的最后一个架构是<em class="lv">闭卷</em>抽象QA。实际上，这只不过是一个生成模型，它接受一个问题，除了依赖自己的内部知识之外，什么都不依赖。没有<strong class="lb iu">没有</strong>检索步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/8ad9aaea43d21e21781fef97f6d1ffbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DrOtd_TmJEuPQIlo.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">闭卷架构就简单多了。无非是一个<em class="ne">发电机</em>的型号。</p></figure><p id="a1b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我们放弃了检索器模型，但这并不意味着我们坚持使用相同的阅读器模型。正如我们之前看到的,<code class="fe nu nv nw nl b">yjernite/bart_eli5</code>模型需要如下输入:</p><pre class="kj kk kl km gt nk nl nm bn nn no bi"><span id="ac2b" class="np mg it nl b be nq nr l ns nt">question: &lt;our question&gt; context: &lt;a (hopefully) relevant context&gt;</span></pre><p id="d001" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果没有上下文输入，以前的模型不会执行得很好。这是可以预料的。当给定问题<em class="lv">和</em>上下文时，seq2seq模型被优化以产生一致的答案。如果我们的输入是一种新的、意想不到的格式，性能会受到影响:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="9b3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型不知道答案，翻转了提问的方向。不幸的是，这并不是我们真正想要的。然而，我们可以尝试许多替代模型。OpenAI的GPT模型是众所周知的生成变压器的例子，可以产生良好的结果。</p><p id="7f2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">OpenAI的最新GPT GPT-3被锁定在一个API后面，但也有开源的替代品，如Eleuther AI的GPT-尼奥。让我们试试较小的GPT-尼奥模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="8f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们使用的是<code class="fe nu nv nw nl b">'text-generation'</code>管道。我们在这里所做的就是在一个问题之后生成文本。我们得到了一个有趣的、真实的答案，但它不一定回答了问题。我们可以多试几个问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="5cd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以调整参数来减少重复的可能性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol lx l"/></div></figure><p id="49a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们确实得到了一些有趣的结果，尽管很明显<em class="lv">闭卷</em>抽象QA是一项具有挑战性的任务。更大的模型存储更多的内部知识；因此，闭卷性能与模型大小密切相关。</p><p id="5121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用更大的模型，我们可以得到更好的结果，但对于一致的答案，开卷的替代方法往往优于闭卷的方法。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="1b83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们关于开放领域问答(ODQA)的文章。我们已经研究了语义相似性背后的思想及其在QA模型中的应用。</p><p id="ab47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们探索了产生这些“QA栈”的各种组件，如<a class="ae ky" href="https://www.pinecone.io/learn/vector-database/" rel="noopener ugc nofollow" target="_blank">向量数据库</a>，检索器、阅读器和生成器。我们还学习了如何使用不同的工具和模型实现这些不同的堆栈。</p><p id="8666" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些都应该为进一步探索ODQA的世界和机会提供一个坚实的基础。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="1ce2" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">进一步阅读</h1><p id="e563" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">长度翁，<a class="ae ky" href="https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html" rel="noopener ugc nofollow" target="_blank">如何建立一个开放领域的问答系统？</a>，GitHub博客</p><p id="a388" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">D.Khashabi等人，<a class="ae ky" href="https://arxiv.org/pdf/2005.00700.pdf" rel="noopener ugc nofollow" target="_blank"> UnifiedQA:通过单一QA系统跨越格式界限</a> (2020)，EMNLP</p><p id="ed5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://huggingface.co/transformers/usage.html#extractive-question-answering" rel="noopener ugc nofollow" target="_blank">摘抄问题回答</a>，拥抱脸文档</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="6d64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">*除非另有说明，所有图片均出自作者之手</em></p></div></div>    
</body>
</html>