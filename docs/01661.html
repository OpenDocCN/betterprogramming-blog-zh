<html>
<head>
<title>Realtime Video Closed Captioning in SwiftUI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SwiftUI中的实时视频隐藏字幕</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/realtime-video-closed-captioning-in-swiftui-4022ba85c803?source=collection_archive---------6-----------------------#2019-10-02">https://betterprogramming.pub/realtime-video-closed-captioning-in-swiftui-4022ba85c803?source=collection_archive---------6-----------------------#2019-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="64a5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用SFSpeechRecognizer脱机、在设备上且不中断</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/37b9d3fb28a15dd2f2db34dcdd56d247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vqPQeylJ2ETFrNc1kpkUAg.jpeg"/></div></div></figure><p id="702f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">这个项目献给我的父亲，他每天都在克服自己的听力损失，并激励我编写代码来提高技术的可访问性。</em></p><p id="6efc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最近，<a class="ae lr" href="https://towardsdatascience.com/apple-cranks-speech-to-text-up-to-xcode-11-e1848e42252b" rel="noopener" target="_blank">我对苹果今年发布的</a> <code class="fe ls lt lu lv b"><a class="ae lr" href="https://towardsdatascience.com/apple-cranks-speech-to-text-up-to-xcode-11-e1848e42252b" rel="noopener" target="_blank">SFSpeechRecognizer</a></code> <a class="ae lr" href="https://towardsdatascience.com/apple-cranks-speech-to-text-up-to-xcode-11-e1848e42252b" rel="noopener" target="_blank">的变化感到非常兴奋。改变游戏规则的是，它现在能够在本地设备上运行:不需要互联网连接。这意味着:</a></p><ul class=""><li id="77cc" class="lw lx it kw b kx ky la lb ld ly lh lz ll ma lp mb mc md me bi translated">用户不再需要担心在移动时使用数据</li><li id="3213" class="lw lx it kw b kx mf la mg ld mh lh mi ll mj lp mb mc md me bi translated">隐私FTW——一切都在本地完成，而不是与某个服务器来回切换</li><li id="02d5" class="lw lx it kw b kx mf la mg ld mh lh mi ll mj lp mb mc md me bi translated">转录延迟更少，<em class="lq">尤其是实时的</em></li><li id="307e" class="lw lx it kw b kx mf la mg ld mh lh mi ll mj lp mb mc md me bi translated">不再局限于每天只录制有限次数的一分钟音频。它是无限的(或者直到你的声音停止或者你的电池耗尽)</li></ul><p id="ddbb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">总体来说，这对于语音转文本、可访问性和语音技术来说是个大新闻。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="972f" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">用例</h1><p id="b27d" class="pw-post-body-paragraph ku kv it kw b kx nj ju kz la nk jx lc ld nl lf lg lh nm lj lk ll nn ln lo lp im bi translated">我生长在一个通常开着隐藏字幕(CC)看电视的家庭，这让我专注于测试<code class="fe ls lt lu lv b">SFSpeechRecognizer</code>的最新更新变得非常有意义。当考虑如何进行CC时，录制的节目通常会准备好文本进行播放，而现场活动，如体育或新闻，会有人进行现场转录。</p><p id="29a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，我们的目标将是通过制作一个实时转录视频(在我们的例子中是预先录制的)的应用程序来取代该系统。这已经在iOS上完成了，但这次它不会有任何网络延迟、数据使用或转录长度的限制。</p><p id="af4f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另外，作为最后一个挑战，我想在SwiftUI中做这件事。这不是这个项目必须的，但我在过去的几个月里一直在使用它(查看<a class="ae lr" href="https://medium.com/@dbolella" rel="noopener">我的个人资料</a>中我所有关于SwiftUI的文章),并想尝试使用AV。</p><p id="56de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">完全披露:</em>我最终建立在别人的工作之上。令人惊叹的是，它们都是链式进化的一部分——相互依存。我最终只是那个进程的最新迭代，结果变得很棒！</p><p id="ffed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是为了公平起见，我会在整篇文章中引用它们的名字和链接。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="c7fd" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">它是如何工作的</h1><p id="209c" class="pw-post-body-paragraph ku kv it kw b kx nj ju kz la nk jx lc ld nl lf lg lh nm lj lk ll nn ln lo lp im bi translated">如果您想在Github或您的IDE上继续学习，这里是该项目的源代码。否则，我会包括相关的要点。</p><div class="no np gp gr nq nr"><a href="https://github.com/dbolella/SwiftUIClosedCaptioning" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">dbolella/SwiftUIClosedCaptioning</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">SFSpeechRecognizer的最新版本允许离线语音转文本，以及其他新功能。此项目构建…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">github.com</p></div></div></div></a></div><p id="ad0a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第一件事是理解<code class="fe ls lt lu lv b"><a class="ae lr" href="https://developer.apple.com/documentation/speech/sfspeechrecognizer" rel="noopener ugc nofollow" target="_blank">SFSpeechRecognizer</a></code>是如何工作的，以及它需要什么来完成它的工作。文档告诉我们，为了实时识别，我们需要创建一个带音频缓冲区的<code class="fe ls lt lu lv b"><a class="ae lr" href="https://developer.apple.com/documentation/speech/sfspeechaudiobufferrecognitionrequest" rel="noopener ugc nofollow" target="_blank">SFSpeechAudioBufferRecognitionRequest</a></code>(或者是<code class="fe ls lt lu lv b"><a class="ae lr" href="https://developer.apple.com/documentation/avfoundation/avaudiopcmbuffer" rel="noopener ugc nofollow" target="_blank">AVAudioPCMBuffer</a></code>或者是<code class="fe ls lt lu lv b"><a class="ae lr" href="https://developer.apple.com/documentation/coremedia/cmsamplebuffer" rel="noopener ugc nofollow" target="_blank">CMSampleBuffer</a></code>)。然后，我们将我们的请求作为参数传递给<code class="fe ls lt lu lv b">recognitionTask</code>，并设置一个完成处理程序，在那里我们应该期望得到一个<code class="fe ls lt lu lv b">SFSpeechRecognitionResult</code>。</p><pre class="kj kk kl km gt oa lv ob oc aw od bi"><span id="f24b" class="oe ms it lv b gy of og l oh oi">private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))!<br/>    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?<br/>    private var recognitionTask: SFSpeechRecognitionTask?<br/>    <br/>    private func setupRecognition() {<br/>        let recognitionRequest = SFSpeechAudioBufferRecognitionRequest()<br/>        <br/>        recognitionRequest.shouldReportPartialResults = true<br/>        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in<br/>            result!.bestTranscription.formattedString<br/>        }<br/>        <br/>        self.recognitionRequest = recognitionRequest<br/>    }</span></pre><p id="42f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">到目前为止够简单了。现在我只需要找到视频的音频缓冲区，事实证明，这并不那么简单。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="7edb" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">在AVAsset中点击V中的A</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/cf81030f5bebfb47f5a4a0f8e014249f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNCE2qw8mumz_mhA5LSvnw.jpeg"/></div></div><p class="ok ol gj gh gi om on bd b be z dk translated">为了把这些点/回复联系起来，有相当多的线索可循。照片由<a class="ae lr" href="https://unsplash.com/@firmbee?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">威廉·艾文</a>在<a class="ae lr" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="bdfe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我找出如何访问缓冲区的旅程是从Sash Zats的这个回购开始的。这是我发现的第一个在视频缓冲区上使用<code class="fe ls lt lu lv b">SFSpeechRecognition</code>的项目。当我探索他的工作时，他留下了一条评论，讲述了他如何从几个苹果工程师那里得到帮助，修改苹果的一个<code class="fe ls lt lu lv b">MTAudioProcessingTap</code> <a class="ae lr" href="https://developer.apple.com/library/archive/samplecode/AudioTapProcessor/Introduction/Intro.html#//apple_ref/doc/uid/DTS40012324-Intro-DontLinkElementID_2" rel="noopener ugc nofollow" target="_blank">样本(用Obj-C编写)以获得一个<code class="fe ls lt lu lv b">CMSampleBuffer</code>，然后他将这个样本传递给一个代表以供消费。</a></p><p id="8255" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这似乎有些过火，尤其是因为他的解决方案使用了AVPlayer实现。所以我看得更远一点，发现<a class="ae lr" href="https://github.com/peacemoon/SFSpeechRecognizerRealtimeVideoCaptioning" rel="noopener ugc nofollow" target="_blank">这个回购</a>由一个Tran。他创建了一个清理版的Zats repo，但他的笔记提到他希望创建一个快速版的AudioTap，并提供了几个可能的解决方案的链接。</p><p id="7e2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中一个链接引起了我的兴趣。这是奥马尔·华雷斯创造的一个要点。浏览一下，它看起来像是名为<code class="fe ls lt lu lv b">VideoMediaInput</code>的修改tap的Swifty版本，仍然提供CMSampleBuffer。虽然我希望有一个解决方案能给我一个<code class="fe ls lt lu lv b">AVAudioPCMBuffer</code>，但感觉这是我能得到的最好的答案。</p><p id="9f11" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感觉我已经有了拼图的碎片，我尝试用Swift代替Obj-C tap，如果我最终将它放入SwiftUI，这将是更好的选择。结果很成功，可以在<a class="ae lr" href="https://github.com/dbolella/SwiftClosedCaptioning" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="kj kk kl km gt oa lv ob oc aw od bi"><span id="4e47" class="oe ms it lv b gy of og l oh oi">//*********went from this*********<br/>let asset = AVURLAsset(url: url)<br/>guard let audioTrack = asset.tracks(withMediaType: AVMediaType.audio).first else {<br/>    print("can't get audioTrack")<br/>    return<br/>}<br/>playerItem = AVPlayerItem(asset: asset)<br/><br/>tap = MYAudioTapProcessor(audioAssetTrack: audioTrack)<br/>tap.delegate = self<br/><br/>player.insert(playerItem, after: nil)<br/>player.currentItem?.audioMix = tap.audioMix<br/>player.play()<br/><br/>// Player view<br/>let playerView: UIView! = view<br/>playerLayer.player = player<br/>        <br/>//*********to this*********<br/>vmInput = VideoMediaInput(url: url, delegate: self)<br/><br/>// Player view<br/>let playerView: UIView! = view<br/>playerLayer.player = vmInput.player</span></pre><h2 id="51f0" class="oe ms it bd mt oo op dn mx oq or dp nb ld os ot nd lh ou ov nf ll ow ox nh oy bi translated">关闭Swift循环</h2><p id="8425" class="pw-post-body-paragraph ku kv it kw b kx nj ju kz la nk jx lc ld nl lf lg lh nm lj lk ll nn ln lo lp im bi translated">现在我有了勇气，是时候把它放进SwiftUI了。在<a class="ae lr" href="https://medium.com/@chris.mash/avplayer-swiftui-b87af6d0553" rel="noopener"> AVPlayer &amp; SwiftUI </a>上浏览了Chris Mash的系列之后，我在这个<a class="ae lr" href="https://github.com/ChrisMash/AVPlayer-SwiftUI" rel="noopener ugc nofollow" target="_blank"> repo </a>中按照他的文章和代码构建了一个带控件的视频播放器。在做了一些提取和移动之后，<code class="fe ls lt lu lv b">VideoMediaInput</code>被注入到PlayerContainerView中，方法是用tap中的AVPlayer替换它。</p><p id="e2e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">难题的最后一部分是消耗要转录的代表的缓冲区。通过创建一个遵循该协议的<code class="fe ls lt lu lv b">ClosedCaptioning</code>类，我将缓冲区直接附加到识别器中进行处理。</p><p id="6ad3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于绑定，我也有符合<code class="fe ls lt lu lv b">ObservableObject</code>协议的<code class="fe ls lt lu lv b">ClosedCaptioning</code>。里面是一个带有<code class="fe ls lt lu lv b">Published</code>标签的<code class="fe ls lt lu lv b">captioning</code>房产。这使得它可以与我的SwiftUI代码绑定，只需用<code class="fe ls lt lu lv b">ObservedObject</code>标记我的类的实例。将我的文本视图设置为该属性后，每当我们得到新的结果时，绑定都会更新它，最后，显示我们的实时隐藏字幕。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="6611" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结果</h1><p id="19a0" class="pw-post-body-paragraph ku kv it kw b kx nj ju kz la nk jx lc ld nl lf lg lh nm lj lk ll nn ln lo lp im bi translated">在所有的研究、学习、试验和工作之后，我终于有了我的应用程序。我插入了一则90年代杰夫·高布伦的旧苹果广告，并对其进行了测试。结果…还不错。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/f32ae1e48ef91118419ecaccc016db78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6XAHqT03qGYpesQU4WALOQ.png"/></div></div><p class="ok ol gj gh gi om on bd b be z dk translated">我的朋友杰夫在谈论如何不被排除在电子邮件聚会之外。90后，amiright？</p></figure><p id="95a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然杰夫有他独特的说话方式，有时可能会分散，但<code class="fe ls lt lu lv b">SFSpeechRecognizer</code>很好地遵循了这种方式。当我把手机调到飞行模式时，我也得到了同样的结果，证明我们确实是在本地运行。最后一个测试是让它循环两次以上，看看转录是否在一分钟后中断，这也是成功的。</p><p id="a539" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">视频的实时隐藏字幕:完成</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="458c" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">然而…</h1><p id="6f55" class="pw-post-body-paragraph ku kv it kw b kx nj ju kz la nk jx lc ld nl lf lg lh nm lj lk ll nn ln lo lp im bi translated">转录的质量不是100%准确。有几个词错了或漏了，但不足以失去上下文或无法在心理上填补空白，尤其是在听的时候。</p><p id="3c06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不过，为了模拟作为一名失聪用户运行这款应用，我把手机调成静音，一边看一边读着脚本。让文本出现在视频顶部确实在同步时间和手势方面有所帮助。不幸的是，缺少对上下文语法和标点符号的编辑是非常令人遗憾的。静音转录使人们很难理解杰夫的话要说什么，他们被告知的方式，并把一切都变成了一个30秒的连续句子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/4e702866519ed8a6d1e0a19fd614ef73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jr7JR40796ywZchjNFfGCA.jpeg"/></div></div><p class="ok ol gj gh gi om on bd b be z dk translated">没有音频或语法帮助，转录可以得到正确的单词，但仍然缺乏上下文意义。拉斐尔·沙勒在<a class="ae lr" href="https://unsplash.com/s/photos/words?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="dd35" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然语音转文本是一项巨大的无障碍成就，但它仍然无法与隐藏式字幕相媲美。虽然在服务器上运行可能有助于纠正一些单词，但对于没有音频或语法帮助或上下文的用户来说，它仍然是不够的。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="6e4f" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">新的希望</h1><p id="444d" class="pw-post-body-paragraph ku kv it kw b kx nj ju kz la nk jx lc ld nl lf lg lh nm lj lk ll nn ln lo lp im bi translated">这就是Core ML/Create ML中新的<code class="fe ls lt lu lv b"><a class="ae lr" href="https://developer.apple.com/documentation/speech/sfvoiceanalytics" rel="noopener ugc nofollow" target="_blank">SFAudioAnalytics</a></code>和声音分类的用武之地。这些新功能可以(而且肯定会)填补空白。</p><p id="a7fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，结合使用分析和经过训练的ML模型，我们可以确定转录中的标点符号(包括句号、感叹号和问号)。标点符号给出的上下文非常有价值。但是想象一下，通过字体来处理和显示音调、语调或情感，你可以更进一步！可能性是惊人的，这是语音识别的一条伟大的前进道路。</p><p id="75b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我的希望是将这些想法融入到这个项目中，同时也希望看到其他开发人员加入到这个行动中来。</p><p id="2508" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">非常感谢并感谢苹果、</em> <a class="ae lr" href="https://github.com/zats" rel="noopener ugc nofollow" target="_blank"> <em class="lq">萨莎·扎茨</em></a><em class="lq"/><a class="ae lr" href="https://github.com/peacemoon" rel="noopener ugc nofollow" target="_blank"><em class="lq">安·特兰</em> </a> <em class="lq">、</em> <a class="ae lr" href="https://github.com/omarojo" rel="noopener ugc nofollow" target="_blank"> <em class="lq">奥马尔·华雷斯</em> </a> <em class="lq">，以及</em> <a class="ae lr" href="https://github.com/ChrisMash" rel="noopener ugc nofollow" target="_blank"> <em class="lq">克里斯·马什</em> </a> <em class="lq">，他们的作品都有助于将这个激情项目结合在一起。</em></p></div></div>    
</body>
</html>