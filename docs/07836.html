<html>
<head>
<title>How To Create an Email Crawler With Python and Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Python和Scrapy创建电子邮件爬虫</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/how-to-create-an-email-crawler-with-python-and-scrapy-af275ae486e2?source=collection_archive---------10-----------------------#2021-02-24">https://betterprogramming.pub/how-to-create-an-email-crawler-with-python-and-scrapy-af275ae486e2?source=collection_archive---------10-----------------------#2021-02-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c47b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从任何网站收集电子邮件</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5d4b1a0ade05864a711704c1ac29c185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPCK2NYERdmpEe0nOA-fMA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@le_buzz?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Le Buzz </a>在<a class="ae ky" href="https://unsplash.com/s/photos/website?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄。</p></figure><p id="7f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">联系方式不容易得到。这就是为什么有无数的企业为企业和个人提供有针对性的联系方式，如ZoomInfo、Uplead和LinkedIn Sales Navigator。</p><p id="5dae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能有一个你想瞄准的领域列表。有一些解决方案，如Hunter.io或FindThatLead，可以将域名转换为电子邮件和其他联系信息，但这些都是有成本的。</p><p id="ec75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，如果你知道或者想学习Python，为什么不自己做点什么呢？</p><p id="3c55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诚然，它可能没有我概述的解决方案先进，但一个好的爬虫可以很好地获取公司网站上的联系信息，如电子邮件或电话号码。</p><p id="3b39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用Python (3.6+)和scrapy来构建这个小项目。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="df7c" class="ma mb it lw b gy mc md l me mf"># If you need to install scrapy use: pip install scrapy<br/>scrapy startproject contact_details<!-- --> <br/>cd contact_details<br/>scrapy genspider gather_details example.com</span></pre><p id="b01d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先创建一个名为<code class="fe mg mh mi lw b">contact_details</code>的新scrapy项目，并在域名example.com下生成一个新的蜘蛛(我们稍后会更改)。</p><p id="247a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们生成的蜘蛛看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="69c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始解析回调之前，我们需要添加一些东西:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="0d82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们添加了一些可配置的参数，并删除了<code class="fe mg mh mi lw b">start_urls</code>和<code class="fe mg mh mi lw b">allowed_domains</code>属性。我们删除它们的原因是，我们将使crawler可以使用目标域作为参数进行调用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="4898" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们定义的其他一些属性是:</p><ul class=""><li id="cf3e" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated"><code class="fe mg mh mi lw b">greedy</code>:指示爬虫抓取主页后是否继续(设置<code class="fe mg mh mi lw b">greedy</code>为false会减少邮件数量，但会加快进程)。</li><li id="5581" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><code class="fe mg mh mi lw b">email_regex</code>:这个正则表达式将获取HTML中的电子邮件。这将选取页面上的任何电子邮件，包括在<code class="fe mg mh mi lw b">&lt;script&gt;</code>标签中定义的或在页面上不可见的邮件。</li><li id="3d14" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><code class="fe mg mh mi lw b">forbidden_keys</code>:这将用于跳过包含这些关键字的特定URL，例如图像、文档或电子邮件链接。</li></ul><p id="4bf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们开始解析页面。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="a643" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们创建了一个非常简单的<code class="fe mg mh mi lw b">parse</code>方法，并尝试处理页面的HTML内容。在页面返回二进制结果的情况下，会导致一个<code class="fe mg mh mi lw b">UnicodeDecodeError</code>，这反过来会停止解析过程。</p><p id="b036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用我们预先编译的正则表达式，我们在网站的HTML内容中搜索电子邮件。然后，使用<code class="fe mg mh mi lw b">tld</code>包，我们试图排除看起来像电子邮件但不是的文本(例如image@home.png)。当然，。png不是一个有效的TLD，它将返回<code class="fe mg mh mi lw b">None</code>，这是一个假值，在列表理解过程中会被过滤掉。</p><p id="f650" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是时候让我们的爬虫，嗯，爬行了。这个过程的这一部分侧重于解析单个页面，但是如果<code class="fe mg mh mi lw b">greedy</code>设置为<code class="fe mg mh mi lw b">True</code>，我们将不得不遍历整个网站。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="2285" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一部分将找到页面中的所有链接，并使用XPath提取它们的<code class="fe mg mh mi lw b">href</code>。如果您想知道如何使用和调试XPath命令，Google Chrome允许您在控制台中测试XPath命令，同时为您带来结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/793a7242a7c3fc84de9a17d62d7b67bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QEN3RUoYyt5lpPSGarLHRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者展示谷歌Chrome开发工具的截图</p></figure><p id="b278" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们检查所有禁用的键，以确保跳过文档和图像:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="2ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们产生新的请求来抓取其他页面。Scrapy将避免抓取同一个页面两次，也将避免在<code class="fe mg mh mi lw b">allowed_domains</code>属性之外发出请求，所以我们不需要很多检查。</p><p id="5015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<code class="fe mg mh mi lw b">try/except</code>块的原因是我们需要<code class="fe mg mh mi lw b">scrapy.Request</code>用于绝对URL，需要<code class="fe mg mh mi lw b">response.follow</code>用于相对URL。这种方法被称为请求原谅而不是许可(<a class="ae ky" href="https://docs.python.org/3.4/glossary.html" rel="noopener ugc nofollow" target="_blank"> EAFP </a>)，在Python中比首先使用<code class="fe mg mh mi lw b">if</code>语句更受欢迎，后者被称为三思而后行(LBYL)。</p><p id="8177" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">难题的最后一部分是让我们的爬虫可以用参数调用，我们现在就要做:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="f89a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下格式将任何参数传递给scrapy:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="fac2" class="ma mb it lw b gy mc md l me mf">scrapy crawl gather_details -a domain=example.com -o emails.json</span></pre><p id="8ffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将把一个域作为参数传递给<code class="fe mg mh mi lw b">__init__</code>，我们将使用这个域进行请求。<code class="fe mg mh mi lw b">-o</code>标志指示在哪里存储爬行过程的输出——即存储到一个名为<code class="fe mg mh mi lw b">emails.json</code>的文件中。</p><p id="0d04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们准备好了。您可以使用上面的命令为任何域运行您的爬虫，并开始收集电子邮件！</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="01f1" class="nh mb it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">结论</h1><p id="5e09" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">你可以在GitHub 上找到完整的要点和更多的想法。如果我们要建造更先进的东西，我们可以做很多事情:</p><ul class=""><li id="526b" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">改进电子邮件正则表达式。</li><li id="3b93" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">对电子邮件使用更智能的验证。</li><li id="f4bf" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">处理输出文件，按域对电子邮件进行分组。</li><li id="143d" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">用scrapyd设置项目，这样就可以将它作为web服务运行。</li></ul><p id="2f7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多想法可以让你更进一步。要了解更多，你可以开始阅读<a class="ae ky" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> scrapy </a>和<a class="ae ky" href="https://scrapyd.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> scrapyd </a>文档。</p></div></div>    
</body>
</html>