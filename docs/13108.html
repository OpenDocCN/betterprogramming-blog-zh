<html>
<head>
<title>Automatically Generate Site-Wide Meta Descriptions with Python + BART for PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python + BART为PyTorch自动生成站点范围的元描述</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/automatically-generate-site-wide-meta-descriptions-with-python-bart-for-pytorch-cd0e14dd40d3?source=collection_archive---------6-----------------------#2022-07-29">https://betterprogramming.pub/automatically-generate-site-wide-meta-descriptions-with-python-bart-for-pytorch-cd0e14dd40d3?source=collection_archive---------6-----------------------#2022-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="284a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于如何有效地为搜索引擎优化(SEO)创建高质量摘要内容的指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9b26250cec68746c7b469052eea564a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IiYw3ri3MdmvXxWv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">穆罕默德·拉赫马尼在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="d19d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">前言</h1><p id="1236" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我以前的文章中，我探讨了数字营销和电子商务领域中自然语言处理(NLP)实现的几个应用程序。本文面向熟悉Python、NLP基础和简单数据挖掘的技术搜索引擎优化(SEO)专业人员。</p><p id="06fd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本文的目的是为读者提供一个指南，告诉他们如何使用一个简单的Python脚本为整个网站快速生成SEO元描述。该脚本将分为四个主要部分:</p><ol class=""><li id="66a1" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">域名URL提取和清理</li><li id="bf92" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">每个域URL的数据挖掘</li><li id="0ec5" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">应用于挖掘数据的NLP管道</li><li id="7654" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">元描述的标准化</li></ol></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="f3c1" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">项目相关性</h1><p id="3daf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个程序需要几个库才能运行。我们首先简要描述每个库及其各自的安装说明。</p><p id="3de7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">数据组织依赖关系</strong></p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="4833" class="nu kx iq nq b gy nv nw l nx ny">#Data organization dependencies</span><span id="89c2" class="nu kx iq nq b gy nz nw l nx ny">from os import remove<br/>import pandas as pd</span></pre><p id="18c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe oa ob oc nq b">pandas</code>是一个为Python编程语言编写的软件库，用于数据操作和分析。</p><p id="1ed2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">网页抓取依赖关系</strong></p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="04f3" class="nu kx iq nq b gy nv nw l nx ny">#Web scraping dependencies</span><span id="75be" class="nu kx iq nq b gy nz nw l nx ny">#!pip install beautifulsoup4<br/>from bs4 import BeautifulSoup</span><span id="3e28" class="nu kx iq nq b gy nz nw l nx ny">from urllib.request import Request, urlopen</span><span id="f772" class="nu kx iq nq b gy nz nw l nx ny">#!pip install requests<br/>import requests</span><span id="7d0e" class="nu kx iq nq b gy nz nw l nx ny">#!pip install justext<br/>import justext</span><span id="d517" class="nu kx iq nq b gy nz nw l nx ny">import re</span></pre><ul class=""><li id="ad01" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated"><code class="fe oa ob oc nq b"><a class="ae kv" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank">beautifulsoup4</a></code>是一个库，可以很容易地从网页中抓取信息。它位于HTML或XML解析器之上，为迭代、搜索和修改解析树提供了Pythonic习惯用法。</li><li id="3465" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated"><code class="fe oa ob oc nq b"><a class="ae kv" href="https://docs.python.org/3/library/urllib.request.html#module-urllib.request" rel="noopener ugc nofollow" target="_blank">urllib.request</a></code>模块定义了帮助打开URL、基本和摘要认证、重定向、cookies等等的函数和类。</li><li id="0fc8" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated"><code class="fe oa ob oc nq b"><a class="ae kv" href="https://pypi.org/project/requests/" rel="noopener ugc nofollow" target="_blank">requests</a></code>是一个流行的开源HTTP库，它简化了HTTP请求的处理。</li><li id="e4a9" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated"><code class="fe oa ob oc nq b"><a class="ae kv" href="https://pypi.org/project/jusText/" rel="noopener ugc nofollow" target="_blank">jusText</a></code>是一个从HTML页面中移除样板内容的工具，比如导航链接、页眉和页脚。它主要用于保存包含完整句子的文本，因此非常适合创建语言资源，如Web语料库。</li><li id="61de" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated"><code class="fe oa ob oc nq b">re</code>是一个提供正则表达式匹配操作的模块。</li></ul><p id="d0d1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> NLP依赖关系</strong></p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="8ee2" class="nu kx iq nq b gy nv nw l nx ny">#NLP dependencies</span><span id="0c6e" class="nu kx iq nq b gy nz nw l nx ny">!pip install transformers<br/>from transformers import pipeline</span><span id="f25f" class="nu kx iq nq b gy nz nw l nx ny">#Summarize textual content using BART in pytorch<br/>bart_summarizer = pipeline(“summarization”)</span></pre><ul class=""><li id="d9e0" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">HuggingFace <code class="fe oa ob oc nq b">transformers</code>是JAX、PyTorch和TensorFlow最先进的机器学习模块。Transformers提供了数千个经过预先训练的模型，可以在文本、视觉和音频等不同形式上执行任务。</li><li id="f5bd" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated"><code class="fe oa ob oc nq b">transformers</code>将在这个程序中用来生成一个pipeline()，允许从模型中心简单导入任何模型。</li><li id="4b93" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">为了生成元描述，程序需要导入一个能够对文本数据任务进行推理的模型。有很多模型能够完成这个任务，有些是用<a class="ae kv" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a> (Google的深度学习框架)或者<a class="ae kv" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a> (Meta的框架)编写的。</li><li id="72e2" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">在这个程序中，将使用Pytorch模型，命名为<a class="ae kv" href="https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/bart#bart" rel="noopener ugc nofollow" target="_blank"> BART </a>。BART是用于自然语言生成、翻译和理解的去噪序列到序列预训练模型。BART将被用来创建它所接收的文本数据的摘要——因此，摘要管道将被加载。</li><li id="daf3" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">使用以下命令初始化BART和总结管道:</li></ul><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="163a" class="nu kx iq nq b gy nv nw l nx ny"># Summarize textual content using BART in pytorch</span><span id="1e3b" class="nu kx iq nq b gy nz nw l nx ny">bart_summarizer = pipeline(“summarization”)</span></pre><blockquote class="oe of og"><p id="78cd" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>Python ide在默认状态下不支持Pytorch或Tensorflow。可以在本地机器上安装任何一个框架。也就是说，<a class="ae kv" href="https://research.google.com/colaboratory/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>默认安装了这两个框架，是不需要本地安装就可以进行深度学习的推荐平台。</p></blockquote><p id="c192" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所有必需依赖项的列表到此结束。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="9193" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">域名URL提取和清理</h1><p id="ee54" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">出于本文的目的，该程序将解析加拿大安大略省伦敦市一家名为<a class="ae kv" href="https://www.barakatrestaurant.com/" rel="noopener ugc nofollow" target="_blank">巴拉卡特</a>的沙瓦玛餐厅的网站。</p><p id="645e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了生成关于一个网站的所有页面的元描述，需要一个完整的活动URL列表用于相应的域。为此，来自<code class="fe oa ob oc nq b">urllib.request</code>模块的请求函数将请求一个URL，而urlopen函数将打开指定的URL。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="f524" class="nu kx iq nq b gy nv nw l nx ny">#Obtain list of links on a domain</span><span id="55d1" class="nu kx iq nq b gy nz nw l nx ny">domain = "https://www.barakatrestaurant.com"</span><span id="0843" class="nu kx iq nq b gy nz nw l nx ny">req = Request(domain, headers={'User-Agent': 'Mozilla/5.0'})</span><span id="4b4b" class="nu kx iq nq b gy nz nw l nx ny">html_page = urlopen(req)</span></pre><blockquote class="oe of og"><p id="9254" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>抓取站点内容时可能会抛出403错误。建议通过请求的headers参数中指定的用户代理绕过HTML响应错误。用户代理允许服务器和网络对等体识别请求用户代理的应用程序、操作系统、供应商和/或版本。</p></blockquote><p id="c2ca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后将执行BeautifulSoup模块来解析各自打开的URL的HTML页面。为了解析HTML文档，BeautifulSoup构造函数将被配置为使用lxml的HTML解析器，命令如下:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="f6d0" class="nu kx iq nq b gy nv nw l nx ny">soup = BeautifulSoup(html_page, “lxml”)</span></pre><p id="6e63" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，实例化一个空列表来存储域的活动链接。对于BeautifulSoup解析中的每一个链接，其中的<a>标签定义了一个超链接，在空列表中添加一个相关的<code class="fe oa ob oc nq b">href</code>属性来指示链接的目的地。</a></p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="883d" class="nu kx iq nq b gy nv nw l nx ny">links = []</span><span id="0002" class="nu kx iq nq b gy nz nw l nx ny">for link in soup.findAll('a'):<br/>links.append(urljoin(domain, link.get('href')))</span></pre><p id="f5c0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一些附加的URL可能是一个<code class="fe oa ob oc nq b">None</code>类型，它们可能是一个不一致的URL结构，或者它们可能是属于该域的其他URL的副本。</p><p id="a526" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">使用列表理解从链接列表中删除<code class="fe oa ob oc nq b">None</code>类型对象:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="fdb0" class="nu kx iq nq b gy nv nw l nx ny">#Links cleanup for None types</span><span id="b20c" class="nu kx iq nq b gy nz nw l nx ny">links = [link for link in links if link is not None]</span></pre><p id="0081" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">删除列表中不符合标准URL结构的对象:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="62ef" class="nu kx iq nq b gy nv nw l nx ny">#Links cleanup for non-URL objects</span><span id="b4dc" class="nu kx iq nq b gy nz nw l nx ny">for link in links:<br/>  if not link.startswith("http"):<br/>    links.remove(link)</span></pre><p id="1202" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">移除列表中的重复对象以创建唯一的集合:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="be72" class="nu kx iq nq b gy nv nw l nx ny">#Links cleanup for duplicate URLs</span><span id="8432" class="nu kx iq nq b gy nz nw l nx ny">links = [link for n, link in enumerate(links) if link not in links[:n]]</span></pre><p id="4da7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，删除任何潜在的社交链接，这些链接可能会导致车辆离开现场。这些数据很可能无法被正确解析，只能作为附加的噪声数据供摘要模块处理。随意添加任何可能驻留在受检查的域上的附加社交。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="f927" class="nu kx iq nq b gy nv nw l nx ny">#Links cleanup for social URLs</span><span id="b2a1" class="nu kx iq nq b gy nz nw l nx ny">socials = [“instagram”, “facebook”, “twitter”, “linkedin”, “tiktok”, “google”, “maps”, “mealsy”]</span><span id="068a" class="nu kx iq nq b gy nz nw l nx ny">clean_links = []</span><span id="a5a2" class="nu kx iq nq b gy nz nw l nx ny">for link in links:<br/>  if not any(social in link for social in socials):<br/>    clean_links.append(link)</span></pre></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="30d7" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">每个域URL的数据挖掘</h1><p id="fcf0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">给定先前创建的与一个域相关的URL列表，下一步是解析每个单独的HTML页面以获得文本内容，这些文本内容稍后可以抽象成元描述。</p><p id="7755" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先实例化一个新列表来存储所有文本内容。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="7b9c" class="nu kx iq nq b gy nv nw l nx ny"># Extract and clean text from each link</span><span id="d53d" class="nu kx iq nq b gy nz nw l nx ny">content = []</span></pre><p id="f1dd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下面是一个嵌套的条件for循环，将在单个代码块的上下文中解释。代码块的目的如下:</p><ul class=""><li id="ce95" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">对于之前创建的链接列表中的每个链接，用<code class="fe oa ob oc nq b">requests</code>模块请求每个链接的URL，调用<code class="fe oa ob oc nq b">request.get</code>函数(同样的警告适用于HTML错误处理，在这里也有效)。</li><li id="0075" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">对于每个链接，使用<code class="fe oa ob oc nq b">justext</code>模块提取名为<code class="fe oa ob oc nq b">paragraphs.</code>的变量中的文本内容</li></ul><blockquote class="oe of og"><p id="4ce5" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>用于文本数据提取的justext模块的推荐实现通常如下所示:</p></blockquote><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="42da" class="nu kx iq nq b gy nv nw l nx ny">paragraphs = justext.justext(response.content, justext.get_stoplist("English"))<br/>for paragraph in paragraphs:<br/>  if not paragraph.is_boilerplate:<br/>    print paragraph.text</span></pre><blockquote class="oe of og"><p id="f3f0" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated"><strong class="lq ir">注意(续):</strong>在某些情况下，justext在删除错误分类的样板文件内容时可能过于激进。因此，在捕获更多文本信息的情况下，以样板文件的准确性为代价，使用以下替代方法:</p></blockquote><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="ea0a" class="nu kx iq nq b gy nv nw l nx ny">paragraphs = justext.justext(response.content, justext.get_stoplist(“English”))</span></pre><ul class=""><li id="030e" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">对于每个链接，一个名为<code class="fe oa ob oc nq b">for_processing</code>的变量被初始化以存储处理过的段落数据。</li><li id="62f3" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">对于每个链接，创建一组阻止字(<code class="fe oa ob oc nq b">block_words</code>)来排除不感兴趣的href目的地。可以修改这些目的地，以满足您各自应用程序的需求。</li><li id="2e8d" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">对于每个链接，对于每个提取的段落，将提取的段落对象转换为文本。</li><li id="f297" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">对于每个链接，对于每个提取的段落，如果该段落的长度小于50个字符，则将其从处理中忽略。这种情况的存在是为了提供足够长度的摘要模型段落——非常短的段落更难以解释和进一步压缩。</li><li id="67e5" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">对于每个链接，对于每个提取的段落，在给定第一个条件的情况下，省略包含任何上述块词的任何提取的段落。</li><li id="7c85" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj od mv mw mx bi translated">最后，对于每个链接，对于每个提取的段落，最多将5个段落合并成一个条目，并将该条目添加到<code class="fe oa ob oc nq b">contents</code>列表中。</li></ul><blockquote class="oe of og"><p id="9016" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>为什么一个条目只有5个段落连接在一起？这样做是为了提高模型的摘要速度，减少每个摘要任务的计算时间。此外，这样做是为了确保在总结模型限制为1024个令牌的情况下，每个条目不超过1024个令牌。如果返回的段落文本数据不够丰富，可以随意将该参数更改为更高的值。</p></blockquote><ul class=""><li id="0cc9" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated"><strong class="lq ir">可选</strong>:将<code class="fe oa ob oc nq b">links</code>和<code class="fe oa ob oc nq b">content</code>数据组织成<code class="fe oa ob oc nq b">pandas</code>数据帧，快速验证目前为止的结果。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">概述每个域URL的数据挖掘的最终代码块。</p></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="3ef1" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">将NLP管道应用于挖掘的数据</h1><p id="78f1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">下一步是将NLP管道应用于从每个页面的文本内容中挖掘数据。回想一下，NLP管道是用以下命令实例化的:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a0c3" class="nu kx iq nq b gy nv nw l nx ny"># Summarize textual content using BART in pytorch<br/>bart_summarizer = pipeline(“summarization”)</span></pre><p id="9abd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">继续为所有要存储的摘要内容创建一个新的空列表。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="aac1" class="nu kx iq nq b gy nv nw l nx ny">summarized_descriptions = []</span></pre><p id="2c54" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">回想一下，所有文本内容都存储在一个名为<code class="fe oa ob oc nq b">contents</code>的变量中。我们将使用同一个变量并遍历<code class="fe oa ob oc nq b">contents</code>列表中的每一项，这样每一项都由NLP管道处理。</p><p id="fef6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">可以选择打印初始项及其处理后的版本，以便在IDE中实时查看。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="2ed0" class="nu kx iq nq b gy nv nw l nx ny"># Summarize content for meta descriptions</span><span id="7db9" class="nu kx iq nq b gy nz nw l nx ny">summarized_descriptions = []</span><span id="1dca" class="nu kx iq nq b gy nz nw l nx ny">for item in content:<br/>  print(item)<br/>  print(bart_summarizer(item, min_length = 20, max_length = 50))<br/>  summarized_descriptions.append(bart_summarizer(item, min_length = 20, max_length = 50))</span></pre><blockquote class="oe of og"><p id="c963" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>三个参数传入bart_summarizer函数:待处理的<code class="fe oa ob oc nq b">item</code>、<code class="fe oa ob oc nq b">min_length</code>和<code class="fe oa ob oc nq b">max_length</code>。</p><p id="4af3" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated">请注意，要生成的序列的最小和最大长度都是用<strong class="lq ir">记号</strong>来测量的，而不是用字符。因为每个令牌的字符数经常是可变的，所以很难估计每个用例的正确值。</p><p id="b8e4" class="lo lp oh lq b lr mk jr lt lu ml ju lw oi mm lz ma oj mn md me ok mo mh mi mj ij bi translated">元描述最佳实践表明字符范围为120到60个字符—相应地调整<code class="fe oa ob oc nq b">min_length</code>和<code class="fe oa ob oc nq b">max_length</code>参数以满足这些最佳实践。</p></blockquote></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="7757" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">元描述的标准化</h1><p id="c938" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">目前，来自深度学习模型的汇总数据驻留在变量<code class="fe oa ob oc nq b">summarized_descriptions</code>中。下一步是将数据标准化，以便于阅读和导出。</p><p id="c664" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，最终的摘要已经作为字典列表存储在<code class="fe oa ob oc nq b">summarized_descriptions</code>中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/57a43063c5e575b3823e660ccb7042d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*rQWEQSnuw4Zdyjv7Zk5FbA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><code class="fe oa ob oc nq b">The summarized_descriptions variable </code>默认是字典列表。</p></figure><p id="b9b9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从解包变量开始，只提取字典列表的值。将这些值存储在一个名为<code class="fe oa ob oc nq b">meta_descriptions</code>的变量中。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="9a31" class="nu kx iq nq b gy nv nw l nx ny">#Retrieve values from list of dictionaries in summarized_descriptions</span><span id="c3e4" class="nu kx iq nq b gy nz nw l nx ny">meta_descriptions = [summary[0][“summary_text”] for summary in summarized_descriptions]</span></pre><p id="64a1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">清理数据的过程将是迭代的。为了简化起见，完成每个任务所需的步骤已经被分配到同一个变量的迭代中。</p><p id="3087" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">通过将名为<code class="fe oa ob oc nq b">meta_descriptions_clean1</code>的变量实例化为一个空列表，开始一般的数据清理。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="c1f6" class="nu kx iq nq b gy nv nw l nx ny">#General data cleaning</span><span id="02d2" class="nu kx iq nq b gy nz nw l nx ny">meta_descriptions_clean1 = []</span></pre><p id="a7b5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于<code class="fe oa ob oc nq b">meta_descriptions</code>中的每个描述:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="0f1f" class="nu kx iq nq b gy nv nw l nx ny">for description in meta_descriptions:</span></pre><ul class=""><li id="4148" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">清理前导和尾随空格</li></ul><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="5e26" class="nu kx iq nq b gy nv nw l nx ny">#Clean up leading and trailing spaces:</span><span id="0d64" class="nu kx iq nq b gy nz nw l nx ny">description = description.strip()</span></pre><ul class=""><li id="afda" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">清理多余的空间</li></ul><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="409e" class="nu kx iq nq b gy nv nw l nx ny">#Clean up excessive spaces</span><span id="1e6c" class="nu kx iq nq b gy nz nw l nx ny">description = re.sub(‘ +’, ‘ ‘, description)</span></pre><ul class=""><li id="1a2c" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">清理标点符号之间多余的空间</li></ul><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="d03c" class="nu kx iq nq b gy nv nw l nx ny">#Clean up punctuation spaces</span><span id="a11c" class="nu kx iq nq b gy nz nw l nx ny">description = description.replace(‘ .’, ‘.’)</span></pre><ul class=""><li id="05aa" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">清理不完整的句子</li></ul><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="81eb" class="nu kx iq nq b gy nv nw l nx ny">#Clean up incomplete sentences</span><span id="709e" class="nu kx iq nq b gy nz nw l nx ny">if “.” in description and not description.endswith(“.”):<br/>  description = description.split(“.”)[:-1]<br/>  description = description[0]</span></pre><ul class=""><li id="a227" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj od mv mw mx bi translated">将清理后的描述附加到<code class="fe oa ob oc nq b">meta_descriptions_clean1</code></li></ul><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="350e" class="nu kx iq nq b gy nv nw l nx ny">meta_descriptions_clean1.append(description)</span></pre><h2 id="d0e6" class="nu kx iq bd ky oo op dn lc oq or dp lg lx os ot li mb ou ov lk mf ow ox lm oy bi translated">手动和自动截断</h2><p id="d6e4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个脚本还应该将元描述与之前讨论的SEO最佳实践相结合。所有描述应约为160个字符，只有在无法生成其他摘要的情况下才超过该字符限制。</p><p id="5280" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个程序的目的是输出最佳的截断点供人工审查，以及尽可能自动截断元描述。</p><p id="df57" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">要启用此功能，需要在所有元描述中出现完整的句子。使用以下命令确保在每个描述的末尾找到一个句点，将相关值存储在一个名为<code class="fe oa ob oc nq b">meta_descriptions_clean2</code>的列表中:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="e6df" class="nu kx iq nq b gy nv nw l nx ny">meta_descriptions_clean2 = []</span><span id="c3aa" class="nu kx iq nq b gy nz nw l nx ny">#Add a period to all sentences (if missing)</span><span id="4db9" class="nu kx iq nq b gy nz nw l nx ny">for description in meta_descriptions_clean1:<br/>  if not description.endswith(“.”):<br/>    description = description + “.”<br/>  meta_descriptions_clean2.append(description)</span></pre><p id="d6ec" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">既然元描述的格式已经正确，那么就定义一个函数来标识所需标点符号的索引。在这种情况下，所需的标点是表示句子结束的句号。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="e854" class="nu kx iq nq b gy nv nw l nx ny">#Find the index of the punctuation character desired</span><span id="a45a" class="nu kx iq nq b gy nz nw l nx ny">def find_all(string, character):<br/>  index = string.find(character)<br/>  while index != -1:<br/>    yield index<br/>    index = string.find(character, index + 1)</span></pre><p id="65ad" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了显示可行的截断点以供用户手动检查每个描述，将截断点值存储在一个名为<code class="fe oa ob oc nq b">truncation_points</code>的列表中。该列表将存储相关描述的字符坐标，即描述可以拆分的位置，以便遵守最终的160个字符的限制。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a3f7" class="nu kx iq nq b gy nv nw l nx ny"># Store truncation points</span><span id="c70b" class="nu kx iq nq b gy nz nw l nx ny">truncation_points = []</span><span id="c650" class="nu kx iq nq b gy nz nw l nx ny">character = “.”</span><span id="738b" class="nu kx iq nq b gy nz nw l nx ny">for description in meta_descriptions_clean2:<br/>  indexes = list(find_all(description, character))<br/>  truncation_points.append(indexes)</span></pre><p id="40e6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了自动截断元描述，在<code class="fe oa ob oc nq b">meta_descriptions_clean3</code>中为自动截断的值初始化一个新的列表。对于<code class="fe oa ob oc nq b">meta_descriptions_clean2</code>中的每个描述，如果每个描述的长度大于160个字符，并且描述中存在1个以上的句点，则通过描述中的最后一句来缩短描述。如果截断导致描述不以句点结尾，请通过连接标点符号来正确地重新格式化。否则，描述的长度不超过160个字符，只需将其附加到<code class="fe oa ob oc nq b">meta_descriptions_clean3</code>列表中。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="7f98" class="nu kx iq nq b gy nv nw l nx ny"># Auto truncate</span><span id="75c3" class="nu kx iq nq b gy nz nw l nx ny">meta_descriptions_clean3 = []</span><span id="5688" class="nu kx iq nq b gy nz nw l nx ny">character = “.”</span><span id="4954" class="nu kx iq nq b gy nz nw l nx ny">for description in meta_descriptions_clean2:<br/>  if len(description) &gt; 160 and description.count(character) &gt; 1:<br/>    split = description.split(character)[:-2]<br/>    description = character.join(split)<br/>    if not description.endswith(“.”):<br/>      description = description + “.”<br/>    meta_descriptions_clean3.append(description)<br/>  else:<br/>    meta_descriptions_clean3.append(description)</span></pre><p id="e499" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，验证未截断和自动截断的元描述的字符数。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a74c" class="nu kx iq nq b gy nv nw l nx ny">#Verify length adherence for non-truncated descriptions</span><span id="9150" class="nu kx iq nq b gy nz nw l nx ny">len_meta_descriptions = []</span><span id="bce8" class="nu kx iq nq b gy nz nw l nx ny">for description in meta_descriptions_clean2:<br/>  len_meta_descriptions.append(len(description))</span><span id="0bc0" class="nu kx iq nq b gy nz nw l nx ny">#Verify length adherence for auto-truncated descriptions</span><span id="17ff" class="nu kx iq nq b gy nz nw l nx ny">truncated_len_meta_descriptions = []</span><span id="65c7" class="nu kx iq nq b gy nz nw l nx ny">for description in meta_descriptions_clean3:<br/>  truncated_len_meta_descriptions.append(len(description))</span></pre><p id="7367" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，将所有相关变量组织成一个pandas数据框架，以便于查看。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="05de" class="nu kx iq nq b gy nv nw l nx ny"># Organize results into dataframe</span><span id="a36d" class="nu kx iq nq b gy nz nw l nx ny">df = pd.DataFrame({‘link’: clean_links, ‘content’: content, ‘meta_descriptions’: meta_descriptions_clean2, ‘description_length’: len_meta_descriptions, ‘truncation_points’: truncation_points, ‘truncated_descriptions’: meta_descriptions_clean3, ‘truncated_length’: truncated_len_meta_descriptions})</span><span id="8af0" class="nu kx iq nq b gy nz nw l nx ny">df</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol om l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">查看所有输出的最终熊猫数据帧。</p></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="f4c5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">恭喜你，你成功了！有了这个最终的数据框输出，你可以将数据导出到<code class="fe oa ob oc nq b">.CSV</code>，或者用它来提供一个API，该API将自动更新你的网站的元描述。</p><p id="cd14" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">强烈建议对元描述进行审查，即使只是粗略地看一眼。即使对数据进行了大量的编程清理，仍然存在完全依赖于所挖掘的URL的潜在格式错误或截断。</p><p id="b3fe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">和往常一样，您可以在两者中找到相关的项目文件。py和。我的<a class="ae kv" href="https://github.com/rfinatan/Automatically-Generate-Site-Wide-Meta-Descriptions-with-Python-BART-for-PyTorch" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中的ipynb格式。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="daee" class="nu kx iq nq b gy nv nw l nx ny">If you enjoyed this content, feel free to connect on <a class="ae kv" href="https://www.linkedin.com/in/rfinatan/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</span></pre></div></div>    
</body>
</html>