<html>
<head>
<title>A Guide to the Encoder-Decoder Model and the Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码器-解码器模型和注意机制指南</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb?source=collection_archive---------0-----------------------#2020-10-11">https://betterprogramming.pub/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb?source=collection_archive---------0-----------------------#2020-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9986" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在TF2创建和训练一个神经机器翻译模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e9d203672a933294d128b43e22a88878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8zV_Jx4G5y46LaYO"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@alireza_attari?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alireza Attari </a>拍摄</p></figure><p id="37a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">今天，我们将继续我们的NLP世界之旅。在本文中，我们将描述编码器-解码器模型的基本架构，我们将应用于神经机器翻译问题，将文本从英语翻译成西班牙语。</p><p id="836e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">稍后，我们将介绍一项在处理NLP任务方面向前迈进了一大步的技术:注意力机制。我们将详述应用于序列对序列模型、多对多方法场景的注意力的基本处理。</p><p id="16f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但目前，这将是一个简单的注意力模型。我们不会对更复杂的模型发表评论，这些模型将在以后的文章中讨论，比如当我们讨论变形金刚的主题时。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="566d" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">什么是神经机器翻译？</h1><blockquote class="mr ms mt"><p id="4737" class="kw kx mu ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">“机器翻译是将一种语言的源文本自动转换成另一种语言的文本的任务。给定源语言中的一系列文本，不存在将该文本翻译成另一种语言的唯一最佳翻译。这是因为人类语言天生的模糊性和灵活性。这使得自动机器翻译的挑战变得很困难，可能是人工智能中最困难的挑战之一。”</p><p id="878d" class="kw kx mu ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">—“<em class="iq">机器学习大师”杰森·布朗利博士【1】</em></p></blockquote><p id="3617" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初，机器翻译(MT)问题是使用主要基于贝叶斯概率的统计方法来解决的。但是当神经网络变得更加强大和流行时，研究人员开始探索这项技术的能力，并找到了新的解决方案。它被称为神经机器翻译(NMT)。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="1e16" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">编码器-解码器模型的基本方法</h1><p id="7ec5" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">从上面我们可以推断出，NMT是一个我们处理输入序列以产生输出序列的问题，也就是序列对序列(seq2seq)问题。具体来说，多对多类型是标准方法，在输入和输出端都有几个元素的序列，递归神经网络的编码器-解码器架构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/ada084a7969c84c38e993bee5634671d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDDhDYhipj5owgPKmuSs2Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于文本翻译的Sutskever编码器-解码器模型的描述<br/>摘自<a class="ae kv" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwi0qYaLgbfsAhV3AxAIHQHaAvEQFjABegQIARAC&amp;url=http%3A%2F%2Fresearch.google.com%2Fpubs%2Farchive%2F43155.pdf&amp;usg=AOvVaw1LwAO3f8i-osbs86ssoWVm" rel="noopener ugc nofollow" target="_blank">“使用神经网络的序列到序列学习”</a> 2014</p></figure><p id="15f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">seq2seq模型由两个子网络组成，即编码器和解码器。左边的编码器接收来自源语言的序列作为输入，结果产生输入序列的紧凑表示，试图总结或压缩它的所有信息。然后，该输出成为解码器的输入或初始状态，解码器也可以接收另一个外部输入。</p><p id="b415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每个时间步长，解码器基于接收到的输入及其当前状态生成其输出序列的元素，并为下一个时间步长更新其自身的状态。</p><p id="c54f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输入和输出序列的大小是固定的，但它们不必匹配，输入序列的长度可能不同于输出序列的长度。</p><p id="169e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型的关键点是如何让编码器在单个输出元素中向解码器提供其输入序列的最完整和最有意义的表示，因为该向量或状态是解码器将从输入接收以生成相应输出的唯一信息。输入越长，就越难压缩成一个向量。</p><p id="6303" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将详细描述这个模型，并在后面的小节中构建它。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e4c1a2995f5be27d0cf6c1fd6f34f993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*EOhzbzjykV2C_cmw6Rczkw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由mcmurryjulie在Pixabay上提供</p></figure><h1 id="dcd1" class="lz ma iq bd mb mc nf me mf mg ng mi mj jw nh jx ml jz ni ka mn kc nj kd mp mq bi translated"><strong class="ak">数据集和文本处理</strong></h1><p id="1543" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在这个练习中，我们将使用成对的简单句。源文本将是英语，目标文本将是西班牙语，来自人们贡献的Tatoeba项目，每天都添加翻译。这是<a class="ae kv" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">链接</a>到一些不同语言的翻译。在那里你可以下载西班牙语/英语<code class="fe nk nl nm nn b">spa_eng.zip</code>文件；它包含了124，457对句子。</p><p id="2a24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">课文句子几乎是干净的；它们是简单的纯文本，所以我们只需要删除重音符号，将句子小写，并用空格替换所有内容，除了(<code class="fe nk nl nm nn b">a-z</code>、<code class="fe nk nl nm nn b">A-Z</code>、<code class="fe nk nl nm nn b">.</code>、<code class="fe nk nl nm nn b">?</code>、<code class="fe nk nl nm nn b">!</code>和<code class="fe nk nl nm nn b">,</code>)。应用这个预处理的代码取自神经机器翻译的TensorFlow教程。</p><p id="6109" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，让我们看看如何为我们的模型准备数据。很简单，步骤如下:</p><ul class=""><li id="2858" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">对数据进行标记，将原始文本转换为整数序列。首先，我们从<a class="ae kv" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>库中创建一个<code class="fe nk nl nm nn b">Tokenizer</code>对象，并使其适合我们的文本(一个用于输入，另一个用于输出)。</li><li id="4d2a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">从文本中提取一个整数序列:我们为每个输入和输出文本调用标记器的<code class="fe nk nl nm nn b">text_to_sequence</code> <em class="mu"> </em>方法。</li><li id="45ba" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">计算输入和输出序列的最大长度</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><ul class=""><li id="299c" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">创建输入和输出词汇表:使用我们之前创建的标记器，我们可以检索词汇表，一个将单词匹配到整数(<code class="fe nk nl nm nn b">word2idx</code>)，另一个将整数匹配到相应的单词(<code class="fe nk nl nm nn b">idx2word</code>)。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><ul class=""><li id="acac" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">填充句子:我们需要在序列的末尾填充零，这样所有的序列都有相同的长度。否则，我们无法批量训练模型。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><ul class=""><li id="6d75" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">创建一个批量数据生成器:我们想要在批量/一组句子上训练模型，所以我们需要使用<a class="ae kv" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"> tf.data </a>库和函数<code class="fe nk nl nm nn b">batch_on_slices</code>在输入和输出序列上创建一个数据集。</li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="b689" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">用递归神经网络建立一个编码器-解码器模型</strong></h1><p id="9ce9" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">为了更好地理解，我们可以将模型分为三个基本部分:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/aef5bcf49b5e08caf22b061be26e4f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YAPAHVYhsaEARYz45bO0Gg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从Simeon Kostadinov的“理解编码器-解码器序列到序列模型”[3]</p></figure><h2 id="54a0" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated"><strong class="ak">编码器</strong></h2><p id="4cdd" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">多层循环单元，在每个时间步中，接收一个输入令牌，收集相关信息并产生一个隐藏状态。这取决于RNN的类型；在我们的例子中，一个LSTM，单元混合当前隐藏状态和输入，并返回一个输出，丢弃，和一个新的隐藏状态。</p><h2 id="bb9b" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated"><strong class="ak">编码器矢量</strong></h2><p id="9619" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">编码器向量是编码器的最后一个隐藏状态，它试图包含尽可能多的有用输入信息，以帮助解码器获得最佳结果。这是解码器从输入中获得的唯一信息。</p><h2 id="8124" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated"><strong class="ak">解码器</strong></h2><p id="8df3" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">循环单元层，例如LSTMs，其中每个单元在时间步长<em class="mu"> t </em>产生一个输出。第一个单元的隐藏状态是编码器向量，其余单元接受来自前一个单元的隐藏状态。使用softmax函数计算输出，以获得输出词汇表中每个标记的概率。</p><p id="3750" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nk nl nm nn b">Encoder</code>类:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="ae66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nk nl nm nn b">Decoder</code>类:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="1790" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们的编码器和解码器被定义，我们可以初始化它们并设置初始隐藏状态。我们包含了一个简单的测试，调用编码器和解码器来检查它们是否工作正常:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="10df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们需要定义一个自定义损失函数，以避免在计算损失时考虑<code class="fe nk nl nm nn b">0</code>值和填充值。我们还必须定义一个自定义的精度函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="6331" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">训练和评估模型</h1><p id="b94b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">正如我们之前提到的，我们对批量训练网络感兴趣；因此，我们创建一个函数来执行一批数据的训练:</p><ul class=""><li id="0f3e" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">调用批处理输入序列的编码器—输出是编码的向量</li><li id="b9d1" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">将解码器初始状态设置为编码向量</li><li id="d9ff" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">调用解码器，将右移的目标序列作为输入。输出是对数(在损失函数中应用softmax函数)。</li><li id="131a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">计算批次数据的损失和准确性</li><li id="b413" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">更新编码器和解码器的可学习参数</li><li id="e4a5" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">更新优化程序</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="8a03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，我们的训练函数接收三个序列:</p><ul class=""><li id="c8be" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">输入序列:形状:<code class="fe nk nl nm nn b">[batch_size, max_seq_len, embedding dim]</code>的整数数组。这是编码器的输入序列。</li><li id="ae2f" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">目标序列:形状:<code class="fe nk nl nm nn b">[batch_size, max_seq_len, embedding dim]</code>的整数数组。这是我们模型的目标<strong class="ky ir"> </strong>，我们希望我们模型的输出。</li><li id="4be8" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">目标输入序列:形状:<code class="fe nk nl nm nn b">[batch_size, max_seq_len, embedding dim]</code>的整数数组。它是解码器的输入序列，因为我们使用<em class="mu">教师强制</em>。</li></ul><h2 id="11ed" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated">教师强迫</h2><p id="fda7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">教师强制是NLP中深度学习模型开发的关键训练方法。<em class="mu">“这是一种快速有效地训练递归神经网络模型的方法，该模型使用来自先前时间步骤的基础事实作为输入。”</em>，【8】"<a class="ae kv" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">什么是递归神经网络的教师强迫？杰森·布朗利博士</a></p><p id="1fa2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在递归网络中，通常在时间步长<em class="mu"> t </em>对RNN的输入是前一时间步长<em class="mu"> t-1的RNN的输出。</em>但是在老师的强制下，我们可以用实际输出来提高模型的学习能力。</p><blockquote class="mr ms mt"><p id="c399" class="kw kx mu ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">教师强制的工作方式是使用当前时间步长y(t)的训练数据集的实际或预期输出作为下一个时间步长X(t+1)的输入，而不是网络生成的输出</p><p id="7ed2" class="kw kx mu ky b kz la jr lb lc ld ju le mv lg lh li mw lk ll lm mx lo lp lq lr ij bi translated">——伊恩·古德菲勒的《深度学习》</p></blockquote><p id="af9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在我们的示例中，时间步长<em class="mu"> t </em>的目标输出是时间步长<em class="mu"> t+1的解码器输入。</em>解码器的输入序列将是向右移动一个位置的预期目标序列。为此，我们在第一个位置插入序列开始标记<code class="fe nk nl nm nn b">&lt;sos&gt;</code>,这样位置1的标记就转到位置2，位置2的标记转到位置3，依此类推。为了均衡序列的长度并界定它们的结尾，在目标序列中，我们将在最后一个位置放置一个序列结束标记<code class="fe nk nl nm nn b">&lt;eos&gt;</code>。</p><p id="f78a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们的模型输出与模型在训练中所看到的没有变化时，教师的强制是非常有效的。但是如果我们需要一个更有创造性的模型，其中给定一个输入序列可能有几个可能的输出，我们应该避免这种技术或者随机地应用它(只在一些随机的时间步骤中)。</p><p id="d729" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以编写主要的火车功能:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="cc50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们几乎准备好了——我们的最后一步包括调用主train函数，我们创建一个<code class="fe nk nl nm nn b">checkpoint</code>对象来保存我们的模型。因为训练过程需要很长时间来运行，所以每两个纪元我们就保存一次。以后我们可以恢复它，并用它来做预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="9c1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在有限的时间内(大约一小时)训练我们的编码器-解码器模型，采用40，000对句子和512个单元的rnn。我们取得了良好的成果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/6461ec4225ab4eeb9fc7b73d74a9add8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMPbBRO-97v95lKUb37BNA.png"/></div></div></figure><h2 id="da82" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated">做预测</h2><p id="7878" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在预测步骤中，out输入是一个长度为1的序列，即<code class="fe nk nl nm nn b">sos</code>标记。然后我们反复调用编码器和解码器，直到我们得到<code class="fe nk nl nm nn b">eos</code>令牌或者达到定义的最大长度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="b576" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到的一些预测的例子是:</p><pre class="kg kh ki kj gt os nn ot ou aw ov bi"><span id="a41a" class="of ma iq nn b gy ow ox l oy oz">['we re not going .', 'why are you sad ?']<br/>['no vamos . &lt;eos&gt;', '¿ por que estas triste ? &lt;eos&gt;']</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="9561" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">注意机制</h1><p id="906d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">先前描述的基于RNNs的模型在处理长序列时存在严重的问题，因为随着更多的记号被处理，第一记号的信息丢失或被稀释。上下文向量负责将给定源句子中的所有信息编码成一个包含几百个元素的向量。这给模型处理长句带来了挑战。在<a class="ae kv" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> Bahdanau等人，2014</a>【4】和<a class="ae kv" href="https://arxiv.org/abs/1508.04025" rel="noopener ugc nofollow" target="_blank"> Luong等人，2015</a>【5】中提出了解决方案。</p><p id="2ef0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们引入了一种叫做<em class="mu">注意力</em>的技术，这种技术极大地提高了机器翻译系统的质量。<em class="mu">“注意力允许模型根据需要关注输入序列的相关部分，访问编码器所有过去的隐藏状态，而不仅仅是最后一个”，</em>【8】<a class="ae kv" href="https://zhanghanduo.github.io/post/attention/" rel="noopener ugc nofollow" target="_blank">seq 2 seq模型带注意力</a>【张】。在每个解码步骤中，解码器会查看编码器的任何特定状态，并可以从该序列中选择性地挑选出特定元素来产生输出。我们将关注Luong的观点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/b60493563347584446578f1e28d48ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FjmdX1E37Kq1XH66_flsJw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">《注意机制》作者<a class="ae kv" href="https://blog.floydhub.com/author/gabriel/" rel="noopener ugc nofollow" target="_blank">加布里埃尔·洛耶</a>【6】</p></figure><p id="7558" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有两个相关点需要关注:</p><h2 id="a870" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated"><strong class="ak">对准矢量</strong></h2><p id="af0c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><em class="mu">“对齐向量是与输入或源序列具有相同长度的向量，并且在解码器的每个时间步长</em><strong class="ky ir"><em class="mu"/></strong><em class="mu"/>，【9】<a class="ae kv" href="https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a" rel="noopener" target="_blank">注意:具有注意机制的序列2序列模型</a>”由Renu Khandelwal编写。它的每个值都是源序列中相应单词的分数(或概率);它们告诉解码器在每个时间步应该关注什么。有三种方法可以计算比对分数:</p><ul class=""><li id="c711" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated"><strong class="ky ir">点积:</strong>我们只需要将编码器的隐藏状态乘以解码器的隐藏状态</li><li id="9630" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir">概述:</strong>与点积非常相似，但包含一个权重矩阵</li><li id="50aa" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir"> Concat: </strong>解码器隐藏状态和编码器隐藏状态在通过具有tanh激活函数的线性层之前首先相加，最后乘以权重矩阵</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7de676492373ef660760f3c3d7006f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*WL7vYAV1CvM7FZDBZOPGEQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">解码器输出</p></figure><p id="decf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比对分数被软最大化，因此权重将在0-1之间。</p><h2 id="6b25" class="of ma iq bd mb og oh dn mf oi oj dp mj lf ok ol ml lj om on mn ln oo op mp oq bi translated"><strong class="ak">上下文向量</strong></h2><p id="611e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">上下文向量是编码器输出、对齐向量的点积和编码器输出的加权平均和。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="a640" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦定义了我们的<code class="fe nk nl nm nn b">Attention</code>类，我们就可以创建解码器了。调用解码器时的完整步骤序列是:</p><ol class=""><li id="f7b7" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr pc nu nv nw bi translated">照常生成编码器隐藏状态，每个输入标记一个</li><li id="b53d" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr pc nu nv nw bi translated">应用RNN以产生新的隐藏状态，采用其先前的隐藏状态和先前时间步长的目标输出</li><li id="e20e" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr pc nu nv nw bi translated">如前所述，计算比对分数</li><li id="8c61" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr pc nu nv nw bi translated">计算上下文向量</li><li id="caf8" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr pc nu nv nw bi translated">在最后一个操作中，上下文向量与我们之前生成的解码器隐藏状态连接在一起。然后，它通过一个线性层，这个线性层充当分类器，让我们获得下一个预测单词的概率得分。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="01a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就这样，我们刚刚用Luong的注意力机制构建了一个解码器。您可以假设编码器与我们之前在初始seq2seq模型中创建的相同。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4f3f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">专心训练模型</h1><p id="8cad" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在我们可以定义step-train函数来训练批处理数据。这与我们之前看到的非常相似，但是这一次，我们将编码器返回的所有隐藏状态传递给解码器。我们需要创建一个循环来遍历目标序列，为每个序列调用解码器并计算损失函数，将输出与预期目标进行比较。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="3069" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是全部——我们准备专心训练我们的编码器-解码器。我们只需要创建检查点并调用主训练函数(这与我们为没有注意的模型编码是一样的)。</p><p id="7bdd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一次，我们在大约相同的时间内改进了我们的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/1948828a3643f69792ac64a5e7a6b99c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l13mKZu8yQzzPpxd38Iipg.png"/></div></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="27d9" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">推理时间</h1><p id="5f9f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们的模型的预测函数，关注目标序列的迭代，由<code class="fe nk nl nm nn b">&lt;sos&gt;</code>标记开始，接收来自解码器和对齐向量的下一个字。可以画出这个向量来告诉我们解码器应该更关注哪个输入标记。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="8375" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以调用<code class="fe nk nl nm nn b">predict_att_seq2seq</code>函数并绘制<code class="fe nk nl nm nn b">alignments</code> <em class="mu"> </em>来观察我们的模型是如何工作的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/0178acbc2518b27a92243efebfd861df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*j37NQyq9s7D2vvEqJEE6wA.png"/></div></figure><p id="d826" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以通过这个<a class="ae kv" href="https://github.com/edumunozsala/NMT-encoder-decoder-Attention" rel="noopener ugc nofollow" target="_blank">链接</a>访问并获得我的GitHub资源库中的代码，或者您也可以在我的<a class="ae kv" href="https://edumunozsala.github.io/BlogEms/fastpages/jupyter/encoder-decoder/lstm/attention/tensorflow%202/2020/10/07/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.html" rel="noopener ugc nofollow" target="_blank">博客</a>中获得本文和代码。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="2bc4" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><p id="c23e" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">[1] <a class="ae kv" href="https://machinelearningmastery.com/introduction-neural-machine-translation/" rel="noopener ugc nofollow" target="_blank">机器学习掌握，《神经机器翻译的温和介绍》，杰森·布朗利</a></p><p id="2b8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] <a class="ae kv" href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">“用神经网络进行序列对序列学习”，作者Ilya Sutskever </a></p><p id="4ea4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] <a class="ae kv" href="https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346" rel="noopener" target="_blank">“理解编码器-解码器序列到序列模型”，作者Simeon Kostadinov </a></p><p id="bd1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] <a class="ae kv" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译，Bahdanau等人，2014 </a></p><p id="a1d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] <a class="ae kv" href="https://arxiv.org/abs/1508.04025" rel="noopener ugc nofollow" target="_blank">基于注意的神经机器翻译的有效方法。，2015年</a></p><p id="a388" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Gabriel Loye在FloydHub博客上的“注意力机制”</p><p id="85fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">【7】<em class="mu"/><a class="ae kv" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank"><em class="mu">什么是老师对递归神经网络的强迫？</em> </a> <em class="mu">”作者杰森·布朗利博士</em></p><p id="ac89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]<a class="ae kv" href="https://zhanghanduo.github.io/post/attention/" rel="noopener ugc nofollow" target="_blank">seq 2 seq模型与注意事项</a>张</p><p id="515d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9] " <a class="ae kv" href="https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a" rel="noopener" target="_blank">注意:具有注意机制的序列2序列模型</a></p><p id="6d80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">“可视化神经机器翻译模型(Seq2seq模型的机制，请注意)”作者Jay Alammar </a></p><p id="4ce2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39" rel="noopener" target="_blank">“关注与Keras的深层网络”，作者Thushan Ganegedara </a></p></div></div>    
</body>
</html>