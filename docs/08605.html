<html>
<head>
<title>SwiftUI + Core ML + ARKit — Create an Object Detection iOS App</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SwiftUI + Core ML + ARKit —创建一个对象检测iOS应用程序</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/swiftui-core-ml-arkit-create-an-object-detection-ios-app-2c74fc57d984?source=collection_archive---------8-----------------------#2021-05-19">https://betterprogramming.pub/swiftui-core-ml-arkit-create-an-object-detection-ios-app-2c74fc57d984?source=collection_archive---------8-----------------------#2021-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7619" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用机器学习和增强现实的力量来检测你周围的物体</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b63108a3db3e34aef7de790941437eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jyH6-CzhQyQk7EO0"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@patuphotos?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕特里克</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照。</p></figure><p id="b481" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我最近在Medium上阅读和研究iOS文章的时候，发现关于Core ML，ARKit，SwiftUI的文章很少。这就是为什么我想把这三个库放在一起并<a class="ae kv" href="https://github.com/hanleyweng/CoreML-in-ARKit" rel="noopener ugc nofollow" target="_blank">编写一个简单的应用程序</a>。</p><p id="917b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">长话短说，该应用程序使用了使用Python编写的名为ResNet的深度学习库的Swift版本。</p><p id="d201" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在来说说应用逻辑。当我们在应用程序中单击屏幕时，我们将捕获屏幕中心的当前图像，用我们的核心ML模型处理它，并创建一个增强现实文本。逻辑很简单。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="ebbc" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">我们开始吧！</strong></h1><p id="76f8" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先，我们从苹果提供给我们的模型中下载<a class="ae kv" href="https://developer.apple.com/machine-learning/models/" rel="noopener ugc nofollow" target="_blank">我们的模型</a>，并将我们的模型放在我们的项目根目录中。这里我们需要注意的一点是，我们的目标设备是一个真实的设备。否则，模型和<code class="fe mw mx my mz b">ARView</code>可能会出错。我猜是Xcode的bug。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="2c94" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">我们要做什么？</h1><p id="9630" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们将创建一个界面为SwiftUI的增强现实项目。然后我们将删除项目中的名称<code class="fe mw mx my mz b">Experience.rcproject</code>。我们不需要它。</p><p id="6655" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之后，我们将在<code class="fe mw mx my mz b">UIViewRepresentable</code>中创建我们的ResNet模型。因为我们的ResNet模型会抛出错误，所以我们在<code class="fe mw mx my mz b">do catch</code>块中配置我们的模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="4e1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在<code class="fe mw mx my mz b">makeUIView()</code>中名为<code class="fe mw mx my mz b">arView</code>的常量移出函数，并创建一个<code class="fe mw mx my mz b">ARRaycastResult</code>对象，在这里我们可以获得屏幕上被触摸点的信息，以及一个<code class="fe mw mx my mz b">VNRequest</code>数组，在这里我们将收集我们的核心ML请求。</p><pre class="kg kh ki kj gt nc mz nd ne aw nf bi"><span id="373a" class="ng ma iq mz b gy nh ni l nj nk"><strong class="mz ir">let</strong> arView = ARView(frame: .zero)<br/><strong class="mz ir">var</strong> hitTestResultValue : ARRaycastResult!<br/><strong class="mz ir">var</strong> visionRequests = [VNRequest]()</span></pre><p id="f21e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完成所有这些步骤后，我们在<code class="fe mw mx my mz b">Struct</code>中定义了一个<code class="fe mw mx my mz b">Coordinator</code>类，并在我们的<code class="fe mw mx my mz b">UIViewRepresentable</code>对象中提供了对该类中init方法的引用，并使用<code class="fe mw mx my mz b">makeCoordinator()</code>函数将其引入到我们的<code class="fe mw mx my mz b">UIViewRepesentable</code>对象中。</p><p id="0b28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经执行了简单的操作。现在让我们继续主要的操作。</p><p id="6838" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们将一个<code class="fe mw mx my mz b">tapGesture</code>添加到我们的<code class="fe mw mx my mz b">makeUIView()</code>函数中的<code class="fe mw mx my mz b">arView</code>对象中，并将<code class="fe mw mx my mz b">action</code>方法添加到我们的<code class="fe mw mx my mz b">Coordinator</code>类中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="0769" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们在<code class="fe mw mx my mz b">UIViewRepresentable</code>的工作。现在让我们继续我们在<code class="fe mw mx my mz b">Coordinator</code>类中的操作。</p><p id="73af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们创建一个函数，将定义的对象名称放在屏幕上，并为此函数定义一个文本参数。</p><p id="a10b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单说一下功能吧。我们在<a class="ae kv" href="https://developer.apple.com/documentation/realitykit/meshresource" rel="noopener ugc nofollow" target="_blank">网格资源</a>类的<code class="fe mw mx my mz b"><a class="ae kv" href="https://developer.apple.com/documentation/realitykit/meshresource/3244422-generatetext" rel="noopener ugc nofollow" target="_blank">genrateText</a></code>函数中创建一个增强现实对象，并且我们创建一个<a class="ae kv" href="https://developer.apple.com/documentation/realitykit/simplematerial" rel="noopener ugc nofollow" target="_blank">简单材质</a>对象。这些对象有助于应用一些材料(等。颜色、粗糙度)到我们的增强现实对象。</p><p id="e0ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将这个材质转换成一个<code class="fe mw mx my mz b">Entity</code>对象，根据我们上面创建的<code class="fe mw mx my mz b">rayCastValue</code>中的坐标，按照现实世界的坐标来放置。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="f1da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们在<code class="fe mw mx my mz b">visionRequest</code>名称中创建将处理图像的函数。这个函数接受一个"<a class="ae kv" href="https://developer.apple.com/documentation/corevideo/cvpixelbuffer-q2e" rel="noopener ugc nofollow" target="_blank"> CVPixelBuffer </a>"类型的值。</p><p id="063b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在通过这个处理请求的函数创建了一个核心ML请求之后，我们在现实世界中创建了一个文本对象，其中包含了我们想要显示信息的对象的名称和百分比。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="4711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为最后一个过程，我们创建了标记为<code class="fe mw mx my mz b">obj</code>的方法。在该方法中，我们根据来自<code class="fe mw mx my mz b">tapGesture</code>对象的屏幕坐标获得真实世界中的3D位置，这要归功于我们的<code class="fe mw mx my mz b">raycast</code>方法。</p><p id="5eb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们获得点击该屏幕时的快照后，我们从该数据中点击的位置就是<code class="fe mw mx my mz b">rayCastResultValue</code>。我们将常数和当时获得的截图转移到我们的方法中，然后将截图发送到我们的核心ML模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="85ee" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结果</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a9f3090d55aa1e57bd5a30baa6795909.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/1*lhFHmhAEnrDYgXSe0kVsGA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">结果</p></figure><p id="3d0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里只有一个小问题。文本对象看起来并不是双面的，也没有这方面的API。我希望苹果很快会在这个问题上带来一些创新。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="ee92" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="8e5e" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">使用Core ML和ARKit4库，我们在SwiftUI界面中开发了一个对象识别应用程序。你可以在GitHub上找到完整的项目。我在下面留下了有用的资源来获取更多信息。</p><p id="b0bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您的阅读！</p><div class="nm nn gp gr no np"><a href="https://github.com/eren-celik/SwiftUI-CoreMl-ARKit" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd ir gy z fp nu fr fs nv fu fw ip bi translated">额仁-切利克/斯威夫特ui-CoreMl-阿尔基特</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">在GitHub上创建一个帐户，为eren-celik/swift ui-CoreMl-ARKit开发做出贡献。</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">github.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od kp np"/></div></div></a></div><h2 id="6f5a" class="ng ma iq bd mb oe of dn mf og oh dp mj lf oi oj ml lj ok ol mn ln om on mp oo bi translated">核心ML</h2><ul class=""><li id="c74d" class="op oq iq ky b kz mr lc ms lf or lj os ln ot lr ou ov ow ox bi translated"><a class="ae kv" href="https://developer.apple.com/documentation/vision/recognizing_objects_in_live_capture" rel="noopener ugc nofollow" target="_blank">https://developer . apple . com/documentation/vision/recogniting _ objects _ in _ live _ capture</a></li><li id="e152" class="op oq iq ky b kz oy lc oz lf pa lj pb ln pc lr ou ov ow ox bi translated"><a class="ae kv" href="https://www.raywenderlich.com/7960296-core-ml-and-vision-tutorial-on-device-training-on-ios" rel="noopener ugc nofollow" target="_blank">https://www . raywenderlich . com/7960296-core-ml-and-vision-tutorial-on-device-training-on-IOs</a></li></ul><h2 id="ad95" class="ng ma iq bd mb oe of dn mf og oh dp mj lf oi oj ml lj ok ol mn ln om on mp oo bi translated">阿尔基特</h2><ul class=""><li id="e6e7" class="op oq iq ky b kz mr lc ms lf or lj os ln ot lr ou ov ow ox bi translated">【https://developer.apple.com/augmented-reality/ T4】</li><li id="c666" class="op oq iq ky b kz oy lc oz lf pa lj pb ln pc lr ou ov ow ox bi translated"><a class="ae kv" href="https://medium.com/flawless-app-stories/how-to-enforce-swift-style-and-conventions-into-the-project-using-swiftlint-7588b4ffba66" rel="noopener">https://medium . com/无瑕app-stories/how-to-enforce-swift-style-and-conventions-into-the-project-using-swift lint-7588 B4 ffba 66</a></li></ul></div></div>    
</body>
</html>