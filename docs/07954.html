<html>
<head>
<title>Emotion Classification and Face Detection Using ARKit and Core ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于ARKit和Core ML的情感分类和人脸检测</h1>
<blockquote>原文：<a href="https://betterprogramming.pub/emotion-classification-and-face-detection-using-arkit-and-coreml-6f4582363e7d?source=collection_archive---------8-----------------------#2021-03-09">https://betterprogramming.pub/emotion-classification-and-face-detection-using-arkit-and-coreml-6f4582363e7d?source=collection_archive---------8-----------------------#2021-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a4f6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于构建基于机器学习和增强现实的iOS应用的实践教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2c20185ed179bd804ec1db8ed3676a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LjgrN8H43lYk4i0A"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">米在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kv" href="https://unsplash.com/@phammi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍照。</a></p></figure><p id="f145" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">增强现实是一项相当新的技术，它允许移动用户与他们的周围环境进行交互。近年来，苹果、谷歌、微软等大公司在这一领域投入了大量资源。</p><p id="a479" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将深入研究增强现实和机器学习的细节，并创建一个演示应用程序来展示它们的能力。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="a027" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">背景</h1><p id="a387" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated"><a class="ae kv" href="https://developer.apple.com/documentation/arkit" rel="noopener ugc nofollow" target="_blank"> ARKit </a>是一个开发平台，用户可以通过使用设备的传感器和摄像头来创建与环境互动的AR应用。ARKit于2017年在iOS 11中首次推出，于2020年推出第4版，比以往任何时候都更强大。</p><p id="ee24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ARKit的人脸识别解决方案是基于使用设备的原深感摄像头。这是一项能够创建用户面部和头部3D模型的技术，也是Face ID技术背后的基础。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/6432618eac84fb0e54930affb849b3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGhiWENwlZ2NLteqbVTszA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="mx">原深感摄像头系统(来源:</em><a class="ae kv" href="https://www.iphonefaq.org/archives/976228" rel="noopener ugc nofollow" target="_blank"><em class="mx">iPhone常见问题解答</em> </a> <em class="mx"> ) </em></p></figure><p id="45a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">iOS 11引入了机器学习，允许用户构建具有新功能的应用和体验，如<strong class="ky ir"> </strong>图像和音频分析、自然语言处理和语音识别。</p><p id="a5b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Core ML是为运行苹果操作系统的设备构建的框架。创建ML模型包括用足够大的数据集来训练它，以产生精确的分类或预测。</p><p id="611e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">苹果提供了一个<a class="ae kv" href="https://developer.apple.com/machine-learning/models/" rel="noopener ugc nofollow" target="_blank">集合</a>预先训练好的ML模型，可以照原样使用。在我们的教程中，我们不会使用其中的一个，而是使用另一个<a class="ae kv" href="https://talhassner.github.io/home/publication/2015_ICMI" rel="noopener ugc nofollow" target="_blank">预先训练好的模型</a>，它能够根据人脸对情绪进行分类。</p><p id="5570" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://developer.apple.com/documentation/vision" rel="noopener ugc nofollow" target="_blank"> Vision </a>框架是一个强大的工具包，用于在输入图像或视频上应用计算机视觉算法。它能够执行面部和面部标志检测、条形码识别和文本检测(OCR)。它还可以使用定制的CoreML模型来执行检测、预测和分类。今天，我们将重点讨论后者。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="ca11" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">创建新项目</h1><p id="1986" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们首先在Xcode中创建新项目。给它一个名字，选择一个团队，选择“故事板”作为界面，选择“Swift”作为语言。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/8c4cd8cc549bc46870ea91655881388a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f7dQQ6cBlr67NKbq"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在Xcode中创建新项目。</p></figure><p id="d98c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们最终应该有这样的结构:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/b824bc495c142a7479555070b2e40252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Bu7D9dcHVbguhL8t"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Xcode中的项目结构</p></figure><p id="ea5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们需要在<code class="fe my mz na nb b">Info.plist</code>文件中添加这个相机使用描述，以便请求相机权限:<code class="fe my mz na nb b">NSCameraUsageDescription</code> <em class="nc">。</em></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="cc2e" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">创建场景并显示面网格</h1><p id="4590" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">第一步是建立<code class="fe my mz na nb b">sceneView</code>。我们将全屏设置这个视图，由于我们使用的是<code class="fe my mz na nb b">ARFaceTrackingConfiguration</code>，它将显示实时的前置摄像头画面。</p><p id="797a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe my mz na nb b">ARFaceTrackingConfiguration</code>是一种ARKit配置，能够使用前置摄像头和TrueDepth传感器检测和跟踪用户的面部。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="cf3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们已经开始了面部跟踪会话，我们需要我们的<code class="fe my mz na nb b">ViewController</code>通过实现这个函数来符合<code class="fe my mz na nb b">ARSCNViewDelegate</code>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="cd22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将创建一个节点，并在我们的脸上应用白线遮罩:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5fd3b15d8eb58903d02c224e35cf7642.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*aiBfgMYgqfAmW9WX9rLtRg.gif"/></div></figure><p id="ac50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很酷，但是当用户闭上眼睛或张开嘴时更新遮罩怎么办？</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="99da" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">更新面网格</h1><p id="9846" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">为了实现这一点，我们需要实现<code class="fe my mz na nb b">ARSCNViewDelegate</code>协议的另一个功能:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="76e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。现在看起来好多了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/65acb794349674eee97d882e934ef599.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*esLbBfgg0AkBtR1HgDywXA.gif"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="bb75" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">创建核心ML请求</h1><p id="2b63" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">现在，我们已经使用ARKit设置了实时人脸跟踪，我们可以开始实现核心的ML请求，该请求将输出一种情绪(生气、高兴、中性等)。)鉴于用户的面子。</p><p id="0a0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但在此之前，我们先来看看如何创建一个<code class="fe my mz na nb b">.mlmodel</code>文件。</p><h2 id="5e38" class="ng ma iq bd mb nh ni dn mf nj nk dp mj lf nl nm ml lj nn no mn ln np nq mp nr bi translated">创建核心ML模型</h2><p id="22fe" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">苹果公司提供了一个创建这种模型的工具，叫做Create ML。您可以使用Finder打开它，或者通过访问Xcode菜单&gt;打开开发者工具&gt;创建ML从Xcode启动它。</p><p id="0bd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Create ML提供了几个可供选择的模板，每个模板针对一个特定的场景:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/e3f8439858c15a22a01d5f183e0301c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4Fgt8pZx0Rcb25A2"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">创建核心ML模型。</p></figure><p id="12df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们今天不会创建任何分类器，因为我们将使用一个已经训练好的分类器来对情绪进行分类。</p><p id="364b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个模型可以在这个谷歌驱动上找到，我们只需要把它拖放到Xcode项目的根文件夹中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/69c00b41eeefa78bd621eb0830d4cca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YKAZFQubufJvNUJD"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNNEmotions ML模型</p></figure><p id="b6f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，回到创建核心ML请求。</p><p id="458c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要向我们的模型声明一个实例:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="5f72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在更新面部网格的相同<code class="fe my mz na nb b">didUpdate</code>功能中，我们也将这样做:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="0755" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将创建一个<code class="fe my mz na nb b">VNImageRequestHandler</code>把包含我们的脸的当前帧作为<code class="fe my mz na nb b">CVPixelBuffer</code>并在其上执行<code class="fe my mz na nb b">VNCoreMLRequest</code>。</p><p id="d53a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着Core ML框架将对每幅图像运行机器学习算法，以便将面部表情分为以下七类之一:愤怒、厌恶、恐惧、高兴、自然、悲伤和惊讶。</p><p id="bc88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了避免误报，我们将分类的置信阈值设置为92%的任意值。如果分类器至少对情感有92%的把握，我们将把它显示在我们头顶上方的浮动3D标签中。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="f66c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">在浮动标签中显示情感</h1><p id="9dbe" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们将再次使用ARKit的力量来创建一个文本节点，并将其放置在我们的头顶上。</p><p id="e23b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们声明文本节点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="aae7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们实例化文本节点并将其添加到现有场景中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b012" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该函数将由另一个<code class="fe my mz na nb b">ARSCNViewDelegate</code>函数调用:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="c2ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，在请求处理程序中，我们用以下行替换<code class="fe my mz na nb b">//TODO</code>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="a37b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将在三维浮动标签中显示情感:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4f4458db4e2679778290d523de4fd636.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*MJNEHnJUpf6XpKqPnoLntg.gif"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4c51" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="98cc" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">ARKit和Core ML是强大的本地工具，提供了易于使用的设备功能，适合iOS开发的现代趋势。他们已经有了很大的潜力，并且随着每一次新的迭代得到改进。</p><p id="4640" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本教程面向初学者和专业人员。最终项目可以在我的GitHub页面找到:</p><div class="ns nt gp gr nu nv"><a href="https://github.com/bogdan-razvan/ARKit-CoreML-Emotion-Classification" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">bogdan-Raz van/ARKit-CoreML-情绪-分类</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">ARKit和CoreML情感分类用法示例-bogdan-Raz van/ARKit-CoreML-情感-分类</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">github.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj kp nv"/></div></div></a></div><p id="0190" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在<a class="ae kv" href="https://www.linkedin.com/in/bogdan-razvan/" rel="noopener ugc nofollow" target="_blank">领英</a>上连接吧！</p></div></div>    
</body>
</html>